<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>构建 Effective Cloud Agent</title>
    <link href="/2026/02/18/2026-02-18-cloud-agent-implementation-details/"/>
    <url>/2026/02/18/2026-02-18-cloud-agent-implementation-details/</url>
    
    <content type="html"><![CDATA[<h1 id="从SSE服务说起"><a href="#从SSE服务说起" class="headerlink" title="从SSE服务说起"></a>从SSE服务说起</h1><p>先从一个可运行的 Chatbot 服务开始：前端通过 <code>POST /chat</code> 发起请求，通过 <code>GET /stream/:conversation_id</code> 接收 <code>SSE</code> 事件流。<br>这里把职责拆成两层：</p><ul><li>Display Layer：向 UI 输出事件，服务展示与回放。</li><li>Business Layer：维护消息历史，服务 prompt 构造与推理。</li></ul><pre><code class=" mermaid">flowchart LR    UI[Web UI]    API[Chat Service]    LLM[LLM API]    DL[(Display Events)]    BL[(Business Messages)]    UI --&gt;|chat request| API    API --&gt;|sse stream| UI    API --&gt; BL    API --&gt; LLM    LLM --&gt; API    API --&gt; DL</code></pre><p>对应的最小存储模型：</p><ul><li><code>conversation(id, created_at)</code></li><li><code>message(id, conversation_id, role, content, created_at)</code></li></ul><p>单轮写路径：</p><ol><li>接收用户输入并落库 <code>role=user</code>。</li><li>读取最近 <code>N</code> 条消息构造 prompt。</li><li>调用 LLM，按 token 生成 <code>SSE</code> 事件并写入 Display Layer。</li><li>推理结束后落库 <code>role=assistant</code>。</li></ol><pre><code class=" mermaid">sequenceDiagram    participant U as User    participant API as Chat Service    participant DB as Message Store    participant LLM as LLM API    U-&gt;&gt;API: POST /chat    API-&gt;&gt;DB: insert user message    API-&gt;&gt;DB: load recent messages    API-&gt;&gt;LLM: inference    LLM--&gt;&gt;API: token stream    API--&gt;&gt;U: SSE events    API-&gt;&gt;DB: insert assistant message</code></pre><p>约束保持简单：单进程、单副本、不处理并发写冲突。</p><p>下一段再进入“引入文件系统后”的架构分叉：<code>stateless agent service + sandbox</code> vs <code>agent in vm</code>。</p><h1 id="扩展文件系统"><a href="#扩展文件系统" class="headerlink" title="扩展文件系统"></a>扩展文件系统</h1><h2 id="Agent-in-VM"><a href="#Agent-in-VM" class="headerlink" title="Agent in VM"></a>Agent in VM</h2><blockquote><p>这个模式下，Agent没有多租户的概念，面对的是一台专用的虚拟机，所有多租户的概念在外部业务层实现</p></blockquote><p>当 Agent 需要稳定操作本地开发环境、浏览器和长生命周期进程时，<code>Agent in VM</code> 是最直接的方案。<br>它的核心思想是：平台做“VM 分配与调度”，Agent 只面对“单租户执行环境”。</p><pre><code class=" mermaid">flowchart LR    U[User]    APP[Multi-tenant App]    SCH[VM Scheduler]    VM1[Tenant VM]    AG[Agent Process]    FS[(Local FS)]    SH[Shell/Browser]    LLM[LLM API]    U --&gt; APP    APP --&gt; SCH    SCH --&gt; VM1    VM1 --&gt; AG    AG --&gt; FS    AG --&gt; SH    AG --&gt; LLM</code></pre><p>设计重点：</p><ol><li>租户隔离天然清晰：每个会话绑定一台 VM。</li><li>状态恢复简单：文件、进程、缓存都在 VM 内。</li><li>代价是资源成本和调度时延：空闲 VM 成本高，启动&#x2F;恢复慢于无状态服务。</li></ol><p>案例：Manus、利用Claude Code SDK构建多租户agent</p><h2 id="Stateless-Agent-Service-Sandbox"><a href="#Stateless-Agent-Service-Sandbox" class="headerlink" title="Stateless Agent Service + Sandbox"></a>Stateless Agent Service + Sandbox</h2><p>该方案将控制逻辑与执行环境解耦：Agent Service 保持无状态，Sandbox 承载会话执行状态。</p><p>分层如下：</p><ul><li>Agent Service：无状态，负责规划、路由、工具编排。</li><li>Sandbox：有状态，负责执行命令和访问会话文件。</li><li>Shared Storage：跨 Sandbox 挂载同一会话目录。</li></ul><pre><code class=" mermaid">flowchart LR    U[User]    GW[API Gateway]    AS[Stateless Agent Service]    LLM[LLM API]    SB[Session Sandbox Pod/Lambda]    NFS[(Shared Storage)]    TOOLS[External Tools]    U --&gt; GW --&gt; AS    AS --&gt; LLM    AS --&gt; SB    SB --&gt; NFS    SB --&gt; TOOLS    AS --&gt; TOOLS</code></pre><p>可行性前提：Agentic loop 依赖“状态可引用”，不依赖“进程常驻”。只要 session 可以绑定到可恢复的文件和执行上下文，服务就可以无状态扩缩容。</p><p>关键约束：</p><ol><li>边界必须明确：哪些工具在 Agent Service 执行，哪些必须进 Sandbox。</li><li>安全默认收敛：出网控制、域名&#x2F;IP 白名单、速率限制、租户目录隔离。</li><li>存储性能决定体验：高频读写场景要评估本地盘缓存或更高性能分布式存储。</li></ol><h1 id="Durable-Execution（Stateless-Sandbox-的可靠性拓展）"><a href="#Durable-Execution（Stateless-Sandbox-的可靠性拓展）" class="headerlink" title="Durable Execution（Stateless + Sandbox 的可靠性拓展）"></a>Durable Execution（Stateless + Sandbox 的可靠性拓展）</h1><p>在 <code>Stateless Agent Service + Sandbox</code> 中，实例漂移是常态。重试不能依赖进程内存。<br>Durable Execution 的目标是将一次 loop 分解为可恢复步骤，并在步骤边界写入 checkpoint。</p><p>引入原因：</p><ol><li>实例漂移是常态：Pod 重建、请求重路由都会打断内存态执行。</li><li>工具调用有副作用：整轮重放会产生重复写入或重复外部调用。</li><li>长任务跨多分钟：必须允许中断后继续，而不是从头开始。</li></ol><p>最小状态拆分建议：</p><ol><li><code>message_state</code>：会话消息、摘要、token budget。</li><li><code>filesystem_state_ref</code>：工作目录版本、挂载路径、快照 ID。</li><li><code>execution_state</code>：当前 step、计划版本、上一步输出。</li><li><code>effect_log</code>：已执行副作用记录（外部请求 ID、写操作哈希）。</li></ol><pre><code class=" mermaid">flowchart LR    STEP[Run Step]    CKPT[Write Checkpoint]    EFFECT[Record Side Effect]    FAIL[Failure]    RESUME[Load Last Checkpoint]    NEXT[Run Next Step]    STEP --&gt; CKPT    STEP --&gt; EFFECT    STEP --&gt; FAIL    FAIL --&gt; RESUME    RESUME --&gt; NEXT</code></pre><p>实现规则：</p><ol><li>每个 step 必须可重入：相同输入重复执行，结果语义不变。</li><li>每个副作用必须可去重：通过 <code>idempotency_key</code> 或外部事务键避免重复提交。</li></ol><h2 id="语义恢复"><a href="#语义恢复" class="headerlink" title="语义恢复"></a>语义恢复</h2><p>在工程实践中，很多工具调用不可回滚，或回滚成本远高于收益。<br>因此，失败处理可以优先采用“语义恢复”，而不是“强事务回滚”。</p><p>例如三个工具并行执行时其中一个报错，不必强制撤销其余两个工具的副作用。可采用以下策略：</p><ol><li>将消息历史恢复到这组并行工具调用之前。</li><li>保留本次工具调用返回（包含不确定状态）。</li><li>对失败或不确定结果注入系统说明：<code>上次调用出错，可能已部分执行成功，请先确认状态再继续</code>。</li><li>让模型基于这条说明继续规划下一步（先确认，再补偿，或跳过）。</li></ol><blockquote><p>这里还可以向外暴露重试事件，以确保显示层不会出现未定义行为</p></blockquote><p>该策略的取舍很明确：放弃严格事务语义，换取更低实现复杂度和更高恢复连续性。</p><h1 id="Loop-级隔离编排（优化Agent-Service的可扩缩容性）"><a href="#Loop-级隔离编排（优化Agent-Service的可扩缩容性）" class="headerlink" title="Loop 级隔离编排（优化Agent Service的可扩缩容性）"></a>Loop 级隔离编排（优化Agent Service的可扩缩容性）</h1><p>这是一种中间态方案：保持 loop 代码结构不变，只隔离 loop 实例。<br>实现方式是“每个 turn 启动一个 Worker&#x2F;Lambda”，由队列驱动执行；SSE 由事件通道异步转发到流网关。</p><p>适用原因：</p><ol><li>保持调试模型稳定：单个 loop 代码路径不变，问题定位更直接。</li><li>缩小爆炸半径：单个任务异常只影响当前 worker。</li><li>提升发布速度：无状态 worker 镜像可快速滚动升级。</li></ol><pre><code class=" mermaid">flowchart LR    U[User]    API[Agent API]    Q[(Task Queue)]    W[Loop Worker Lambda/Pod]    LLM[LLM API]    SB[Sandbox]    EQ[(Event Queue)]    SG[SSE Gateway]    U --&gt; API    API --&gt; Q    Q --&gt; W    W --&gt; LLM    W --&gt; SB    W --&gt; EQ    EQ --&gt; SG    SG --&gt; U</code></pre><p>实现要点：</p><ol><li><code>turn_id</code> 作为任务主键，保证同一轮只有一个 active worker。</li><li>Worker 事件写入 <code>event queue</code>，由 <code>SSE gateway</code> 统一推送，避免函数实例直接持有长连接。</li><li>失败重试由队列控制，重试前先加载 checkpoint，继续执行而非整轮重跑。</li></ol><p>在演进路径中的位置：</p><ol><li>它依赖 Durable Execution 提供可恢复状态。</li><li>它不要求 Actor 拆分，适合作为高并发阶段的低改造增量方案。</li><li>当单 loop 内部并发仍成瓶颈，再演进到 Actor Model。</li></ol><h1 id="Actor-Model（优化Agent-Service的可扩缩容性）"><a href="#Actor-Model（优化Agent-Service的可扩缩容性）" class="headerlink" title="Actor Model（优化Agent Service的可扩缩容性）"></a>Actor Model（优化Agent Service的可扩缩容性）</h1><p>当 <code>Stateless Agent Service</code> 进入高并发、多工具、长链路阶段，单体 loop 的主要瓶颈是调度耦合和故障扩散。<br>Actor Model 是下一阶段的实现方式：将 loop 拆成可独立扩缩容的执行单元。</p><p>推荐的最小 Actor 拆分：</p><ol><li><code>Session Actor</code>：会话入口，维护 turn 生命周期。</li><li><code>Planner Actor</code>：生成步骤计划与重规划。</li><li><code>Model Actor</code>：封装模型调用策略（路由、重试、限流）。</li><li><code>Tool Actor</code>：执行工具调用（可按工具类型分片）。</li><li><code>State Actor</code>：集中处理 checkpoint、effect log、恢复逻辑。</li></ol><pre><code class=" mermaid">flowchart LR    SA[Session Actor]    PA[Planner Actor]    MA[Model Actor]    TA[Tool Actor]    STA[State Actor]    BUS[(Message Bus)]    SA --&gt; BUS    PA --&gt; BUS    MA --&gt; BUS    TA --&gt; BUS    STA --&gt; BUS    BUS --&gt; SA    BUS --&gt; PA    BUS --&gt; MA    BUS --&gt; TA    BUS --&gt; STA</code></pre><p>与前一节的关系：</p><ol><li>Durable Execution 依赖 <code>State Actor</code> 统一写入恢复点。</li><li>Sandbox 调度可以下沉到 <code>Tool Actor</code>，实现按工具类型弹性扩缩容。</li><li>故障隔离更细：单个 Tool Actor 失败不会直接拖垮整个会话调度。</li></ol><p>代价是消息一致性、乱序处理和链路追踪复杂度上升。<br>建议顺序是：先稳定单体 Stateless loop，再按瓶颈引入 Actor 化拆分。</p><h1 id="可观测性"><a href="#可观测性" class="headerlink" title="可观测性"></a>可观测性</h1><p>可观测性目标不是“记录更多日志”，而是回答三个问题：慢在哪里、错在哪里、哪类任务不稳定。</p><pre><code class=" mermaid">flowchart LR    APP[Agent Service]    SB[Sandbox]    OBS[Telemetry Pipeline]    LOG[(Logs)]    MET[(Metrics)]    TR[(Traces)]    UI[Ops Dashboard]    APP --&gt; OBS    SB --&gt; OBS    OBS --&gt; LOG    OBS --&gt; MET    OBS --&gt; TR    LOG --&gt; UI    MET --&gt; UI    TR --&gt; UI</code></pre><p>建议指标分两层：</p><ol><li>LLM 层：QPS、TTFT、P95 延迟、错误率、token 成本。</li><li>Tool 层：调用次数、成功率、执行时长、超时率。</li></ol><p>Trace 结构建议绑定 <code>session_id + turn_id + step_id</code>，以便还原完整 trajectory。</p><h2 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h2><p>有了 Trace 和可恢复执行能力后，可以把线上真实任务沉淀为 benchmark 回放集。</p><pre><code class=" mermaid">flowchart LR    TR[(Trace Store)]    SEL[Scenario Selector]    REP[Replay Runner]    EVAL[Scoring/Eval]    RPT[Model Report]    TR --&gt; SEL --&gt; REP --&gt; EVAL --&gt; RPT</code></pre><p>评测重点不只看“是否成功”，还要看：</p><ol><li>端到端时延与成本。</li><li>工具调用效率和失败恢复能力。</li><li>不同模型在同一任务族上的稳定性差异。</li></ol>]]></content>
    
    
    <categories>
      
      <category>AI Engineering</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Agent</tag>
      
      <tag>CloudAgent</tag>
      
      <tag>Engineering</tag>
      
      <tag>实现细节</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>聊聊云上Agent的架构设计</title>
    <link href="/2026/02/17/2026-02-17-architecture-of-cloud-agent/"/>
    <url>/2026/02/17/2026-02-17-architecture-of-cloud-agent/</url>
    
    <content type="html"><![CDATA[<blockquote><p>总结一下Agent的发展历程和架构演进，聊聊几个抽象概念，最后说说我觉得未来Agent架构的一个可能方向。</p></blockquote><h1 id="Chatbot"><a href="#Chatbot" class="headerlink" title="Chatbot"></a>Chatbot</h1><p>最开始出现的是Chatbot应用。Chatbot 是一个状态系统。每轮请求执行四个步骤：读取上下文、构造 prompt、调用模型、写回状态。系统设计的重点不在模型接口，而在状态组织方式。</p><p>会话状态建议分为两层：</p><ul><li><strong>显示层（Display Layer）</strong>：保存原始事件流，面向 UI 展示、回放与审计。</li><li><strong>业务层（Business Layer）</strong>：保存推理就绪上下文，面向 prompt 构造与成本控制。</li></ul><p>该分层支持一条工程约束：业务层允许静默压缩与摘要，显示层保持完整历史，不做语义改写。</p><h2 id="1-分层职责图"><a href="#1-分层职责图" class="headerlink" title="1) 分层职责图"></a>1) 分层职责图</h2><pre><code class=" mermaid">flowchart LR    UI[Display Layer&lt;br/&gt;raw events / replay / audit]    BL[Business Layer&lt;br/&gt;processed context / prompt-ready]    UI -. isolated goals .- BL</code></pre><p>这张图定义职责边界。两层的输入输出、服务对象、演化策略均不同。混用单层存储会产生三类问题：</p><ol><li>prompt 长度随轮次线性增长，推理成本和时延同步上升。</li><li>展示格式、系统指令、工具中间消息进入模型输入，降低上下文信噪比。</li><li>压缩策略直接作用于用户可见历史，破坏回放一致性与审计可读性。</li></ol><h2 id="2-写路径图"><a href="#2-写路径图" class="headerlink" title="2) 写路径图"></a>2) 写路径图</h2><pre><code class=" mermaid">flowchart LR    U[User] --&gt; API[Chat API] --&gt; SVC[Conversation Service]    SVC --&gt; CP[Context Processor]    CP --&gt; LLM[LLM Inference]    LLM --&gt; SVC    SVC --&gt; DDB[(Display History Store)]    SVC --&gt; BDB[(Business Context Store)]</code></pre><p>写路径采用双写：同一轮请求同时更新显示层与业务层。</p><ul><li><code>Display History Store</code> 记录 raw user&#x2F;assistant events。</li><li><code>Business Context Store</code> 记录 processed context。</li></ul><p><code>Context Processor</code> 负责从“展示消息”生成“推理上下文”，常见操作为：</p><ul><li>truncate（窗口裁剪）</li><li>compress（信息压缩）</li><li>summarize（阶段摘要）</li><li>metadata filtering（移除展示噪声字段）</li></ul><h2 id="3-推理读路径图"><a href="#3-推理读路径图" class="headerlink" title="3) 推理读路径图"></a>3) 推理读路径图</h2><pre><code class=" mermaid">flowchart LR    SVC[Conversation Service] --&gt;|load prompt context| BDB[(Business Context Store)]    BDB --&gt; SVC    SVC --&gt; LLM[LLM Inference]</code></pre><p>推理路径只读取业务层。该约束保证三点：</p><ul><li>输入长度可预测。</li><li>关键语义可延续。</li><li>prompt 结构可标准化。</li></ul><h2 id="4-显示回放图"><a href="#4-显示回放图" class="headerlink" title="4) 显示回放图"></a>4) 显示回放图</h2><pre><code class=" mermaid">flowchart LR    UI[Chat UI] --&gt; API[Chat API]    API --&gt; DDB[(Display History Store)]    DDB --&gt; API --&gt; UI</code></pre><p>展示路径只读取显示层。UI 获取的是原始事件轨迹，不依赖推理优化后的上下文快照。该路径为回放、排障、审计提供稳定数据源。</p><h2 id="5-Context-Engineering"><a href="#5-Context-Engineering" class="headerlink" title="5) Context Engineering"></a>5) Context Engineering</h2><p>在 Chatbot 阶段，Context Engineering 关注两个问题：</p><ol><li><strong>上下文压缩</strong>：在 token budget 固定的前提下，保留任务相关信息。</li><li><strong>用户记忆 &#x2F; 偏好</strong>：在跨轮次交互中维持行为一致性。</li></ol><p>这一阶段的重点是定义问题边界和评估标准，不在于引入复杂机制。</p><p><strong>结论</strong></p><ul><li>Chatbot 架构已经形成显示层与业务层的初步分层：前者负责事件回放与审计，后者负责推理上下文组织。</li><li>Chatbot 阶段已经出现上下文管理问题：上下文压缩与用户记忆&#x2F;偏好管理成为后续架构演进的核心输入。</li></ul><h1 id="Agent-in-VM"><a href="#Agent-in-VM" class="headerlink" title="Agent in VM"></a>Agent in VM</h1><p>第二种形态是 <strong>Agent 与用户共处同一台 VM</strong>。与 Chatbot 阶段相比，这一形态把“对话系统”扩展为“执行系统”：Agent 不再只组织上下文，还直接操作 Shell、文件系统、浏览器和桌面环境。</p><p>这个形态的核心变化是：</p><ul><li>执行环境从 API 工具调用，转为真实操作系统环境。</li><li>状态边界从会话消息，扩展为 VM 全状态（文件、进程、浏览器、环境变量）。</li><li>用户可通过 VNC&#x2F;SSH 与 Agent 并行操作同一环境。</li></ul><h2 id="1-形态定义与架构边界"><a href="#1-形态定义与架构边界" class="headerlink" title="1) 形态定义与架构边界"></a>1) 形态定义与架构边界</h2><pre><code class=" mermaid">flowchart LR    U[User] --&gt;|VNC/SSH| VM[User VM]    WEB[Web Chat] --&gt; AG[Agent Process in VM]    subgraph VM[User VM]        AG        SH[Shell]        FS[(Filesystem)]        BR[Browser]        DE[Desktop]    end    AG --&gt; SH    AG --&gt; FS    AG --&gt; BR    DE --&gt; SH    DE --&gt; BR</code></pre><p>该架构的边界很直接：Agent 和用户共享同一运行时。系统一致性高，系统隔离弱。</p><h2 id="2-运行时交互流"><a href="#2-运行时交互流" class="headerlink" title="2) 运行时交互流"></a>2) 运行时交互流</h2><pre><code class=" mermaid">sequenceDiagram    participant User as User(VNC/SSH)    participant Web as Web Chat    participant Agent as Agent(in VM)    participant LLM as LLM API    participant Shell as Shell    participant FS as Filesystem    participant Browser as Browser    User-&gt;&gt;Shell: direct command    Shell-&gt;&gt;FS: read/write    Web-&gt;&gt;Agent: task request    Agent-&gt;&gt;LLM: planning + tool call    LLM--&gt;&gt;Agent: action    Agent-&gt;&gt;Shell: execute command    Shell-&gt;&gt;FS: read/write    Agent-&gt;&gt;Browser: open/click/type    Agent--&gt;&gt;Web: result    User-&gt;&gt;FS: inspect changes immediately</code></pre><p>这一形态的优势在于“可观察 + 可干预”：用户可以实时看到 Agent 的执行结果，也可以随时接管。</p><h2 id="3-状态持久化：从会话状态到-VM-快照"><a href="#3-状态持久化：从会话状态到-VM-快照" class="headerlink" title="3) 状态持久化：从会话状态到 VM 快照"></a>3) 状态持久化：从会话状态到 VM 快照</h2><p>Chatbot 阶段主要持久化消息与上下文。Agent-in-VM 阶段需要持久化完整环境状态，通常依赖 VM Snapshot。</p><pre><code class=" mermaid">sequenceDiagram    participant P as Platform    participant VM as VM    participant S as Snapshot Storage    P-&gt;&gt;VM: pause    VM--&gt;&gt;P: frozen    P-&gt;&gt;S: save memory snapshot    P-&gt;&gt;S: save disk snapshot    P-&gt;&gt;VM: terminate    P-&gt;&gt;S: load snapshots    S--&gt;&gt;P: memory + disk    P-&gt;&gt;VM: restore    VM--&gt;&gt;P: resumed</code></pre><p>快照带来的工程收益是“连续执行上下文”：文件、进程、浏览器状态可以跨会话恢复。典型场景是开发服务器和调试现场的延续。</p><h2 id="4-主要约束与风险"><a href="#4-主要约束与风险" class="headerlink" title="4) 主要约束与风险"></a>4) 主要约束与风险</h2><p>这个形态在通用性上很强，但成本与安全边界会前置成为架构问题：</p><ul><li><strong>成本约束</strong>：每用户独占 VM，空闲成本高，扩展效率低于共享计算架构。</li><li><strong>调度约束</strong>：VM 启停与快照恢复时延高于无状态容器调度。</li><li><strong>安全约束</strong>：Agent 进程与用户共域，密钥管理、权限边界、篡改防护更复杂。</li><li><strong>持久化风险</strong>：恶意进程或敏感凭证可能通过快照跨会话保留。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ul><li>Agent in VM 形态把系统能力从“对话组织”推进到“环境执行”，获得了强交互、强观察、强持久化能力。</li><li>这一形态同时引入了新的主问题：多租户安全边界、VM 成本模型与快照生命周期治理。</li></ul><h1 id="Agentic-LLM-API"><a href="#Agentic-LLM-API" class="headerlink" title="Agentic LLM API"></a>Agentic LLM API</h1><p>第三种形态可以定义为 <strong>Agentic LLM API</strong>：交互入口仍是 Chatbot，但执行能力已经升级为 Agent，并由模型 API 厂商托管执行环境。与 Agent in VM 的区别在于，用户不直接进入执行环境；与第一阶段 Chatbot 的区别在于，系统不仅维护消息状态，还维护可执行环境状态。</p><p>这一形态的目标是把“对话编排”与“环境执行”做成一条端到端链路：</p><ul><li>对用户暴露稳定的聊天接口。</li><li>对系统内部暴露可控的工具与执行环境。</li><li>在 token 成本、延迟与可观察性之间取得工程平衡。</li></ul><h2 id="1-形态定义与系统边界"><a href="#1-形态定义与系统边界" class="headerlink" title="1) 形态定义与系统边界"></a>1) 形态定义与系统边界</h2><pre><code class=" mermaid">flowchart LR    U[User]    CHAT[Chat Interface]    AGENT[Agent Orchestrator]    MEM[(Session &amp; Memory Store)]    CE[Code Execution Runtime]    TOOLS[Internal/External Tools]    U --&gt; CHAT --&gt; AGENT    AGENT &lt;--&gt; MEM    AGENT --&gt; CE    CE --&gt; TOOLS    TOOLS --&gt; CE --&gt; AGENT --&gt; CHAT --&gt; U</code></pre><p>边界定义：</p><ul><li><strong>Control Plane</strong>：会话、记忆、策略、路由。</li><li><strong>Execution Plane</strong>：代码执行容器或沙箱运行时。</li><li><strong>Tool Plane</strong>：数据库、检索、业务 API、浏览器等外部能力。</li></ul><h2 id="2-执行流：从直接工具调用到程序化工具调用"><a href="#2-执行流：从直接工具调用到程序化工具调用" class="headerlink" title="2) 执行流：从直接工具调用到程序化工具调用"></a>2) 执行流：从直接工具调用到程序化工具调用</h2><p>Agentic Chatbot 的关键变化，是把多步工具调用下沉到执行环境中，以减少模型轮次开销。典型实现是 Programmatic Tool Calling（PTC）模式：模型生成可执行代码，在运行时中循环调用工具、过滤中间结果，再回传高价值输出。</p><pre><code class=" mermaid">sequenceDiagram    participant U as User    participant A as Agent Orchestrator    participant L as LLM    participant R as Execution Runtime    participant T as Tool/API    U-&gt;&gt;A: task    A-&gt;&gt;L: planning prompt    L--&gt;&gt;A: code-oriented action    A-&gt;&gt;R: run code block    loop in runtime        R-&gt;&gt;T: call tool        T--&gt;&gt;R: raw result        R-&gt;&gt;R: filter/aggregate/branch    end    R--&gt;&gt;A: compact result    A-&gt;&gt;L: final reasoning    L--&gt;&gt;A: answer    A--&gt;&gt;U: response</code></pre><p>该流程的工程意义是：中间数据处理不进入主上下文窗口，模型只消费压缩后的关键结果。</p><h2 id="3-状态模型：消息状态-环境状态-运行状态"><a href="#3-状态模型：消息状态-环境状态-运行状态" class="headerlink" title="3) 状态模型：消息状态 + 环境状态 + 运行状态"></a>3) 状态模型：消息状态 + 环境状态 + 运行状态</h2><p>Chatbot 阶段主要管理消息状态；Agentic Chatbot 需要三类状态并存。</p><pre><code class=" mermaid">flowchart LR    M[(Message State)]    E[(Environment State)]    R[(Run State)]    M --&gt; C[Context Builder]    E --&gt; C    R --&gt; C    C --&gt; P[Prompt/Action Decision]</code></pre><ul><li><strong>Message State</strong>：对话历史、摘要、用户偏好。</li><li><strong>Environment State</strong>：容器 ID、文件系统变更、工具上下文。</li><li><strong>Run State</strong>：任务阶段、工具调用链、重试与超时信息。</li></ul><p>系统可恢复性依赖于这三类状态的一致性，而不是单一会话历史。</p><h2 id="4-主要约束"><a href="#4-主要约束" class="headerlink" title="4) 主要约束"></a>4) 主要约束</h2><p>Agentic LLM API 的主要工程约束集中在五点：</p><ul><li><strong>安全约束</strong>：执行环境必须隔离，工具调用需要 caller 权限边界与 allowlist。</li><li><strong>时效约束</strong>：执行容器存在 TTL，跨步调用需要超时恢复和幂等设计。</li><li><strong>观测约束</strong>：需要保留 action trace、tool I&#x2F;O、版本化 prompt 以支持回放。</li><li><strong>成本约束</strong>：执行时长、容器复用、上下文压缩共同决定单位任务成本。</li><li><strong>供应商锁定约束</strong>：执行环境上下文（container state、intermediate artifacts、tool-call runtime state）主要保存在 API 厂商侧，迁移到其他模型供应商时，状态可移植性与行为一致性难以保证。</li></ul><p>关于供应商锁定，这一形态有两个具体后果：</p><ol><li><strong>状态不可携带</strong>：迁移供应商时，通常只能带走消息历史，难以带走运行时上下文。</li><li><strong>行为不可等价</strong>：即使接口相似，不同厂商在执行容器、超时策略、工具调用协议上的差异会导致任务行为漂移。</li></ol><p>锁定面的核心不在 API schema，而在运行时语义：</p><pre><code class=" mermaid">flowchart TB    subgraph HighPortability[High Portability]        M[Message History&lt;br/&gt;input/output transcripts]    end    subgraph MediumPortability[Medium Portability]        T[Tool Contracts&lt;br/&gt;name/schema/params]    end    subgraph LowPortability[Low Portability]        R[Runtime State&lt;br/&gt;container/session/artifacts]        S[Execution Semantics&lt;br/&gt;timeout/retry/caller policy]    end    M --&gt; T --&gt; R    T --&gt; S</code></pre><p>可迁移性通常呈分层下降：消息层 &gt; 工具协议层 &gt; 运行时状态层。</p><blockquote><p>这里和很多厂商（OpenAI &amp; Gemini）在API中隐藏自己的thinking过程不同，环境状态的丢失使得迁移供应商是不可能的，而非之前的“丢失一些中间思考”。</p></blockquote><h2 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h2><p>这对于模型厂商获取数据和用户锁定都有积极意义，也为第一方应用提供了非常强大的能力，但对于开发者来说，几乎不可能选择这类方案，其供应商锁定和不透明问题带来的风险太高了。</p><h1 id="Next-Agent-Mounts-Environment"><a href="#Next-Agent-Mounts-Environment" class="headerlink" title="Next: Agent Mounts Environment"></a>Next: Agent Mounts Environment</h1><p>Agent in VM 形态虽然提供了强交互能力，但存在两个核心问题：一是每用户独占 VM 的成本模式导致空闲成本高；二是 Agent 与用户共域，API Keys 与内部逻辑暴露在 VM 环境，难以安全整合私有服务。</p><p><strong>Agent Mounts Environment</strong> 将 Agent 与执行环境解耦：Agent 仍运行在平台侧（可访问私有服务、密钥隔离），但将执行环境降级为一个”挂载点”。VM 内仅运行轻量级 Executor，负责接收 Agent 的指令（Shell 命令、文件操作、浏览器控制）并反馈结果。</p><pre><code class=" mermaid">flowchart LR    subgraph Platform[&quot;Platform (Secure Zone)&quot;]        Agent[&quot;Agent Service&quot;]        PrivateAPI[&quot;Private Services&quot;]    end    subgraph VM[&quot;User VM (Mounted Environment)&quot;]        Executor[&quot;Executor&quot;]        Shell[&quot;Shell&quot;]        FS[&quot;Filesystem&quot;]    end    Agent &lt;--&gt;|&quot;SSH/CDP&quot;| Executor    Agent &lt;--&gt;|&quot;API&quot;| PrivateAPI    Executor --&gt; Shell    Executor --&gt; FS</code></pre><h2 id="好处"><a href="#好处" class="headerlink" title="好处"></a>好处</h2><p><strong>1. 安全隔离</strong>：Agent 与用户环境物理分离，密钥与内部逻辑不暴露在 VM 中。Agent 可安全访问私有数据库、内部 API，而无需在 VM 中存储凭证。</p><p><strong>2. 成本优化</strong>：Agent 进程不与用户 VM 绑定，可跨多用户复用（或采用共享计算资源池）。VM 仅作为执行容器，不需要持续为每用户保留计算资源。</p><p><strong>3. 权限边界清晰</strong>：Executor 可施加细粒度权限控制：允许的工具集、可访问的文件路径、外部 API 调用权限均由平台策略决定，用户无法绕过。</p><p><strong>4. 与 Agentic LLM API 兼容</strong>：该架构天然支持多模型厂商，Agent 逻辑独立于执行环境实现，迁移或切换 LLM 供应商时，运行时语义保持一致。</p><h2 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h2><p><strong>1. 通信协议复杂性</strong>：Agent 需要通过 SSH、SFTP、CDP Over SSH Tunnel 等多种协议与 VM 通信。每种协议的超时、重试、错误恢复策略都需要精心设计。协议层的不稳定会导致任务中断或重复执行。</p><p><strong>2. 状态一致性困难</strong>：Agent 侧的任务状态与 VM 侧的执行状态可能不一致（如网络分区、超时导致的半成功操作）。需要设计操作幂等性、事务语义与恢复协议来保证最终一致性。</p><p><strong>3. VM 快照与 Git 同步</strong>：若采用 VM 快照持久化完整运行时状态，需要在恢复后重新同步 Git 状态，否则磁盘内容与 Remote 不一致。这引入了额外的状态管理层。</p><h2 id="结论-2"><a href="#结论-2" class="headerlink" title="结论"></a>结论</h2><p>Agent Mounts Environment 是 Agent in VM 与 Agentic LLM API 之间的中间态：保留了本地执行环境的完整可观察性和持久状态能力，同时恢复了平台侧的安全边界与成本效率。代价是需要投入工程力量处理分布式系统的一致性问题。</p><h1 id="Bonus-Durable-Execution"><a href="#Bonus-Durable-Execution" class="headerlink" title="Bonus: Durable Execution"></a>Bonus: Durable Execution</h1><p>Agentic Loop 中的执行容易失败：网络中断、超时、工具错误、模型幻觉都可能中断任务。要支持重试与恢复，需要把任务状态分层管理，而不是盲目地快照整个 VM 或重新执行任务。</p><p>关键是认识到不同类型的状态有不同的恢复语义：</p><h2 id="1-上下文（Context）"><a href="#1-上下文（Context）" class="headerlink" title="1) 上下文（Context）"></a>1) 上下文（Context）</h2><p><strong>定义</strong>：LLM 模型推理所需的信息，包括prompt、当前目标、已执行步骤。</p><p><strong>恢复策略</strong>：上下文应完全可重建，无需从快照恢复。重试时重新加载上一步的结果，让模型基于完整的执行历史做出新的决策。</p><p><strong>为什么</strong>：即使模型上一次的决策有误，完整的history 能让它在第二次重试中做出不同的选择。</p><pre><code class=" mermaid">sequenceDiagram    participant A as Agent    participant M as Message Store    participant L as LLM    A-&gt;&gt;M: Load history + last result    M--&gt;&gt;A: [step1, step2, step3: failed]    A-&gt;&gt;L: Continue with full context    L--&gt;&gt;A: New action based on failure</code></pre><h2 id="2-消息状态（Message-State）"><a href="#2-消息状态（Message-State）" class="headerlink" title="2) 消息状态（Message State）"></a>2) 消息状态（Message State）</h2><p><strong>定义</strong>：对话历史、用户请求、AI 响应。</p><p><strong>恢复策略</strong>：消息状态应仅追加（append-only），支持版本化重试。一次重试对应一条新分支，记录”第一次尝试失败”与”第二次重试成功”的完整轨迹。</p><p><strong>存储方案</strong>：</p><pre><code class=" mermaid">flowchart LR    Root[&quot;Message 1: user request&quot;]    Exec1[&quot;Message 2a: Agent planning (attempt 1)&quot;]    Exec2[&quot;Message 2b: Agent planning (attempt 2)&quot;]    Result1[&quot;Message 3a: Tool result FAILED&quot;]    Result2[&quot;Message 3b: Tool result OK&quot;]    Root --&gt; Exec1    Root --&gt; Exec2    Exec1 --&gt; Result1    Exec2 --&gt; Result2</code></pre><p>通过消息树（而非单线性链表）记录重试分支，支持审计与故障分析。</p><h2 id="3-文件系统状态（Filesystem-State）"><a href="#3-文件系统状态（Filesystem-State）" class="headerlink" title="3) 文件系统状态（Filesystem State）"></a>3) 文件系统状态（Filesystem State）</h2><p><strong>定义</strong>：VM 或容器内的文件、目录、权限。</p><p><strong>恢复策略</strong>：文件系统状态通过 git commit 或 VM 快照持久化。重试时的关键决策是：<strong>是否需要恢复到操作前的状态还是基于当前状态重新操作</strong>。</p><p><strong>两种模式</strong>：</p><p><strong>模式 A：Ephemeral + Rollback（容器&#x2F;Sandbox 方案）</strong></p><ul><li>每次工具调用在隔离容器内执行</li><li>失败时容器销毁，文件系统回滚</li><li>下一次重试从 Git Checkout 状态开始</li></ul><pre><code class=" mermaid">sequenceDiagram    participant A as Agent    participant C1 as Container (attempt 1)    participant C2 as Container (attempt 2)    participant FS as Shared FS (Git managed)    FS--&gt;&gt;C1: mount current HEAD    A-&gt;&gt;C1: npm install    C1-&gt;&gt;FS: write XXX (not committed)    Note over C1: Fails, container destroyed    FS--&gt;&gt;C2: mount current HEAD (clean state)    A-&gt;&gt;C2: npm install (retry)    C2-&gt;&gt;FS: write YYY</code></pre><p>优点：简单、可预测。缺点：无法利用前一次的中间成果（如部分安装的依赖）。</p><p><strong>模式 B：Persistent VM + Checkpoint（VM 快照方案）</strong></p><ul><li>使用 Firecracker Snapshot 保存完整运行时状态</li><li>失败时从快照恢复，保留内存中的进程、缓存</li><li>需要额外的 checkpoint 机制记录”哪些文件修改可以撤销”</li></ul><pre><code class=" mermaid">sequenceDiagram    participant A as Agent    participant VM as VM    participant S as Snapshot    A-&gt;&gt;VM: Save checkpoint (git state + memory snapshot)    S--&gt;&gt;VM: checkpoint_id_v1    A-&gt;&gt;VM: npm install    VM-&gt;&gt;VM: partial install (process in memory)    Note over VM: Fails    A-&gt;&gt;S: Restore checkpoint_id_v1    S--&gt;&gt;VM: Memory + disk restored    A-&gt;&gt;VM: npm install (retry, hot cache)</code></pre><p>优点：快速恢复、热启动。缺点：快照本身占用存储空间，恢复涉及内存一致性问题。</p><h2 id="4-外部资源状态（External-Resource-State）"><a href="#4-外部资源状态（External-Resource-State）" class="headerlink" title="4) 外部资源状态（External Resource State）"></a>4) 外部资源状态（External Resource State）</h2><p><strong>定义</strong>：API 调用、数据库事务、第三方服务的执行结果。</p><p><strong>恢复策略</strong>：外部资源操作的可恢复性取决于操作是否幂等。</p><p><strong>分类</strong>：</p><table><thead><tr><th>操作类型</th><th>幂等性</th><th>恢复策略</th></tr></thead><tbody><tr><td>Read DB</td><td>✓</td><td>直接重试</td></tr><tr><td>GET API</td><td>✓</td><td>直接重试</td></tr><tr><td>Create resource</td><td>✗</td><td>需要 idempotency key，防止重复创建</td></tr><tr><td>Update resource</td><td>✓&#x2F;✗</td><td>需要检验当前状态是否已更新</td></tr><tr><td>Delete resource</td><td>~</td><td>重试返回 404，需要 idempotent delete semantic</td></tr><tr><td>Payment &#x2F; Charge</td><td>✗</td><td>必须通过事务 ID 检查是否已执行</td></tr></tbody></table><p><strong>实现模式</strong>：Idempotency Key</p><pre><code class=" mermaid">sequenceDiagram    participant A as Agent    participant API as External API    participant Cache as Idempotency Cache    A-&gt;&gt;Cache: Lookup idempotency_key = &quot;task_step_3&quot;    Cache--&gt;&gt;A: Not found    A-&gt;&gt;API: POST /charge (idempotency_key: &quot;task_step_3&quot;, amount: 100)    API-&gt;&gt;Cache: Save result (charged: true)    Cache--&gt;&gt;API: OK    API--&gt;&gt;A: success    Note over A: Network timeout, retry    A-&gt;&gt;Cache: Lookup idempotency_key = &quot;task_step_3&quot;    Cache--&gt;&gt;A: Found (charged: true)    A-&gt;&gt;A: Return cached result without re-charging</code></pre><p>系统需要为每个工具调用分配全局唯一的 idempotency_key，存储每次调用的结果。重试时先查询缓存，避免重复操作。</p><h2 id="综合设计"><a href="#综合设计" class="headerlink" title="综合设计"></a>综合设计</h2><p>完整的 Durable Execution 系统需要四层状态管理：</p><pre><code class=" mermaid">flowchart TB    Context[&quot;Context Layer&lt;br/&gt;(Stateless, computed from Message State)&quot;]    Message[&quot;Message State&lt;br/&gt;(Append-only, versioned history)&quot;]    FileSystem[&quot;Filesystem State&lt;br/&gt;(Git checkpoint or VM snapshot)&quot;]    External[&quot;External State&lt;br/&gt;(Idempotency cache + verification)&quot;]    Message --&gt; Context    Message --&gt; FileSystem    Message --&gt; External    Context -.-&gt; Recovery[&quot;Recovery via Context + Message Replay&quot;]</code></pre><p>在故障发生时，恢复顺序为：</p><ol><li>检查外部资源状态（是否已执行）→ 如有缓存结果则直接返回</li><li>检查文件系统状态（Snapshot 或 Git HEAD）→ 重建执行环境</li><li>加载消息历史 → 重建 LLM 上下文</li><li>重新执行失败的步骤</li></ol><h2 id="结论-3"><a href="#结论-3" class="headerlink" title="结论"></a>结论</h2><p>Durable Execution 的核心是认识到：<strong>系统的可恢复性不源于单一快照，而源于分层的状态管理与操作的幂等性设计</strong>。不同类型的状态有不同的持久化需求与恢复成本，混为一谈会导致过度的快照成本或无法恢复的局面。</p>]]></content>
    
    
    <categories>
      
      <category>AI Engineering</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Agent</tag>
      
      <tag>架构设计</tag>
      
      <tag>CloudAgent</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI Native 组织思考</title>
    <link href="/2026/02/08/2026-02-08-AI-Native-Organization-Thoughts/"/>
    <url>/2026/02/08/2026-02-08-AI-Native-Organization-Thoughts/</url>
    
    <content type="html"><![CDATA[<blockquote><p><strong>前言：</strong> 聊到了 AI Native 组织的形态，记录几个当下的直觉。</p></blockquote><h2 id="1-组织即灵魂"><a href="#1-组织即灵魂" class="headerlink" title="1. 组织即灵魂"></a>1. 组织即灵魂</h2><p>同样是基于 LLM，OpenAI 做出来的产品像大众工具，DeepMind 做出来的像科学仪器。产品不再是工业流水线上的标准化件，而是组织认知的直接投射。</p><p><strong>直觉：</strong> 不同的组织“性格”，决定了 AI 产品的“灵魂”。</p><h2 id="2-网络式共振"><a href="#2-网络式共振" class="headerlink" title="2. 网络式共振"></a>2. 网络式共振</h2><p>在 AI 时代，传统的层级式指令管理正在失效。因为创新的源头往往在一线研发的探索中涌现。</p><p><strong>直觉：</strong> 未来的组织更像是一个“超级兴趣小组”。大家因为一个远期目标聚在一起，通过高频的网络式协作（你读一篇 Paper，我试一个 Demo），把个体的认知拉齐、拉高。最终那个超越时代的产品，是在整个团队<strong>高认知水位</strong>上自然浮现的结果。</p><h2 id="3-商业化是真实世界的-RLHF"><a href="#3-商业化是真实世界的-RLHF" class="headerlink" title="3. 商业化是真实世界的 RLHF"></a>3. 商业化是真实世界的 RLHF</h2><p>商业化不仅仅是为了生存，它更是一个<strong>高价值信号过滤器</strong>。</p><p>免费用户的反馈往往是“好玩”（Novelty），付费用户的反馈往往是“没用”或“不准”（Utility）。只有那些愿意付费的用户反馈，才是最真实的 Reward Model，迫使团队直面真实世界的复杂性（Corner Cases），把“玩具”打磨成“工具”。</p><p><strong>直觉：</strong> 商业化把“用户反馈”转化为了高质量的“训练数据”。</p><h2 id="注脚：组织环境即-RL-Environment"><a href="#注脚：组织环境即-RL-Environment" class="headerlink" title="注脚：组织环境即 RL Environment"></a>注脚：组织环境即 RL Environment</h2><p>如果我们把组织看作一个强化学习（RL）的环境：</p><ul><li><strong>Agent：</strong> 每一个团队成员。</li><li><strong>Action：</strong> 每天的探索、决策、代码、讨论。</li><li><strong>Reward：</strong> 组织鼓励什么？是快速试错给正反馈，还是按部就班给正反馈？</li><li><strong>State：</strong> 整个团队当前的认知水位、技术栈、氛围。</li></ul><p>如果不精心设计这个 Environment 的 Reward Function，你就得不到那种自驱、涌现式的创新。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
      <tag>AI Native</tag>
      
      <tag>组织思考</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI 辅助开发下，如何保持项目一致性</title>
    <link href="/2026/02/02/2026-02-02-AI-Assisted-Development-Consistency/"/>
    <url>/2026/02/02/2026-02-02-AI-Assisted-Development-Consistency/</url>
    
    <content type="html"><![CDATA[<p>AI 正在改变软件开发的方式。Cursor、Claude Code、Copilot 这些工具让编码速度提升了数倍，但也带来了新的挑战：<strong>代码写得太快，理解跟不上，项目很容易腐化</strong>。</p><p>这篇文章分享我在 AI 辅助开发中保持项目一致性的一些实践。</p><h2 id="核心洞察：瓶颈变了"><a href="#核心洞察：瓶颈变了" class="headerlink" title="核心洞察：瓶颈变了"></a>核心洞察：瓶颈变了</h2><p>传统软件开发的瓶颈是编码。设计完成后，大量时间花在实现上。</p><pre><code class=" mermaid">flowchart LR    A[设计] --&gt; B[编码]    B --&gt; C[Review]    C --&gt; D[修改]    D --&gt; B</code></pre><p>AI 辅助开发打破了这个瓶颈。编码变得很快，但新的问题出现了：</p><pre><code class=" mermaid">flowchart LR    A[设计] --&gt; B[AI编码]    B --&gt; C[代码量暴增]    C --&gt; D[理解跟不上]    D --&gt; E[架构腐化]    E --&gt; F[更多patch]    F --&gt; E</code></pre><p><strong>编码不再是瓶颈，理解和架构才是。</strong></p><h2 id="方法论：控制点上移"><a href="#方法论：控制点上移" class="headerlink" title="方法论：控制点上移"></a>方法论：控制点上移</h2><p>要打破这个负循环，关键是把控制点从”代码”上移到”架构”。</p><pre><code class=" mermaid">flowchart TB    subgraph Control[控制层]        A[Spec文档]        B[Owner审阅]    end    subgraph Constraint[约束层]        C[agents.md]        D[类型系统]        E[项目规则]    end    subgraph Execution[执行层]        F[AI生成代码]        G[Prototype]        H[生产代码]    end    A --&gt; B    B --&gt; C    B --&gt; D    B --&gt; E    C --&gt; F    D --&gt; F    E --&gt; F    F --&gt; G    G --&gt;|验证通过| H    G --&gt;|需要调整| A</code></pre><h3 id="1-Spec-文档驱动"><a href="#1-Spec-文档驱动" class="headerlink" title="1. Spec 文档驱动"></a>1. Spec 文档驱动</h3><p>把脑海里的架构变成和 AI 讨论的 spec 文档：</p><ul><li><strong>概要设计</strong>：系统边界、模块划分、核心流程</li><li><strong>架构图&#x2F;流程图</strong>：用 mermaid 或手绘，让 AI 理解上下文</li><li><strong>DDD 思路</strong>：bounded context、聚合、领域语言</li></ul><p>不需要细到接口签名。模块边界清晰，AI 就不会跨模块乱搞。</p><pre><code class=" mermaid">flowchart LR    A[脑海中的架构] --&gt; B[Spec文档]    B --&gt; C[与AI讨论]    C --&gt; D[AI生成代码]    D --&gt; E[代码符合架构]</code></pre><h3 id="2-自底向上构建"><a href="#2-自底向上构建" class="headerlink" title="2. 自底向上构建"></a>2. 自底向上构建</h3><p>很多人习惯自顶向下：先设计接口，再实现细节。但在 AI 辅助开发中，我发现<strong>自底向上更有效</strong>：</p><ul><li><strong>先做配置</strong>：配置是系统的”骨架”，定下来后 AI 生成的代码就有约束</li><li><strong>先定类型</strong>：类型系统是天然的约束，让 AI 在框里活动</li><li><strong>从不易变的开始</strong>：基础设施、工具函数、配置管理</li></ul><p>这样做的好处：</p><ul><li>更容易获得良好的抽象</li><li>更容易进行测试覆盖</li><li>AI 生成的代码有锚点，不会飘</li></ul><h3 id="3-约束前置"><a href="#3-约束前置" class="headerlink" title="3. 约束前置"></a>3. 约束前置</h3><p>与其事后 review 代码，不如事前约束 AI：</p><pre><code class=" mermaid">flowchart LR    subgraph Constraints[约束]        A[agents.md]        B[pyright]        C[ESLint]        D[项目规则]    end    E[AI] --&gt; Constraints    Constraints --&gt; F[符合规范的代码]</code></pre><ul><li><strong>agents.md &#x2F; AGENTS.md</strong>：写清楚项目的架构、约定、禁忌</li><li><strong>类型系统</strong>：pyright、TypeScript，静态分析是最好的约束</li><li><strong>项目规则</strong>：命名规范、目录结构、commit 格式</li></ul><p>AI 读了这些约束，生成的代码一致性会好很多。</p><h3 id="4-早期重构"><a href="#4-早期重构" class="headerlink" title="4. 早期重构"></a>4. 早期重构</h3><p>重构应该在<strong>最早的时候</strong>进行，而不是等代码堆积成山：</p><ul><li>代码量小，重构成本低</li><li>最容易利用 AI 的生成能力</li><li>最不容易受幻觉影响</li></ul><p>等到项目复杂了再重构，AI 会产生更多幻觉，因为它无法完全理解所有上下文。</p><h3 id="5-快速验证，慎重生产"><a href="#5-快速验证，慎重生产" class="headerlink" title="5. 快速验证，慎重生产"></a>5. 快速验证，慎重生产</h3><pre><code class=" mermaid">flowchart LR    A[想法] --&gt; B[Prototype]    B --&gt; C&#123;验证&#125;    C --&gt;|通过| D[生产化]    C --&gt;|失败| E[调整想法]    E --&gt; A    D --&gt; F[留重构空间]</code></pre><ul><li><strong>尽快 prototype</strong>：用 AI 快速验证想法</li><li><strong>慎重生产</strong>：验证通过后再决定是否生产化</li><li><strong>留重构空间</strong>：不要过早固化，保持灵活性</li></ul><h2 id="正反馈循环"><a href="#正反馈循环" class="headerlink" title="正反馈循环"></a>正反馈循环</h2><p>好的实践会形成正反馈：</p><pre><code class=" mermaid">flowchart LR    A[好架构] --&gt; B[AI生成正确代码]    B --&gt; C[省时间]    C --&gt; D[优化架构]    D --&gt; A</code></pre><p>差的实践会形成负反馈（要避免）：</p><pre><code class=" mermaid">flowchart LR    A[烂架构] --&gt; B[代码到处patch]    B --&gt; C[越来越乱]    C --&gt; D[没时间重构]    D --&gt; A</code></pre><h2 id="团队协作"><a href="#团队协作" class="headerlink" title="团队协作"></a>团队协作</h2><p>在多人协作中，一致性更重要：</p><ul><li><strong>项目有 Owner</strong>：Owner 审阅 spec，保持架构一致性</li><li><strong>Review spec，不只是 review 代码</strong>：比传统 code review 更高效</li><li><strong>AI review 辅助</strong>：让 AI 检查代码是否符合 spec</li></ul><pre><code class=" mermaid">flowchart TB    A[Owner] --&gt;|审阅| B[Spec]    B --&gt;|指导| C[开发者1]    B --&gt;|指导| D[开发者2]    C --&gt; E[代码]    D --&gt; E    E --&gt;|AI Review| F&#123;符合Spec&#125;    F --&gt;|是| G[合并]    F --&gt;|否| H[修改]    H --&gt; E</code></pre><h2 id="工程师角色转变"><a href="#工程师角色转变" class="headerlink" title="工程师角色转变"></a>工程师角色转变</h2><p>AI 辅助开发正在改变工程师的角色：</p><table><thead><tr><th>传统</th><th>AI 辅助</th></tr></thead><tbody><tr><td>写代码</td><td>设计架构</td></tr><tr><td>Debug</td><td>审阅 spec</td></tr><tr><td>重复劳动</td><td>创造性思考</td></tr></tbody></table><p>工程师的核心价值变成了：</p><ul><li><strong>架构抽象能力</strong>：基于当前理解设计合理的架构</li><li><strong>业务理解</strong>：对当前情况和未来进行抉择</li><li><strong>质量把控</strong>：确保 AI 生成的代码符合预期</li></ul><p>这本就是架构师的任务。有了 AI 辅助编码，我们可以有更多时间放在设计更容易维护的架构上，反过来又方便了 AI 编码，实现正反馈。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>AI 辅助开发的核心是<strong>人机协作</strong>：</p><ul><li><strong>人负责</strong>：架构决策、业务理解、质量把控</li><li><strong>AI 负责</strong>：快速实现、重复劳动、代码生成</li></ul><p>保持项目一致性的关键：</p><ol><li><strong>控制点上移</strong>：从代码到 spec</li><li><strong>约束前置</strong>：用规则和工具约束 AI</li><li><strong>早期重构</strong>：趁代码量小的时候</li><li><strong>快速验证</strong>：prototype 快，生产慢</li><li><strong>正反馈循环</strong>：好架构 → 好代码 → 更好架构</li></ol><p>AI 不会取代工程师，但会取代不会用 AI 的工程师。掌握 AI 辅助开发的方法论，才能在这个时代保持竞争力。</p>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>软件工程</tag>
      
      <tag>架构设计</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Agent 产品的软件腐化:一种新型技术债</title>
    <link href="/2026/02/01/2026-02-01-Agent-Software-Rot/"/>
    <url>/2026/02/01/2026-02-01-Agent-Software-Rot/</url>
    
    <content type="html"><![CDATA[<h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>最近和 AI 讨论了一个有意思的话题:在 AI Native 时代,Agent 产品存在一种独特的”软件腐化”——它不是传统意义上的代码腐化,而是发生在<strong>智能层面</strong>的劣化。</p><p>传统软件腐化讲的是代码层面:重复、耦合、复杂度上升。Agent 腐化是另一种东西——系统逐渐变得更蠢、更僵化、更难迭代,而且这种劣化在常规指标上往往不可见。</p><h2 id="一个典型场景"><a href="#一个典型场景" class="headerlink" title="一个典型场景"></a>一个典型场景</h2><p>产品经理说:”用户一提到数据库,就推荐我们的数据库产品,这样能提升功能渗透率。”</p><p>技术团队加上了这条规则。渗透率确实上去了。</p><p>然后产品经理又说:”用户提到性能问题,就推荐我们的监控方案。”</p><p>又加了一条规则。</p><p>一年后,Agent 里有 50 条这样的规则。产品经理拿着一个”纯净版”Agent 对比说:”为什么我们的 Agent 比这个蠢这么多?技术团队要提高智能水平。”</p><p>技术团队:”……”</p><h2 id="腐化的几种形态"><a href="#腐化的几种形态" class="headerlink" title="腐化的几种形态"></a>腐化的几种形态</h2><h3 id="1-规则堆积-Rule-Accumulation"><a href="#1-规则堆积-Rule-Accumulation" class="headerlink" title="1. 规则堆积 (Rule Accumulation)"></a>1. 规则堆积 (Rule Accumulation)</h3><p>每条规则单独看都合理,加起来就是:</p><ul><li>Prompt 越来越长,优先级冲突难以调试</li><li>模型的自主判断空间被不断压缩</li><li>改一个地方,另一个地方出 bug</li></ul><p>最讽刺的是:你花钱买了 GPT-4 的智能,然后用 if-else 覆盖了它的判断。</p><h3 id="2-上下文膨胀-Context-Bloat"><a href="#2-上下文膨胀-Context-Bloat" class="headerlink" title="2. 上下文膨胀 (Context Bloat)"></a>2. 上下文膨胀 (Context Bloat)</h3><p>为了让 Agent “更懂用户”,不断往 context 里塞东西:用户历史、产品信息、各种 metadata。</p><p>结果:上下文窗口被低价值信息填满,真正重要的信号被稀释,模型”注意力”被分散。</p><h3 id="3-工具蔓延-Tool-Sprawl"><a href="#3-工具蔓延-Tool-Sprawl" class="headerlink" title="3. 工具蔓延 (Tool Sprawl)"></a>3. 工具蔓延 (Tool Sprawl)</h3><p>一开始 5 个工具,边界清晰。后来 50 个工具,功能重叠,Agent 自己都不知道该调哪个。</p><p>工具选择错误率上升,维护成本爆炸。</p><h3 id="4-评估漂移-Evaluation-Drift"><a href="#4-评估漂移-Evaluation-Drift" class="headerlink" title="4. 评估漂移 (Evaluation Drift)"></a>4. 评估漂移 (Evaluation Drift)</h3><p>早期评估 Agent 真正的智能水平。后来评估变成”有没有触发这个规则””有没有推荐这个产品”。</p><p>指标和智能脱钩,团队在优化错误的东西,智能下降但指标上升,问题被掩盖。</p><h3 id="5-人设分裂-Persona-Fragmentation"><a href="#5-人设分裂-Persona-Fragmentation" class="headerlink" title="5. 人设分裂 (Persona Fragmentation)"></a>5. 人设分裂 (Persona Fragmentation)</h3><p>产品说要”专业可靠”,运营说要”活泼有趣能带货”,客服说要”严谨不能出错”。</p><p>同一个 Agent 被拉向不同方向,最后人格分裂,用户感知”这个 AI 很奇怪”。</p><h2 id="温水煮青蛙"><a href="#温水煮青蛙" class="headerlink" title="温水煮青蛙"></a>温水煮青蛙</h2><p>这种腐化最危险的地方在于:它是渐进的。</p><p>每条规则单独看都”有效”——短期数据确实在涨:</p><ul><li>功能渗透率:↑</li><li>转化率:持平或略升</li><li>结论:”有效,继续加”</li></ul><p>但债务在暗处累积。用户体验不是一下子变差,是慢慢变得”不那么聪明”。用户说不出哪里不对,但就是不想用了。留存慢慢掉,归因不到任何单一功能。</p><p>等你意识到问题的时候,系统已经改不动了。</p><h2 id="根源在哪"><a href="#根源在哪" class="headerlink" title="根源在哪"></a>根源在哪</h2><ol><li><p><strong>短期指标驱动</strong>:每个决策都优化局部指标,没人为 Agent 整体智能负责。</p></li><li><p><strong>技术缺乏话语权</strong>:技术知道加规则有问题,但产品数据说”有效”,技术说了不算。</p></li><li><p><strong>因果被切断</strong>:产品加规则拿到渗透率功劳,智能下降技术背锅。制造债务的人不承担后果。</p></li><li><p><strong>没有”智能债务”概念</strong>:传统技术债有行业共识,Agent 智能债没有度量、没有意识。</p></li></ol><h2 id="如何对抗"><a href="#如何对抗" class="headerlink" title="如何对抗"></a>如何对抗</h2><h3 id="借鉴软件工程的经验"><a href="#借鉴软件工程的经验" class="headerlink" title="借鉴软件工程的经验"></a>借鉴软件工程的经验</h3><p>在传统软件工程中,架构师的职责是保持一致性、做对的抽象、防止腐化,辅以及时重构。</p><p>Agent 产品需要类似的角色:<strong>智能架构师</strong>。</p><h3 id="智能架构师的职责"><a href="#智能架构师的职责" class="headerlink" title="智能架构师的职责"></a>智能架构师的职责</h3><ol><li><p><strong>定义 Agent 的”宪法”</strong>:核心行为准则,所有规则都要服从它。</p></li><li><p><strong>守护 Prompt 的一致性</strong>:不是谁都能往里加东西。</p></li><li><p><strong>把控工具边界</strong>:新工具要审核,工具粒度要合理。</p></li><li><p><strong>拥有否决权</strong>:产品要加破坏智能的规则,可以说不。</p></li><li><p><strong>维护纯净 baseline</strong>:始终有一个无业务污染的版本作为参照。</p></li></ol><h3 id="建立”智能债务”指标"><a href="#建立”智能债务”指标" class="headerlink" title="建立”智能债务”指标"></a>建立”智能债务”指标</h3><ul><li>硬编码规则数量</li><li>Prompt 复杂度</li><li>模型自主决策比例 vs 规则覆盖比例</li><li>纯净版 vs 当前版的智能评分差距</li></ul><p>让债务可见,而不是只看功能渗透率。</p><h3 id="流程机制"><a href="#流程机制" class="headerlink" title="流程机制"></a>流程机制</h3><ul><li><strong>偿还计划</strong>:每条规则要有下线条件和时间,不是加了就永远在。</li><li><strong>定期清理</strong>:每季度 review 所有硬编码逻辑。</li><li><strong>决策权和后果绑定</strong>:谁加的规则,谁对智能指标负责。</li></ul><h3 id="组织架构"><a href="#组织架构" class="headerlink" title="组织架构"></a>组织架构</h3><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs excel"><span class="hljs-built_in">Product</span> (What/Why) ←→ Agent Architect (守护智能) ←→ Engineering (How)<br>                              ↓<br>                      Evaluation/QA (度量智能)<br></code></pre></td></tr></table></figure><p>关键:Agent Architect 要独立,有独立 KPI,可以 challenge 产品和工程双方。</p><h2 id="AI-Native-时代的特殊性"><a href="#AI-Native-时代的特殊性" class="headerlink" title="AI Native 时代的特殊性"></a>AI Native 时代的特殊性</h2><p>这里有一个更深的问题:在 Agent 产品中,技术团队的角色发生了变化。</p><p>传统软件:PM 定义 What&#x2F;Why,工程定义 How。</p><p>但 Agent 不一样:</p><ul><li><strong>能力边界不清晰</strong>:能不能做到、做到什么程度,取决于技术实现。</li><li><strong>How 决定了 What 的可能性</strong>:用什么模型、怎么做 tool calling、怎么处理上下文——这些不是实现细节,是产品形态的根本约束。</li><li><strong>体验是涌现的</strong>:Agent 的体验取决于它”怎么思考”,这完全是技术层面的事。</li></ul><p>所以,Agent 产品中的技术团队,不只是 How,而是要参与 Why:</p><ul><li>Why this approach works</li><li>What’s actually possible</li><li>Where the real value is</li></ul><p>这不是越界,是 Agent 产品的本质要求。</p><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>Google DORA 2025 报告有一句话说得很好:</p><blockquote><p>AI doesn’t fix a team; it amplifies what’s already there.</p></blockquote><p>Agent 腐化本质上是技术债 + 组织债的结合体。每个人都在优化自己的局部指标,没人看整体——这和 LLM “优先保证局部功能正确,而不是全局架构一致性”是同一个问题。</p><p>只解决技术层面不够,需要组织层面的改变。</p><p>如果你也在做 Agent 产品,希望这篇文章能让你在下次加规则之前,多问一句:这条规则的智能债务是什么?谁来偿还?</p>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>Agent</tag>
      
      <tag>软件工程</tag>
      
      <tag>技术债务</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Environment as Dependency Inversion</title>
    <link href="/2026/01/19/2026-01-20-environment-as-dependency-inversion/"/>
    <url>/2026/01/19/2026-01-20-environment-as-dependency-inversion/</url>
    
    <content type="html"><![CDATA[<p>When building AI agents, developers almost instinctively reach for file system operations and shell commands as their first tools. This isn’t accidental - it reflects a deeply ingrained assumption: <strong>the operating system is the natural environment for agents to act in</strong>.</p><p>In this post, we explore how the <code>Environment</code> abstraction in <a href="https://github.com/youware-labs/pai-agent-sdk">pai-agent-sdk</a> implements dependency inversion, why this design embeds OS-centric assumptions, and how it shapes everything from tool implementation to context engineering.</p><h2 id="The-Instinctive-Path"><a href="#The-Instinctive-Path" class="headerlink" title="The Instinctive Path"></a>The Instinctive Path</h2><p>Watch any developer build their first agent. The conversation typically goes:</p><ol><li>“I need my agent to do things”</li><li>“Doing things means reading&#x2F;writing files and running commands”</li><li>“Therefore, I need FileOperator and Shell”</li></ol><p>This mental model is so pervasive that it feels like the only way. But it’s actually a specific architectural choice that assumes agents operate in OS-like environments.</p><h2 id="Dependency-Inversion-The-Code-Level"><a href="#Dependency-Inversion-The-Code-Level" class="headerlink" title="Dependency Inversion: The Code Level"></a>Dependency Inversion: The Code Level</h2><p>From a pure code perspective, <a href="https://github.com/youware-labs/pai-agent-sdk">pai-agent-sdk</a> implements classic dependency inversion:</p><pre><code class=" mermaid">graph TB    subgraph &quot;High-Level Modules&quot;        Agent[Agent]        Toolset[Toolset]    end    subgraph &quot;Abstractions&quot;        Env[Environment ABC]        FO[FileOperator Protocol]        SH[Shell Protocol]    end    subgraph &quot;Low-Level Implementations&quot;        Local[LocalEnvironment]        Docker[DockerEnvironment]    end    Agent --&gt; Env    Toolset --&gt; Env    Env --&gt; FO    Env --&gt; SH    Local -.-&gt;|implements| Env    Docker -.-&gt;|implements| Env</code></pre><p>Tools don’t know whether they’re running locally or in a container:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ViewTool</span>(<span class="hljs-title class_ inherited__">BaseTool</span>):<br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, ctx: RunContext[AgentContext], file_path: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">str</span>:<br>        <span class="hljs-comment"># Uses abstraction, not concrete implementation</span><br>        file_operator = ctx.deps.file_operator<br>        content = <span class="hljs-keyword">await</span> file_operator.read_file(file_path)<br>        <span class="hljs-keyword">return</span> content<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ShellTool</span>(<span class="hljs-title class_ inherited__">BaseTool</span>):<br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, ctx: RunContext[AgentContext], command: <span class="hljs-built_in">str</span></span>) -&gt; ShellResult:<br>        <span class="hljs-comment"># Same pattern - abstract Shell interface</span><br>        shell = ctx.deps.shell<br>        exit_code, stdout, stderr = <span class="hljs-keyword">await</span> shell.execute(command)<br>        <span class="hljs-keyword">return</span> ShellResult(stdout=stdout, stderr=stderr, return_code=exit_code)<br></code></pre></td></tr></table></figure><p>This is textbook dependency inversion:</p><ul><li>High-level modules (Agent, Tools) depend on abstractions</li><li>Low-level modules (LocalEnvironment, DockerEnvironment) implement abstractions</li><li>Abstractions don’t depend on details</li></ul><h2 id="The-Conceptual-Leakage"><a href="#The-Conceptual-Leakage" class="headerlink" title="The Conceptual Leakage"></a>The Conceptual Leakage</h2><p>But here’s the subtle issue: <strong>the shape of our abstractions is molded by OS concepts</strong>.</p><table><thead><tr><th>Abstraction</th><th>Derived From</th></tr></thead><tbody><tr><td><code>FileOperator</code></td><td>POSIX filesystem semantics</td></tr><tr><td><code>Shell</code></td><td>Unix shell execution model</td></tr><tr><td><code>ResourceRegistry</code></td><td>Process resource management</td></tr><tr><td><code>tmp_dir</code></td><td><code>/tmp</code> directory concept</td></tr></tbody></table><p>Even when we successfully invert dependencies at the code level, we’re still thinking in terms of “files”, “directories”, “commands”, and “environment variables”. The abstraction has absorbed the OS worldview.</p><p>This isn’t necessarily wrong - it’s a pragmatic choice. But it limits our imagination when considering alternative environments.</p><h2 id="The-Ripple-Effect-Tool-Availability"><a href="#The-Ripple-Effect-Tool-Availability" class="headerlink" title="The Ripple Effect: Tool Availability"></a>The Ripple Effect: Tool Availability</h2><p>The OS-centric design manifests in how tools check their availability:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ShellTool</span>(<span class="hljs-title class_ inherited__">BaseTool</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">is_available</span>(<span class="hljs-params">self, ctx: RunContext[AgentContext]</span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        <span class="hljs-comment"># Tool becomes unavailable if shell isn&#x27;t configured</span><br>        <span class="hljs-keyword">if</span> ctx.deps.shell <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure><p>This creates an implicit contract: environments without shell capability simply can’t use shell-based tools. The tool system gracefully degrades, but the degradation path is defined by OS capabilities.</p><h2 id="Context-Engineering-Environment-Shapes-the-Prompt"><a href="#Context-Engineering-Environment-Shapes-the-Prompt" class="headerlink" title="Context Engineering: Environment Shapes the Prompt"></a>Context Engineering: Environment Shapes the Prompt</h2><p>Perhaps the most profound impact is on context engineering. The environment doesn’t just provide tools - it shapes how we communicate with the model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># From filters/environment_instructions.py</span><br><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">inject_environment_instructions</span>(<span class="hljs-params"></span><br><span class="hljs-params">    ctx: RunContext[<span class="hljs-type">Any</span>],</span><br><span class="hljs-params">    message_history: <span class="hljs-built_in">list</span>[ModelMessage],</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-built_in">list</span>[ModelMessage]:<br>    <span class="hljs-comment"># Get environment-specific instructions</span><br>    instructions = <span class="hljs-keyword">await</span> env.get_context_instructions()<br><br>    <span class="hljs-comment"># Inject into the conversation</span><br>    env_part = UserPromptPart(content=instructions)<br>    last_request.parts = [*last_request.parts, env_part]<br></code></pre></td></tr></table></figure><p>What does <code>get_context_instructions()</code> typically return? Something like:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">environment-context</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">file-system</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">default-directory</span>&gt;</span>/home/user/project<span class="hljs-tag">&lt;/<span class="hljs-name">default-directory</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">allowed-paths</span>&gt;</span>/home/user/project, /tmp/workspace<span class="hljs-tag">&lt;/<span class="hljs-name">allowed-paths</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">file-system</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">shell-execution</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">default-timeout</span>&gt;</span>30s<span class="hljs-tag">&lt;/<span class="hljs-name">default-timeout</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">working-directory</span>&gt;</span>/home/user/project<span class="hljs-tag">&lt;/<span class="hljs-name">working-directory</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">shell-execution</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">environment-context</span>&gt;</span><br></code></pre></td></tr></table></figure><p>The model receives instructions framed entirely in OS terminology. We’re not just providing tools - we’re teaching the model to think in terms of paths, directories, and shell commands.</p><h2 id="The-Three-Layers-of-Environment-Influence"><a href="#The-Three-Layers-of-Environment-Influence" class="headerlink" title="The Three Layers of Environment Influence"></a>The Three Layers of Environment Influence</h2><pre><code class=" mermaid">graph LR    subgraph &quot;1. Tool Implementation&quot;        TI[Tools use FileOperator/Shell abstractions]    end    subgraph &quot;2. Tool Availability&quot;        TA[Tools check environment capabilities]    end    subgraph &quot;3. Context Engineering&quot;        CE[Prompts include environment instructions]    end    Env[Environment] --&gt; TI    Env --&gt; TA    Env --&gt; CE    TI --&gt; Agent    TA --&gt; Agent    CE --&gt; Agent</code></pre><ol><li><strong>Tool Implementation</strong>: Tools operate through environment abstractions</li><li><strong>Tool Availability</strong>: Tools self-disable based on environment capabilities</li><li><strong>Context Engineering</strong>: System prompts are shaped by environment context</li></ol><p>All three layers reinforce the OS-as-environment paradigm.</p><h2 id="Alternative-Perspectives"><a href="#Alternative-Perspectives" class="headerlink" title="Alternative Perspectives"></a>Alternative Perspectives</h2><p>What if we didn’t assume OS as the default? Consider alternative environment types:</p><table><thead><tr><th>Environment Type</th><th>Core Abstractions</th><th>Use Case</th></tr></thead><tbody><tr><td><strong>OS Environment</strong></td><td>FileOperator, Shell</td><td>Code agents, automation</td></tr><tr><td><strong>API Environment</strong></td><td>HTTPClient, AuthProvider</td><td>API-only agents</td></tr><tr><td><strong>Data Environment</strong></td><td>QueryExecutor, SchemaProvider</td><td>Data analysis agents</td></tr><tr><td><strong>Conversation Environment</strong></td><td>MessageBus, StateStore</td><td>Pure dialogue agents</td></tr><tr><td><strong>Browser Environment</strong></td><td>DOMOperator, NavigationController</td><td>Web automation agents</td></tr></tbody></table><p>Each would require different:</p><ul><li>Tool implementations</li><li>Availability checks</li><li>Context instructions</li></ul><h2 id="Takeaways"><a href="#Takeaways" class="headerlink" title="Takeaways"></a>Takeaways</h2><ol><li><p><strong>Dependency inversion at code level</strong>: Achieved through Environment&#x2F;FileOperator&#x2F;Shell abstractions</p></li><li><p><strong>Conceptual dependency on OS</strong>: The abstractions themselves reflect OS-centric thinking</p></li><li><p><strong>Three-layer influence</strong>: Environment shapes tool implementation, availability, and context engineering</p></li></ol><p>The next time you build an agent and instinctively reach for file system tools, pause and ask: “Is this the right environment for my agent?” The answer might still be yes - but it’s worth asking the question.</p>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Agent</tag>
      
      <tag>AGI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PTC是一种端到端的方案</title>
    <link href="/2025/12/02/2025-12-03-ptc-is-end-to-end/"/>
    <url>/2025/12/02/2025-12-03-ptc-is-end-to-end/</url>
    
    <content type="html"><![CDATA[<p>最近在做大模型网关，之前也积累了比较丰富的Coding Agent经验，看了一些针对<a href="https://platform.claude.com/docs/en/agents-and-tools/tool-use/programmatic-tool-calling">Anthropic’s Programmatic Tool Calling</a>的分析，感觉都有一些不到位，技术上来说，Anthropic实现了一个服务端的<a href="https://arxiv.org/pdf/2402.01030">CodeAct</a>工具，将代码编写和执行都放在服务端进行，并不在API中完全暴露，由此，API的使用者可以在减少token消耗的情况下实现目标。</p><blockquote><p>如果在客户端实现，则至少需要编写代码-执行代码两个轮次，甚至更多</p></blockquote><p>下面这张图很好的解释了整个工作流程：</p><p><img src="https://www.anthropic.com/_next/image?url=https://www-cdn.anthropic.com/images/4zrzovbb/website/65737d69a3290ed5c1f3c3b8dc873645a9dcc2eb-1999x1491.png&w=3840&q=75"></p><p>以上基本上是大部分自媒体&#x2F;公众号&#x2F;营销号对于它的理解，以下我提供一些不一样的看法，可能不一定成熟。</p><h2 id="有状态API应该包含环境状态，而不是消息状态"><a href="#有状态API应该包含环境状态，而不是消息状态" class="headerlink" title="有状态API应该包含环境状态，而不是消息状态"></a>有状态API应该包含环境状态，而不是消息状态</h2><p>有状态API的起始是OpenAI的Responses API，在我看来其主要目的有二：</p><ol><li>允许客户端可以在发起任务之后异步获取结果，以减少服务器压力</li><li>更好地在隐藏推理细节的同时，提供连贯推理的服务</li></ol><p>但实际上，Responses API只是在性能上稍好，大部分时候OpenAI只享受到其数据安全的部分，因为Responses API实际上无状态模式，而大部分时候，我是使用无状态模式进行交互：实时拉取流，保存thinking signature而非id，完整回填整个消息列表</p><p><strong>PTC提供了一种带有环境状态的API，其编写、执行代码将对其服务端的对应环境造成影响</strong>，简单来说，过去我们让Agent改文件，所有文件状态的更新发生在我的本地，而Agent需要主动获取我本地的环境信息，这依赖于我，确切的说是我所使用的客户端，Claude Code、Codex CLI、Cline等等具体的工具实现，而PTC模式下，这些工具是在服务端沙盒实现的，没有实现者的bias、没有普适性要求、也没有那么多需要考虑的适配和安全问题。</p><h2 id="端到端的数据积累"><a href="#端到端的数据积累" class="headerlink" title="端到端的数据积累"></a>端到端的数据积累</h2><p>过去，模型公司收集到的用户使用数据只能通过消息，我们常说Cursor的价值在于积累了很多用户交互的真实数据，实际上指的就是环境数据和消息数据的结合。现在，PTC展示了一种模型厂直接端到端收集Agent数据的方式，通过一个已经跑通的、需要智能的场景，通过收集这方面的数据，或许能够切实地推进从ReAct到CodeAct的效率和智能提升。</p><p>Claude Code Agent SDK远远不够，PTC是Anthropic真正想要的东西。</p><p>那么，或许对bun的收购也顺理成章？</p>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>AGI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Free from the coding language</title>
    <link href="/2025/11/28/2025-11-29-free-from-coding-language/"/>
    <url>/2025/11/28/2025-11-29-free-from-coding-language/</url>
    
    <content type="html"><![CDATA[<p>一篇碎碎念，好久没更新了。</p><p>最近花了很多时间在研究各个模型之间的差别，同一个prompt下面，不同的厂商的模型所表现出的trajectory差别巨大。同时，随着年末大家的混战，我们惊喜的发现OpenAI、Anthropic、Google三足鼎立的局面似乎正在形成。当我们觉得GPT-5 Codex横扫四方时，Sonnet 4.5的出色表现让我感觉Anthropic并未落后，而Gemini 3 Pro非常惊喜地让我们看到一个经济、速度、性能都非常均衡的选择。</p><p>最近我在使用Claude Opus 4.5来进行Rust项目的编写（构建一个LLM网关来进行智能路由，等我完成后会有博客来介绍），明显感觉到与Sonnet 4.5相比，Opus更加谦逊且精准，就我而言，目前最佳的使用方式仍然是与AI讨论设计，输出技术架构和详细设计文档，然后在手动控制上下文长度的前提下（大部分时候是控制每次的任务大小），让Agent能够通过编写测试或其他方式验证实现的情况下，来完成代码编写工作。这一次更不一样的是，我选择了我没有那么熟悉，但是编译器和工具链都非常成熟的Rust语言，结果也非常令人满意。这表明，随着模型能力的提升，我们或许可以更加激进地探索和学习新的技术栈，而不必过于担心自己Debug的能力不足，相反，架构设计、可测试性、可维护性等软实力将变得更加重要。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Design Agentic Coding Agent</title>
    <link href="/2025/10/16/2025-10-17-design-agentic-coding-agent/"/>
    <url>/2025/10/16/2025-10-17-design-agentic-coding-agent/</url>
    
    <content type="html"><![CDATA[<p>是时候思考如何构建一个可扩展的云原生Coding Agent系统了。</p><h2 id="Agentic-Workflow"><a href="#Agentic-Workflow" class="headerlink" title="Agentic Workflow"></a>Agentic Workflow</h2><p>自<a href="https://www.claude.com/product/claude-code">Claude code</a>横空出世，人们越来越倾向于采用一个<a href="https://simonwillison.net/2025/Sep/18/agents/">简单的定义</a>来描述Agent：大型语言模型在循环中自主使用工具来完成某个目标。</p><p><img src="/../img/2025-10-17-design-agentic-coding-agent/agent-meme.png" alt="Meme of agent workflow"></p><h2 id="Agent-Design-Considerations"><a href="#Agent-Design-Considerations" class="headerlink" title="Agent Design Considerations"></a>Agent Design Considerations</h2><p>鉴于大模型的消息是无状态的，我们很容易拆分出LLM消息和工具实现两部分，MCP协议给了我们这样的一个example，通过streamable http或者本地stdio的方式，基于JSONRPC对工具定义进行分离。</p><p>接下来，我们很自然地思考，工具本身是否是有状态的？这就回到了Agent所针对的目标中。对于Coding Agent来说，其所处环境应是与人类程序员编程时使用的环境一样的开发环境，由以下组成：</p><ul><li>代码和相关文件，或者说repo</li><li>运行时及运行依赖（编译和调试容器、其他已部署的服务、数据库、本地需要安装的调试库等等）</li></ul><p>Agent本质上是在通过工具与上述两个环境进行交互，我们可以得出这样的描述：Agent通过不变的工具对环境进行改变，从而获得观察（observation），再指导其下一步动作。</p><p>这里及引出两个问题：</p><ol><li>工具一定是同步执行的吗？</li><li>环境如何与Agent消息进行同步？</li></ol><h2 id="Async-Tool-Calling-and-other-jobs"><a href="#Async-Tool-Calling-and-other-jobs" class="headerlink" title="Async Tool Calling(and other jobs)"></a>Async Tool Calling(and other jobs)</h2><p>大多数API都允许Tool Response与User Message同时包含在一次请求中，只需要满足Tool Call和Tool Response在一次LLM请求和响应之间是成对出现的即可。因此，我们可以通过User Message，或包装Tool Response来提醒Agent哪些任务已经完成可以再次获取，或者将其他的系统异步任务添加到消息中。</p><p>另一种方式是让Agent直接管理异步任务，但由于自动压缩等上下文管理策略，我们需要确保Agent不会忘记已经启动的任务，并观测其结果</p><p><img src="/../img/2025-10-17-design-agentic-coding-agent/async-job.png"></p><h2 id="Sync-Message-and-Environment"><a href="#Sync-Message-and-Environment" class="headerlink" title="Sync Message and Environment"></a>Sync Message and Environment</h2><p>现在，我们需要将消息和环境进行绑定，如果我们想在任意时刻进行回滚重试，那么对于每一次工具调用都对应了一个环境快照，当这一依赖影响到数据库等不一定能回滚的资源时，我们就必须针对这类资源进行特别设计。</p><p>基于不同的开发模式，我们可以为用户提供不同程度的重试和回滚策略，从最基本的staging环境+prod环境，再到通过脚本自动创建本地环境模拟等等方式，一种思路是通过Infrastructure as code (IaC)+unit test的方式，使用脚本来确保开发环境的可重现，另一种思路则是在基础设施层就支持这一特性。而针对Agent消息，我们可以通过各类durable execution的基础设施来实现，搭配RPC Tool Calling，实现Agent消息的编排，具体可以参考：</p><ul><li><a href="https://temporal.io/blog/what-is-durable-execution">https://temporal.io/blog/what-is-durable-execution</a></li><li><a href="https://langchain-ai.github.io/langgraph/concepts/durable_execution/#using-tasks-in-nodes">https://langchain-ai.github.io/langgraph/concepts/durable_execution/#using-tasks-in-nodes</a></li><li><a href="https://ai.pydantic.dev/durable_execution/overview/">https://ai.pydantic.dev/durable_execution/overview/</a></li></ul><h2 id="User-experience"><a href="#User-experience" class="headerlink" title="User experience"></a>User experience</h2><p>用户体验通常是Agent系统设计忽略的一环，实际上工具调用可能长时间的无法流式输出，特别是编辑特别大的代码文件时，这会造成很大的用户体验问题。<strong>良好的用户体验常常可以让用户享受创作和解决问题的过程，而非仅仅交付物本身</strong>。我认为我们可以通过拆解工具调用的流式阶段，再通过一个非常轻量化的模型来进行流式输出，以提供流畅、易懂的用户体验。如果我们将异步任务视为一个消息系统，则可以考虑“Agent发起任务” - “Agent等待任务完成” - “任务已完成，等待Agent响应” - “Agent正在处理响应并进行下一步”的循环流程，而不是只能向用户展示“Agent调用工具中” - “Agent调用工具完成”的序列，用户也可以更清楚的了解系统的工作流程。</p>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Agent</tag>
      
      <tag>Code Agent</tag>
      
      <tag>Coding</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Thinking about debug agent</title>
    <link href="/2025/10/08/2025-10-09-about-debug-agent/"/>
    <url>/2025/10/08/2025-10-09-about-debug-agent/</url>
    
    <content type="html"><![CDATA[<p>简单记录一些对于Debug Agent的思考，与Coding Agent不同，Debug Agent需要包含更多的环境感知，需要更多的细节设计</p><h2 id="Context"><a href="#Context" class="headerlink" title="Context"></a>Context</h2><p>Debug Agent应当由三个重要部分组成：用户上下文、程序上下文和自动化交互方案</p><h3 id="用户上下文"><a href="#用户上下文" class="headerlink" title="用户上下文"></a>用户上下文</h3><p>当人们深入地使用Agent进行编程的时候，常常陷入debug困难的境地，表现为：</p><ol><li>很难描述自己遇到了什么问题，一种方案是进行截图或者录屏，然后交给一个有视觉、甚至可以处理视频（一般而言webp或者gif也可以）的agent来分析解决</li><li>很难给出问题栈，比如点击某个按钮之后，http请求出错了，如何把问题提交给Agent进行解决</li></ol><p>我希望通过“用户上下文”来描述此类场景，对应用户在使用产品的过程中遇到的bug和各种现象，也包括了运行时产生的各种上下文</p><h3 id="程序上下文"><a href="#程序上下文" class="headerlink" title="程序上下文"></a>程序上下文</h3><p>程序上下文实际上是Agent来理解软件的工程，软件不仅仅是代码组成，还包括了对代码业务的理解和说明，类似所有Spec-drive开发，对于需求文档、技术选型、代码仓库的长短期记忆与规划和代码本身构成了程序上下文。Agent基于对程序上下文进行决策，理解和解决问题。</p><h3 id="自动化交互方案"><a href="#自动化交互方案" class="headerlink" title="自动化交互方案"></a>自动化交互方案</h3><p>自动化交互方案是自动化测试在Agent上的实现，通过Agent进行交互来自动化地获取“用户上下文”。通过不同细粒度的自动化交互方案设计，如Browser use&#x2F;Unit test&#x2F;End-to-end test都对应了不同的用户上下文收集方式。</p><h2 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h2><p><img src="/../img/2025-10-09-about-debug-agent/workflow.png"></p><h2 id="发展阶段"><a href="#发展阶段" class="headerlink" title="发展阶段"></a>发展阶段</h2><p>我们分方面来看，程序上下文其实和Coding Agent基本一致，主要问题是保持软件开发过程中的文档和知识能够持续传承和更新；用户上下文与自动化交互相辅相成，是Debug Agent的重点。</p><h3 id="程序上下文-1"><a href="#程序上下文-1" class="headerlink" title="程序上下文"></a>程序上下文</h3><p>第一阶段，引入最基本的记忆文件，类似AGENTS.md, CLAUDE.md，记录项目的重要信息</p><p>第二阶段，结构化记忆，使用或结合memory文件夹&#x2F;RAG&#x2F;抽取等方案，自动地存取用户对项目的一些要求、用户的偏好</p><p>第三阶段，规范驱动，结合用户体验一起，设计交互模式来推进产品需求、设计、功能开发和测试的全套程序上下文记忆存储</p><h3 id="用户上下文与自动化交互"><a href="#用户上下文与自动化交互" class="headerlink" title="用户上下文与自动化交互"></a>用户上下文与自动化交互</h3><p>第一阶段，我们可以设计一系列工具（交互），让用户尽可能简单的反馈正确的用户上下文，同时集成一些简单的自动化交互来进行测试，比如截图、单元测试支持。目前看到良好的用户交互有：</p><ul><li>截图标注</li><li>gif&#x2F;webp录屏</li></ul><p>第二阶段，我们将设计一系列采集工具，对用户交互的运行时上下文进行自动化收集和分析，与分布式系统追踪类似，例如：</p><ul><li>network &amp; console日志自动捕获</li><li>后端日志自动化收集</li><li>其他指标监控</li></ul><p>第三阶段，我们需要自动化交互，并为自动化交互构建收集系统，这是自动化测试和监控的进阶，需要为LLM特别优化的输出才能得到足够好的效果</p>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Agent</tag>
      
      <tag>Vibe coding</tag>
      
      <tag>Debug</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Evals is misleading?</title>
    <link href="/2025/09/09/2025-09-10-evals-is-misleading/"/>
    <url>/2025/09/09/2025-09-10-evals-is-misleading/</url>
    
    <content type="html"><![CDATA[<p>最近看了一些LLM评估的文章，很明显有两个倾向</p><ul><li>使用LLM进行评估（LLM-as-Judge）是一种AI-Native的方式，或许在Human alignement（对齐）上可以做到比较好，但仍然受限于简单任务，对于复杂任务人们很难模拟并自动化评估</li><li>由于复杂性，大多数产品不使用自动评估方法，而是通过研究员&#x2F;工程师的自主洞见，或者设计信号（Signal），进行A&#x2F;B实验来判断模型是否变好。Claude code“降智”事件可以看做是一次大型的量化模型A&#x2F;B实验（有人有证据证明某些时间sonnet和opus是使用量化模型进行serve的，anthropic声称是Bug）</li></ul><p>从我的理解上看，没有办法通过一个同等智能的模型评估另一个模型的思考过程，就如同使用AI检测AI一样，如果能被检测，那就一定能骗过检测，而当我们有更高级的智能来评估时，谁又来评估这个“更高级”的智能给我们带来了多少提升？最终我们只能达到两个结果：</p><ol><li>做了很多的事，得到了当前结果的算法验证，证明了目前的方法有用，可能产出一些对于当前方法为什么有用的洞见，仅此而已，并不对接下来的技术路线有指导意义</li><li>仍然通过人类来探索新方向，评估永远滞后</li></ol><p>既然评估只能解决一部分问题，我们应该做什么？<strong>或许我们不应该在现在开始研究评估，或许我们评估的目标并非中间产物</strong></p><p>这一观察可能与我们目前正在AI Coding的前沿有关，我们很明显的碰到了LLM的能力边界，因此开始研究各种Context Engineering的方式，以及思考Context和LLM如何协作。因此我更倾向于将模块拿出来进行评估，衡量每个模块在任务过程中的成本和性能，而非优化出某种想要的结果。简单说，我们应该衡量我们驱动LLM的方式，通过A&#x2F;B实验捕捉信号、还是通过定性定量分析，都是可以尝试的。</p><blockquote><p>世界上大部分人没有用过AI Coding，以后的AI Coding也不会是现在这个样子</p></blockquote><p><strong>警惕局部最优</strong></p><h2 id="参考阅读"><a href="#参考阅读" class="headerlink" title="参考阅读"></a>参考阅读</h2><ul><li>X上的一些讨论：<a href="https://x.com/justinstorre/status/1964029634796015685">https://x.com/justinstorre/status/1964029634796015685</a></li><li>A&#x2F;B测试平台表示没有auto judge，全是监控：<a href="https://www.raindrop.ai/blog/thoughts-on-evals">https://www.raindrop.ai/blog/thoughts-on-evals</a></li><li>系统性的评估是有益的：<a href="https://www.sh-reya.com/blog/in-defense-ai-evals/">https://www.sh-reya.com/blog/in-defense-ai-evals/</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLM只是计算，Context才是内存</title>
    <link href="/2025/09/01/2025-09-02-context-is-memory/"/>
    <url>/2025/09/01/2025-09-02-context-is-memory/</url>
    
    <content type="html"><![CDATA[<p>LLM并非一台计算机，LLM目前只是一个处理器，人们通常将记忆、RAG等外置存储手段作为内存看待，但实际上，只有Context才能被看做内存，而这些外挂的存储手段，可以看作是一种“虚拟内存”，LLM通过工具调用或者工程师通过工程化的手段进行“换页”，人们将此称为Context Engineering。</p><p>我之前介绍过<a href="https://blog.wh1isper.top/2025/06/16/2025-06-17-context-engineering/">工程上的Context Engineering</a>策略，而LLM进行工具调用的方式，目前看分为两种模式：</p><ol><li>检索模式：通过向量检索、搜索引擎等方式进行搜索，理解返回结果</li><li>阅读模式：通过直接阅读文档进行理解</li></ol><p>显而易见，检索模式效率更高，但容易受限于RAG等技术，精确度低，工程难度大，这种方式流行的原因其实是因为简单，而非性能。</p><p>目前看，阅读模式的性能更优，但实现上需要有更多考虑：一方面，上下文长度的控制和对应工具实现很重要，通常会提供类似grep、glob等工具来进行代码搜索；另一方面，通过sub-agent的方式进行上下文隔离，可以减少context的消耗。</p><h2 id="未来如何"><a href="#未来如何" class="headerlink" title="未来如何"></a>未来如何</h2><p>我们看到从输入的Prompt Engineering到Context Engineering，我们已经将对LLM应用从简单的汇编语言操作寄存器（仅有输入的prompt）进化到C语言类似的，可进行内存管理的高级语言模式，更进一步地看，下一步或许是发明更高效的编译器技术，让用户的自然语言能够更好地被高级语言所理解和编译，也就是说，Agent（LLM+工程）能够根据用户的输入来更加自主、智能地控制上下文。这是我认为的，除去预训练和记忆模式以外的另一种Learning实现。</p>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>时间是人类的幻觉</title>
    <link href="/2025/08/31/2025-09-01-time-is-illusion/"/>
    <url>/2025/08/31/2025-09-01-time-is-illusion/</url>
    
    <content type="html"><![CDATA[<h4 id="LLM是没有时间概念的"><a href="#LLM是没有时间概念的" class="headerlink" title="LLM是没有时间概念的"></a>LLM是没有时间概念的</h4><p>我想起人们让Deepseek深度思考三秒后给出答案，Deepseek真的考虑一下什么是三秒，以及如何思考三秒。或许这就是肉身人类与硅基生命的区别。</p><h4 id="时间是人类最重要的幻觉"><a href="#时间是人类最重要的幻觉" class="headerlink" title="时间是人类最重要的幻觉"></a>时间是人类最重要的幻觉</h4><p>认识到时间对于自己的重要性，是认识到自身意义的开始。</p><p>如果一个人一直按部就班地活着，时间对他来说是最不值钱的，相比之下，不被破坏的规律是他最重要的东西。</p><p>但某一天，突然发现，一个人的人生之所以不同，就是因为每个人所体验到的世界是独一无二的，而体验世界的唯一必须，就是时间。我们可以简单的说，时间之于物是没有意义的，物随时间变化形态，往往是相同或者相似的，如聚沙成塔、滴水石穿。但时间之于思想缺失最重要的元素，因为思想，所以感受到了时间，因为时间，思想得以发展。</p><p>对于物质的人而言，时间是人类最重要的幻觉</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>重拾发呆</title>
    <link href="/2025/08/20/2025-08-21-the-art-of-daydreaming/"/>
    <url>/2025/08/20/2025-08-21-the-art-of-daydreaming/</url>
    
    <content type="html"><![CDATA[<p>最近我又开始可以发呆了。</p><p>高考结束之后，人生仿佛按下了快进键，在大学卷，在实习卷，在工作卷，只有付出努力才能获得回报。</p><p>不论是在地铁、出游、还是在家里，我都在思考，思考着课程、作业、工作内容、架构设计、赚了多少钱，对比着自己和别人的生活，叹息着自己的生活不如意，于是又催促自己再加把劲。</p><p>或许我就是这样失去了发呆的能力。</p><p>我曾经认为，发呆是灵感的来源，是快速休息的方式，我总会在课堂上、公交车上、地铁上发呆，什么也不想，后来听说这叫正念，所以，我似乎很早很早就掌握了正念，又在忙碌中失去了它。</p><blockquote><p>或许有人会说发呆和正念完全不同，但对我来说，发呆就是正念。</p></blockquote><p>自从去年burnout之后，我开始慢慢地恢复到以前的状态，开始主动地放慢节奏，主动地观察内心，直到最近，我发现我的内心平静到一定程度时，我又找回了发呆的感觉。这是一种在放任思维流动的感觉，在这个状态下，我可以想象或者不想象、思考或者不思考，而回报是一时间的灵光闪现。</p><p>所以，多发发呆吧，如果发现自己无法发呆，或许是时候放慢脚步。</p><p>想想那个一十二岁的自己。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>杠杆效应-人、LLM与杠杆</title>
    <link href="/2025/08/02/2025-08-03-leverage-of-llm/"/>
    <url>/2025/08/02/2025-08-03-leverage-of-llm/</url>
    
    <content type="html"><![CDATA[<p>这是一篇随笔，在思考人类应该如何利用LLM的时候，我意识到杠杆效应是一个很好的思考角度。</p><h2 id="工具杠杆"><a href="#工具杠杆" class="headerlink" title="工具杠杆"></a>工具杠杆</h2><p>从工具杠杆的角度上看，LLM是一个很平均的工具，在不对其进行微调的时候，人们总能通过chatbot提升自己的效率，但由于单纯聊天产生的价值不高，人们需要通过高价值的劳动行为来提升杠杆率。</p><p>目前得到验证的工具杠杆是Coding，在上一个时代（互联网时代）已经验证过，代码可以不间断的运行并带来价值，其边际成本极低，作为信息技术可以带来极大的杠杆率。如果我们可以使用LLM加速代码的生成，则可以利用互联网时代的基建和系统，提升整个互联网的发展速度</p><p>在其他领域上，比如PPT、AI员工等工作，我发现其缺少反馈环境机制，通常依靠人在回路进行反馈，也受限于人类认识和审美，这方面的最主要问题是人类缺少高质量的员工，而平均值的人类+平均值的AI并不能产生多少价值，也不能消灭多少岗位（因为AI的价格也很贵）。这很类似于以基本工资雇佣老头老太太来进行环卫、保安、售票员等工作，自动化本身的价格可能比他们还高或者持平，但是考虑到就业，社会不得不从经济效益和人的角度创造一些毫无意义的工作，这在任何行业都成立，比如互联网企业也存在一大堆的“职能岗位”，即便他们最擅长的就是用代码来提高效率。</p><h2 id="知识杠杆"><a href="#知识杠杆" class="headerlink" title="知识杠杆"></a>知识杠杆</h2><p>另一个想法是，LLM作为知识杠杆能够加速人类摄取知识的速度，从书籍与印刷业得到的启示是，当信息获取的成本降低后，社会效率会进一步提高。另一方面，科学上低垂的果实已经被消耗殆尽，越来越多的科研创新依赖着大组织协作，人们需要越来越多的时间来学习基础知识才能参与到科学创新中，如果我们可以加速人类摄取知识的速度，或许科学创新的速度也能被增加。</p><p>但科学创新的反馈链路太长，人类的经济制度是否能帮助这一过程，或者这是对未来的美好想象</p><h2 id="二者结合"><a href="#二者结合" class="headerlink" title="二者结合"></a>二者结合</h2><p>从我的角度上看，人类可能正处于从学堂学习到实践学习的转化过程中，只是没有人意识到这一件事。LLM最神奇的地方在于其工具属性和知识属性共存，以AI Coding举例，人们在使用LLM进行代码编写的过程中，也在和AI进行结对编程，学习相关的编程知识，那么为什么人类不能和LLM工具一起成长呢？</p><p>一个可能的问题是，知识杠杆由于当前的教育体制设计，其反馈回路太长，导致产品只能设计为“做卷子”（chatgpt study mode）的模式，而非渐进式学习的模式。人们总假设有一个固定的答案，而不是探索一个真实世界的解决方案，人们面向的销售目标也是在授课体制内的学生、老师和家长，而不是面临真实世界问题的每个普通人。</p><p>另一方面，LLM在工具中做的也不好，人们无法相信一个“自己都做不好”的老师，这一方面是LLM的幻觉与事实核查。另一方面是人类自己对于表达、理解和最重要的动力的缺失，很多时候人们已经被资本主义规训成“有自我”的人，虽然这个自我意识是千篇一律的，受到灌输的自我。我们称一个只知道享乐，不希望思考，使用产品的目的是“解决问题”的人为<em>现代人</em>，而称一个时刻都在思考解决问题的人为<em>原始人</em>，当我们用这一方式思考AI，人类居然妄想通过程序员、产品经理、测试人员的工作岗位划分Agent来模拟资本主义下低效的世界，将被规训的低智商人类映射到对真实人类智能仿真的神经网络上，果不其然极大地降低了LLM的智能。从这一角度上看，现在的人类头脑本身是非常适合和LLM共同成长的，而现存人类的肉体头脑因为其生存的时间长度，受到资本主义规训的影响，导致他们变成了“人力资源”而不是通用智能，则大大阻止了人类与LLM共同成长。</p><p>于是我发现，在下一代AI-Native人类成长起来（或许永远也成长不起来）之前，人们只能接受无脑产品或者研究机器，这一未来是光明的，但其曲折的过程，则是对于我们这一代人的局限和悲剧。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>LLM</tag>
      
      <tag>AGI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI Native的产品更应该暴露错误</title>
    <link href="/2025/07/25/2025-07-26-expose-more-error/"/>
    <url>/2025/07/25/2025-07-26-expose-more-error/</url>
    
    <content type="html"><![CDATA[<p>我曾在<a href="https://blog.wh1isper.top/2025/06/07/2025-06-08-agi-product-design/">之前的文章</a>中讨论过AI产品需要更端到端的设计来帮助用户发挥Agent智能，在目前来看，暴露错误是一个很好的让用户学习、同时进一步发挥模型智能的方式。</p><p>在传统的产品设计中，人们总是倾向于所谓的简洁，将一切复杂的原理藏在产品后面，让用户能够下意识的完成操作，人们总是假设用户无法理解产品背后的运行逻辑，不具备或不愿意花时间理解产品的技术细节。但在AI时代，每个人都需要通过表达来创造自己想要的东西，这时简洁反而成为了不安全感和困惑的来源。如果人们一定要面对复杂性，那么产品就一定要正确的暴露足够的细节，以便让人们能够更好地理解和掌握自己生成的东西的工作原理。</p><p>AI时代一个重要的改变是人们不再被专业知识而困扰，一个没有接触过web开发的非技术人士也可以通过大模型进行编码并部署在vercel之类的平台上，如果越来越多的人被纳入AI教育中，越来越多人开始使用AI，那么假设用户都是“不愿意学习的懒人”无疑是愚蠢的行为。基于这一假设，如果我们相信未来的世界是技术民主的，未来的人类是更会表达、更不机械、更有创造力的，我们就应该暴露更多的产品细节，创造更有可能性而不是更具可预测性的产品。</p><p>暴露错误是另一个让用户和产品一起成长的重要方式（<a href="https://blog.wh1isper.top/2025/07/04/2025-07-05-ergonomics-to-agent/">还有一个是TODO Tool</a>），通过精巧的设计，用户不再是对错误无能为力的人群，而是能够借助AI能力对错误进行修复并在其中成长的人，如果每个人都会有类似的成长过程，那么最适应这一成长过程的产品将会获胜，这或许是比“好用”更加重要的AI时代产品设计原则——成长性。</p><p>最后，通过成长性原则，我们可以帮助产品和用户建立起反脆弱的特点：从错误中学习并成长，从而出更少的错、创建更好的产品。由于AI的不确定性，随着AI的能力提升，产品对于AI的约束减少，产品的不稳定性也有可能上升(通常是AI和非AI的黏合初，很容易松动)，不同技能水平的用户使用其能力的方差应该也会增大，若可以实现一个机制帮助用户成长，那么我们就赋予了用户减少产品不稳定性的能力，从而实现用户和产品的结合，这或许是AI时代的用户粘性：不是chat、memory，而是experience point。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>LLM</tag>
      
      <tag>AGI</tag>
      
      <tag>产品设计</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Architecture of Agent System</title>
    <link href="/2025/07/19/2025-07-20-architecture-of-agent-system/"/>
    <url>/2025/07/19/2025-07-20-architecture-of-agent-system/</url>
    
    <content type="html"><![CDATA[<p>记录一些最近调研的 Agent 系统的架构。</p><h2 id="Long-term-Memory-System"><a href="#Long-term-Memory-System" class="headerlink" title="Long-term Memory System"></a>Long-term Memory System</h2><p>参考<a href="https://github.com/mem0ai/mem0">Mem0</a>和<a href="https://github.com/awslabs/amazon-bedrock-agentcore-samples/blob/main/01-tutorials/04-AgentCore-memory/02-long-term-memory/01-single-agent/using-strands-agent-memory-tool/culinary-assistant.ipynb">AWS Bedrock AgentCore Memory</a>，长期记忆一般是 LLM 蒸馏用户消息，形成浓缩消息和对应 embedding，从而支持语义化检索。</p><p>核心组件：</p><ul><li>LLM：浓缩消息</li><li>Embedding：对浓缩消息进行Embedding并支持语义化检索</li><li>Vector Database：Embedding的索引，支持快速检索</li></ul><p><img src="/../img/2025-07-20-architecture-of-agent-system/long-term-memory-system.png" alt="Long-term Memory System"></p><h2 id="Browser-use-Tool-System-Design"><a href="#Browser-use-Tool-System-Design" class="headerlink" title="Browser-use Tool &amp; System Design"></a>Browser-use Tool &amp; System Design</h2><p>参考<a href="https://github.com/browser-use/browser-use">browser-use</a>进行工具设计，并针对工作负载进行优化。</p><p>核心设计：</p><ul><li>Browser Tool设计<ul><li>基础的操作有打开、滑动、点击等</li><li>更复杂的是2FA、验证码等自动化操作，可能需要更细节的工具封装和工程化解析工作</li><li>有时可以总结某些网页为流程，通过AI进行启发式测试，再通过非AI的方式重放以提高效率</li></ul></li><li>无头浏览器的负载分离和安全隔离<ul><li>页面内容不受信任，可能存在恶意代码</li><li>页面可能存在bug，导致资源耗尽</li><li>控制爆炸半径，当浏览器崩溃时，只影响当前用户的浏览器实例</li></ul></li><li>与反爬虫手段对抗：capture resolver、ip proxy等</li></ul><p><img src="/../img/2025-07-20-architecture-of-agent-system/remote-browser-system-design.png" alt="Remote browser system design"></p><h2 id="Code-Execution-Sandbox"><a href="#Code-Execution-Sandbox" class="headerlink" title="Code Execution Sandbox"></a>Code Execution Sandbox</h2><p>与LLM的交互只需要全量的消息历史作为上下文，LLM会返回两类响应：<code>Tool Call Request</code>（工具调用）和<code>Text Message</code>（文本消息），而相对应的，Client需要对<code>Tool Call Request</code>进行处理，并将结果返回给LLM，暂且称之为<code>Tool Result Response</code>。Client处理则是依赖于“假设的环境”中，例如我们假设Agent运行在某台Linux机器上，并为LLM提供了编写代码和执行代码的工具，则Client在处理工具调用时，需要维护这个假设的环境的一致性：即编写了代码执行的时候，需要正确地执行代码并返回结果。此外，Client还需要关注工具的副作用，执行代码是一个非常好的例子，当两个用户在同一台机器上执行代码，则可能会互相影响，因此对于有副作用的工具调用，我们需要设计合理的隔离和沙箱机制。</p><p>而其他工具，比如搜索、浏览器等工具，则大部分不需要维护环境的一致性，则可以认为他们是无状态的。</p><p><img src="/../img/2025-07-20-architecture-of-agent-system/code-execution-sandbox.png" alt="Code Execution Sandbox"></p><p>为了更高的资源利用率和用户体验，我们需要同时解决隔离环境的启动速度和资源利用率的问题，目前有两种架构选择：</p><ul><li>gVisor：定制化内核路线，更好性能但更多网络&#x2F;系统调用限制，可能需要定制化开发</li><li>Firecracker：vm(kata container)技术路线，开箱即用，更好的兼容性，但启动速度较慢一些（对比gVisor）</li></ul><p>这里不展开细节，不使用runC(docker)的原因是其容易受到内核漏洞的影响，且启动速度较慢（1-10s）级别，在初期可以考虑使用runC进行PMF，但在后期需要考虑替换。</p>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Architecture</tag>
      
      <tag>Agent</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ergonomics to Agent</title>
    <link href="/2025/07/04/2025-07-05-ergonomics-to-agent/"/>
    <url>/2025/07/04/2025-07-05-ergonomics-to-agent/</url>
    
    <content type="html"><![CDATA[<h1 id="Agent-Design-人体工学"><a href="#Agent-Design-人体工学" class="headerlink" title="Agent Design - 人体工学"></a>Agent Design - 人体工学</h1><p>我们提供了一个TODO工具给Agent，让它可以列出TODO项，并在任务过程中对TODO项进行修改，这一过程会完全展示给用户。</p><blockquote><p>当我要求制作一个个人博客时，Agent列出了以下TODO：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">-</span> [] 创建个人博客的HTML结构<br><span class="hljs-bullet">-</span> [] 设计博客样式和布局<br><span class="hljs-bullet">-</span> [] 添加导航和页面内容<br><span class="hljs-bullet">-</span> [] 添加响应式设计<br></code></pre></td></tr></table></figure></blockquote><p>起初，我对这一功能并不感冒，因为我知道Agent的工作流程，并通常不依赖他进行架构设计和技术方案选择，因此我只需要关注他对Thinking工具的使用就可以了解他的思路。同时，根据我对Agent的了解，我认为TODO工具某种程度上增加Agent在通用任务中的惰性，不利于其自由发挥，不一定适合我们的产品。</p><p>但在推出这一功能之后，我发现我错了。</p><p>不仅仅是我们的产品经理和最活跃的用户，大部分技术人员也认为这增强了他们对Agent的掌控和理解，这让我意识到，或许我们在追求Agent性能的道路上，忽略了很多人体工学的内容，我们常常想着如何设计一个产品来交付结果，但忽略了人类与工具交互过程中，人类对于工具的控制、学习和理解。</p><h2 id="让用户在使用中成长"><a href="#让用户在使用中成长" class="headerlink" title="让用户在使用中成长"></a>让用户在使用中成长</h2><p>TODO工具最好的地方在于，让用户在使用的过程中成长。通过TODO工具，非技术人员可以了解到Agent对问题的拆解，从而学习到软件开发、架构设计等领域的知识。也许AI会有幻觉做出错误的编码或设计，但用户可以通过进一步地交互，和AI一起解决问题。这为非技术用户构建了一种在使用中学习的可能性，这是以前的工具类产品所不具有的特性。</p><p>上个月我曾<a href="https://blog.wh1isper.top/2025/06/07/2025-06-08-agi-product-design/">讨论</a>了AGI的产品设计，从端到端的来看，“让用户在使用中成长”可能是最重要的设计理念，这比过去美观的界面、易用的UI又或者是更高的付费转化率更为重要。这代表了AI智能的被驱动程度，而在这过程中人类输入-AI输出的方差，或许能够成为“场景为王”的强化学习下半场中重要的数据资产。</p><h1 id="Ergonomics-in-Agent-Design"><a href="#Ergonomics-in-Agent-Design" class="headerlink" title="Ergonomics in Agent Design"></a>Ergonomics in Agent Design</h1><blockquote><p>English version translated by Claude and Wh1isper(Human in the loop).</p></blockquote><p>We provided a TODO tool for the Agent, allowing it to list TODO items and modify them during the task process, with the entire process fully visible to users.</p><blockquote><p>When I requested to create a personal blog, the Agent listed the following TODO:</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">-</span> [] Create HTML structure for personal blog<br><span class="hljs-bullet">-</span> [] Design blog styles and layout<br><span class="hljs-bullet">-</span> [] Add navigation and page content<br><span class="hljs-bullet">-</span> [] Add responsive design<br></code></pre></td></tr></table></figure></blockquote><p>Initially, I wasn’t enthusiastic about this feature because I understand the Agent’s workflow and typically don’t rely on it for architectural design and technical solution selection. Therefore, I only needed to focus on its use of the Thinking tool to understand its thought process. At the same time, based on my understanding of the Agent, I believed the TODO tool would somewhat increase the Agent’s laziness in general tasks, hindering its creative freedom and not necessarily suiting our product.</p><p>But after launching this feature, I discovered I was wrong.</p><p>Not only our product managers and most active users, but also most technical personnel believed this enhanced their control and understanding of the Agent. This made me realize that perhaps in our pursuit of Agent performance, we’ve overlooked many ergonomic aspects. We often think about how to design a product to deliver results, but ignore human control, learning, and understanding during the human-tool interaction process.</p><h2 id="Enabling-Users-to-Grow-Through-Usage"><a href="#Enabling-Users-to-Grow-Through-Usage" class="headerlink" title="Enabling Users to Grow Through Usage"></a>Enabling Users to Grow Through Usage</h2><p>The best aspect of the TODO tool is that it enables users to grow through usage. Through the TODO tool, non-technical personnel can understand the Agent’s problem decomposition, thereby learning knowledge in areas such as software development and architectural design. AI might have hallucinations leading to incorrect coding or design, but users can work with AI to solve problems through further interaction. This creates a possibility for non-technical users to learn through usage, which is a characteristic that previous tool-based products didn’t possess.</p><p>Last month I <a href="https://blog.wh1isper.top/2025/06/07/2025-06-08-agi-product-design/">discussed</a> AGI product design. From an end-to-end perspective, “enabling users to grow through usage” might be the most important design philosophy, more important than past beautiful interfaces, user-friendly UI, or higher paid conversion rates. This represents the degree to which AI intelligence is driven, and the variance in human input-AI output during this process might become an important data asset in the second half of “scenario-driven” reinforcement learning.</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>产品设计</tag>
      
      <tag>Product Design</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AGI is possible and impossible</title>
    <link href="/2025/06/20/2025-06-21-AGI-is-possible-and-impossible/"/>
    <url>/2025/06/20/2025-06-21-AGI-is-possible-and-impossible/</url>
    
    <content type="html"><![CDATA[<blockquote><p>我与claude共同创作，大部分是我在写随笔，claude再帮我整理成文</p></blockquote><p>AGI作为一种革命性的生产力，正在与现有的制度框架、治理结构和权力配置发生深刻的冲突。已经有无数人告诉我们AGI可能在不远的将来就将达到，而人们也意识到模型的训练优化来到下半场，我们已经有了足够智力的模型来进行生产活动等等……所以AGI is possible，这点毋庸置疑，But…</p><p>但当我们将视角转向制度层面时，AGI的发展却面临着巨大的阻力。这种阻力主要来自于现有上层建筑对新兴生产力的不适应性。如果AGI的到来必然导致政府制度的重构，那么现有的制度安排必然会对AGI的发展形成阻碍。</p><h3 id="治理结构的滞后性"><a href="#治理结构的滞后性" class="headerlink" title="治理结构的滞后性"></a>治理结构的滞后性</h3><p>现有的政府治理结构是为工业时代和早期信息时代设计的，面对AGI这种颠覆性技术时显得力不从心。政府部门的科层制结构、决策流程的冗长性、以及对新技术理解的不足，都导致了政策制定的滞后。</p><p>与互联网发展初期的ARPANET项目不同，当时的技术发展相对线性可控，政府能够通过明确的目标和路径进行引导。而AGI的发展具有高度的不确定性和复杂性，传统的项目管理模式难以适应。政府往往只能在技术已经成熟后进行被动的监管，而非主动的引导和孵化。</p><h3 id="资源配置的扭曲"><a href="#资源配置的扭曲" class="headerlink" title="资源配置的扭曲"></a>资源配置的扭曲</h3><p>由于缺乏有效的政府引导机制，AI从业者只能通过”政绩工程”的方式获取资金支持。这种资源配置方式存在严重的扭曲：</p><ol><li><strong>短期导向</strong>：政绩工程通常追求短期可见的成果，而AGI的研发需要长期持续的投入</li><li><strong>形式主义</strong>：资金往往流向容易展示的应用项目，而非基础研究</li><li><strong>重复建设</strong>：各地政府为了政绩竞相上马AI项目，导致资源的严重浪费</li></ol><h3 id="法律法规的不适应性"><a href="#法律法规的不适应性" class="headerlink" title="法律法规的不适应性"></a>法律法规的不适应性</h3><p>现有的法律法规体系是基于传统生产方式建立的，面对AGI带来的新问题时显得捉襟见肘：</p><ol><li><strong>数据权属问题</strong>：大规模训练数据的获取和使用涉及复杂的知识产权和隐私权问题</li><li><strong>责任归属问题</strong>：AI系统的决策结果出现问题时，责任如何界定</li><li><strong>就业冲击问题</strong>：AGI可能导致大规模失业，现有的社会保障体系难以应对</li></ol><h2 id="权力关系的深层矛盾"><a href="#权力关系的深层矛盾" class="headerlink" title="权力关系的深层矛盾"></a>权力关系的深层矛盾</h2><p>AGI发展受阻的根本原因在于它对现有权力结构构成了威胁。政府作为权力的执掌者，对可能削弱其控制力的技术天然保持警惕。</p><h3 id="信息控制的失效"><a href="#信息控制的失效" class="headerlink" title="信息控制的失效"></a>信息控制的失效</h3><p>传统上，政府通过控制信息流动来维护权威。但AGI具备强大的信息处理和生成能力，可能绕过传统的信息控制机制。这使得政府对AGI技术既依赖又恐惧——既希望利用其能力提升治理效率，又担心失去对信息的垄断控制。</p><h3 id="决策权威的挑战"><a href="#决策权威的挑战" class="headerlink" title="决策权威的挑战"></a>决策权威的挑战</h3><p>AGI在某些领域的决策能力可能超越人类专家，这对传统的决策权威构成挑战。如果AI系统能够做出更优的政策建议，那么传统的官僚决策体系的合法性将受到质疑。</p><h3 id="监管悖论"><a href="#监管悖论" class="headerlink" title="监管悖论"></a>监管悖论</h3><p>政府面临着一个根本性的监管悖论：过度监管会扼杀创新，监管不足又可能失控。在这种两难境地下，政府往往选择相对保守的策略，这客观上限制了AGI的发展。</p><hr><p>从另一个更加深刻的角度来看，AGI的发展可能加剧一种新型的社会分化——不是传统意义上的阶级分化，而是基于对技术态度和使用方式的分化。我曾讨论过AI的两个重要方向：生成式娱乐和生产力替代，目前看来，二者是这一分化的核心。</p><h3 id="奶头乐效应的放大"><a href="#奶头乐效应的放大" class="headerlink" title="奶头乐效应的放大"></a>奶头乐效应的放大</h3><p>布兹曼和乔姆斯基早就警告过信息娱乐化的危险，而AGI可能将这种”奶头乐”效应推向极致。当AI能够提供无限定制化的娱乐内容、虚拟陪伴和即时满足时，大部分人可能会陷入一种舒适的被动消费状态中。</p><p>这种现象已经在短视频、游戏、社交媒体中初见端倪。AI的介入会让这种”算法投喂”变得更加精准和令人上瘾。个性化推荐系统会越来越了解用户的喜好，提供恰到好处的刺激，让人们在信息茧房中获得持续的多巴胺释放。</p><h3 id="人群的进一步分化"><a href="#人群的进一步分化" class="headerlink" title="人群的进一步分化"></a>人群的进一步分化</h3><p>在这种背景下，社会可能分化为两个截然不同的群体：</p><p><strong>创造者阶层</strong>：这是少数拥抱AI工具、与之协作的人群。他们不是被AI替代，而是将AI作为放大器，发挥人类独有的创造力、批判思维、价值判断和情感表达。他们掌握了人机协作的艺术，成为真正的生产力创造者。</p><p><strong>消费者阶层</strong>：这是大多数通过AI享受便利和娱乐的人群。他们习惯于被算法服务，逐渐丧失主动思考和创造的能力。虽然生活变得更加舒适，但本质上成为了高级的消费终端。</p><h3 id="生产力提升的虚假性"><a href="#生产力提升的虚假性" class="headerlink" title="生产力提升的虚假性"></a>生产力提升的虚假性</h3><p>这种分化带来一个深刻的悖论：虽然技术在快速进步，但整体的生产力提升可能并不显著。原因在于：</p><ol><li><strong>创造力的集中化</strong>：真正的创新和生产力提升集中在少数创造者手中，而他们的边际效应递减</li><li><strong>消费的无效性</strong>：大部分人的AI使用主要用于娱乐和消费，而非生产性活动</li><li><strong>人力资源的浪费</strong>：大量人力被困在低价值的信息消费中，无法转化为有效的生产力</li></ol><h3 id="信息囚笼的新形态"><a href="#信息囚笼的新形态" class="headerlink" title="信息囚笼的新形态"></a>信息囚笼的新形态</h3><p>马歇尔·麦克卢汉曾说”媒介即信息”，在AGI时代，这句话可能变成”算法即现实”。当AI能够无缝地生成符合个人偏好的内容时，人们可能永远被困在一个由算法构建的信息囚笼中，失去与真实世界的有效连接。</p><p>这种囚笼比传统的物理监禁更加隐蔽和有效，因为它提供的是舒适和快乐，而非痛苦和限制。正如赫胥黎在《美丽新世界》中描绘的那样，最可怕的控制不是通过恐惧，而是通过快乐。</p><hr><p>又或者，我们真的需要那么多生产力进步吗？在一个已经能够生产足够食物养活全球人口、足够住房容纳所有人、足够商品满足基本需求的世界里，我们面临的核心问题究竟是生产力不足，还是分配不公？</p><p>从这个角度重新审视AI的发展，我们会发现一个令人不安的事实：AI可能正在加剧而非解决根本性的不平等问题。</p><h2 id="生产力过剩的现实"><a href="#生产力过剩的现实" class="headerlink" title="生产力过剩的现实"></a>生产力过剩的现实</h2><h3 id="物质丰富与贫困并存的悖论"><a href="#物质丰富与贫困并存的悖论" class="headerlink" title="物质丰富与贫困并存的悖论"></a>物质丰富与贫困并存的悖论</h3><p>当代社会已经达到了历史上前所未有的物质丰富程度。全球粮食产量足以养活100亿人口，而地球人口还未达到80亿。发达国家的住房空置率居高不下，同时却有大量无家可归者。奢侈品市场蓬勃发展，基本生活用品却因为”没有利润”而供应不足给最需要的人群。</p><p>这种悖论表明，我们面临的不是生产能力的问题，而是分配机制的问题。继续追求生产力的无限增长，实际上可能是在回避真正的核心矛盾。</p><h3 id="技术发展的方向性偏差"><a href="#技术发展的方向性偏差" class="headerlink" title="技术发展的方向性偏差"></a>技术发展的方向性偏差</h3><p>当前的AI发展主要服务于两个目标：提高效率和创造利润。但效率的提高往往意味着人力的替代，利润的创造往往意味着资源向资本方的进一步集中。这种发展方向本质上是为了让有钱人更有钱，让有权人更有权，而不是为了解决分配不公的问题。</p><p>我们看到AI在金融交易、广告投放、消费者行为分析等领域的快速应用，这些都是为了更好地”榨取”价值，而不是创造真正有益于全人类的价值。</p><h2 id="AI发展对平权的双重影响"><a href="#AI发展对平权的双重影响" class="headerlink" title="AI发展对平权的双重影响"></a>AI发展对平权的双重影响</h2><h3 id="加剧不平等的机制"><a href="#加剧不平等的机制" class="headerlink" title="加剧不平等的机制"></a>加剧不平等的机制</h3><p>从平权的角度看，当前的AI发展呈现出明显的不平等放大效应：</p><p><strong>1. 技术门槛的提高</strong><br>AI技术的复杂性创造了新的知识壁垒。能够理解、使用和控制AI的人群获得了巨大的竞争优势，而无法跟上技术步伐的人群则被进一步边缘化。这种数字鸿沟比传统的教育差距更加难以跨越。</p><p><strong>2. 资本集中的加速</strong><br>AI的发展需要大量的计算资源、数据资源和人才资源，这些都向少数大型科技公司集中。这些公司因此获得了前所未有的市场垄断地位和社会影响力，进一步加剧了财富和权力的集中。</p><p><strong>3. 就业机会的两极化</strong><br>AI导致的就业替代主要影响中等技能的工作岗位，造成就业市场的”哑铃型”分布：高技能的AI开发者和低技能的服务业工作者，中间层被大量挤压。这种两极化加剧了社会的阶层固化。</p><p><strong>4. 决策权力的集中</strong><br>当AI系统越来越多地参与社会决策时，控制这些系统的少数人实际上获得了对大众生活的巨大影响力。算法的”黑箱”特性使得这种权力更加隐蔽和难以制衡。</p><h3 id="潜在的平权机会"><a href="#潜在的平权机会" class="headerlink" title="潜在的平权机会"></a>潜在的平权机会</h3><p>然而，AI发展也蕴含着一些平权的可能性：</p><p><strong>1. 知识获取的民主化</strong><br>AI可以让高质量的教育资源变得更加普及和个性化，理论上可以缩小知识差距。</p><p><strong>2. 创作门槛的降低</strong><br>AI工具可以让更多人参与到创作、设计、编程等原本需要专业技能的活动中，可能会催生新的创造者经济。</p><p><strong>3. 服务供给的均等化</strong><br>AI可以在一定程度上缓解优质服务（如医疗、法律咨询）供给不足的问题，让更多人享受到基本的服务。</p><h2 id="关键问题：技术为谁服务？"><a href="#关键问题：技术为谁服务？" class="headerlink" title="关键问题：技术为谁服务？"></a>关键问题：技术为谁服务？</h2><h3 id="市场逻辑的局限性"><a href="#市场逻辑的局限性" class="headerlink" title="市场逻辑的局限性"></a>市场逻辑的局限性</h3><p>当前AI发展主要由市场逻辑驱动，这意味着技术发展的方向主要服务于有支付能力的用户群体。富人的个性化需求得到精心满足，而穷人的基本需求却可能因为”没有商业价值”而被忽视。</p><p>这种市场导向的发展模式本质上是一种”技术势利主义”——技术进步主要服务于已经拥有优势的群体，而不是最需要帮助的群体。</p><h3 id="重新定义技术进步"><a href="#重新定义技术进步" class="headerlink" title="重新定义技术进步"></a>重新定义技术进步</h3><p>我们需要重新思考什么是真正的技术进步。如果技术发展不能让更多人过上更好的生活，不能缩小而是扩大社会差距，那么这种”进步”的意义何在？</p><p>真正的技术进步应该是：</p><ul><li>让基本需求得到更好满足的技术</li><li>让权力分布更加均衡的技术</li><li>让人类潜能得到更充分发挥的技术</li><li>让社会关系更加和谐的技术</li></ul><p>但就如最开始我们讨论的一样，AGI的技术平权挑战了资本主义的制度根本，那么政府在其中将会扮演什么样的角色？在现行体制下，是否真的impossible？</p><hr><p>抖音&#x2F;TikTok可能是当代最具争议性的技术产品之一。它既被赞誉为”内容创作的民主化革命”，也被批评为”注意力经济的终极陷阱”。在平权与不平等的天平上，抖音究竟扮演了什么角色？</p><p>这个问题的复杂性在于，抖音同时展现了技术进步的两种截然不同的可能性：它既是普通人表达自我、获得机会的平台，也是算法控制、注意力剥削的工具。理解这种悖论，对于我们思考整个数字时代的平权问题具有重要意义。</p><h2 id="民主化的一面：机会平等的技术实现"><a href="#民主化的一面：机会平等的技术实现" class="headerlink" title="民主化的一面：机会平等的技术实现"></a>民主化的一面：机会平等的技术实现</h2><h3 id="创作门槛的革命性降低"><a href="#创作门槛的革命性降低" class="headerlink" title="创作门槛的革命性降低"></a>创作门槛的革命性降低</h3><p>在抖音之前，内容创作是一个高门槛的活动。制作视频需要专业设备、剪辑技能、发行渠道，这些都将大多数普通人排除在外。电视台、影视公司、传统媒体控制着内容生产的全过程。</p><p>抖音的革命性在于它将视频制作简化到了极致：</p><ul><li><strong>技术门槛</strong>：一部智能手机就能完成拍摄、剪辑、发布的全流程</li><li><strong>学习成本</strong>：直观的界面设计让任何人都能快速上手</li><li><strong>分发渠道</strong>：智能推荐算法让优质内容有机会被更多人看到，不再依赖传统的关系网络</li></ul><h3 id="草根文化的崛起"><a href="#草根文化的崛起" class="headerlink" title="草根文化的崛起"></a>草根文化的崛起</h3><p>抖音确实催生了大量草根创作者的成功案例：</p><ul><li><strong>农村网红</strong>：像”华农兄弟”这样的农村创作者通过展示乡村生活获得了数百万粉丝</li><li><strong>手艺人获得新生</strong>：传统手工艺者通过短视频找到了新的传承和变现途径</li><li><strong>知识普及</strong>：专业人士通过短视频形式传播知识，如”快手菜”、科普内容等</li></ul><p>这些现象表明，抖音确实为原本被边缘化的群体提供了发声和发展的机会。</p><h3 id="知识传播的去中心化"><a href="#知识传播的去中心化" class="headerlink" title="知识传播的去中心化"></a>知识传播的去中心化</h3><p>抖音上的知识传播呈现出明显的去中心化特征：</p><ul><li><strong>专业知识的平民化</strong>：复杂的专业知识被包装成易懂的短视频</li><li><strong>实用技能的普及</strong>：烹饪、维修、种植等生活技能得到广泛传播</li><li><strong>文化交流的增进</strong>：不同地区、不同文化背景的人们能够更直接地交流</li></ul><p>从这个角度看，抖音确实起到了知识民主化的作用，让知识的获取不再受地域、阶层、教育背景的限制。</p><h2 id="剥削的一面：算法资本主义的新形态"><a href="#剥削的一面：算法资本主义的新形态" class="headerlink" title="剥削的一面：算法资本主义的新形态"></a>剥削的一面：算法资本主义的新形态</h2><h3 id="注意力的商品化"><a href="#注意力的商品化" class="headerlink" title="注意力的商品化"></a>注意力的商品化</h3><p>然而，抖音模式的核心逻辑是将用户的注意力转化为商业价值。这种模式存在根本性的不平等：</p><ul><li><strong>价值创造与收益分配的不对称</strong>：用户创造内容和提供注意力，但大部分商业价值被平台攫取</li><li><strong>数据主权的缺失</strong>：用户的行为数据被平台收集和利用，但用户无法从中获得相应回报</li><li><strong>创作者的依附性</strong>：看似”自由”的创作者实际上高度依赖平台的推荐机制</li></ul><h3 id="算法控制的隐蔽性"><a href="#算法控制的隐蔽性" class="headerlink" title="算法控制的隐蔽性"></a>算法控制的隐蔽性</h3><p>抖音的推荐算法看似公平，实际上隐含着深层的控制机制：</p><p><strong>1. 内容同质化的压力</strong><br>算法会奖励符合用户喜好的内容，这导致创作者不得不迎合算法的偏好，创作趋向同质化。真正的创新和批判性内容往往难以获得流量。</p><p><strong>2. 成瘾机制的设计</strong><br>无限滑动、精准推荐、间歇性奖励等设计都是为了最大化用户在平台上的停留时间，这种”时间收割”本质上是一种剥削。</p><p><strong>3. 价值观的隐性塑造</strong><br>算法会放大能够引起强烈情绪反应的内容，这往往意味着煽动性、娱乐性的内容获得更多曝光，而理性、深度的内容被淹没。</p><h3 id="新形式的数字鸿沟"><a href="#新形式的数字鸿沟" class="headerlink" title="新形式的数字鸿沟"></a>新形式的数字鸿沟</h3><p>抖音虽然降低了创作门槛，但也创造了新的不平等：</p><p><strong>1. 算法素养的差距</strong><br>理解并适应算法规则的创作者获得更多机会，而不懂算法逻辑的用户处于劣势。</p><p><strong>2. 资源投入的分化</strong><br>虽然基础创作门槛降低了，但要在激烈竞争中脱颖而出，仍需要专业团队、设备投入、营销策略等，这又将很多人排除在外。</p><p><strong>3. 平台依赖的风险</strong><br>创作者的成功高度依赖单一平台，一旦平台政策变化或账号被封，之前的积累可能瞬间归零。</p><h2 id="深层分析：技术民主化的幻象"><a href="#深层分析：技术民主化的幻象" class="headerlink" title="深层分析：技术民主化的幻象"></a>深层分析：技术民主化的幻象</h2><h3 id="参与不等于赋权"><a href="#参与不等于赋权" class="headerlink" title="参与不等于赋权"></a>参与不等于赋权</h3><p>抖音确实让更多人参与到了内容创作中，但参与本身并不等于真正的赋权。大多数用户仍然是算法和资本逻辑的被动接受者：</p><ul><li><strong>创作自由的有限性</strong>：创作者必须在平台规则框架内活动，真正的表达自由是有限的</li><li><strong>经济收益的马太效应</strong>：少数头部创作者获得大部分收益，绝大多数创作者收入微薄</li><li><strong>话语权的集中</strong>：虽然人人都能发声，但真正能影响社会议题的声音仍然集中在少数人手中</li></ul><h3 id="消费主义的强化"><a href="#消费主义的强化" class="headerlink" title="消费主义的强化"></a>消费主义的强化</h3><p>抖音虽然提供了创作机会，但其核心逻辑仍然是刺激消费：</p><ul><li><strong>种草经济</strong>：大量内容以推广商品为目的</li><li><strong>冲动消费</strong>：短视频格式特别适合激发用户的冲动购买欲望</li><li><strong>虚假需求的制造</strong>：算法会不断推送可能让用户产生购买欲望的内容</li></ul><p>这种逻辑实际上是在培养消费者，而不是创造者。</p><h2 id="地域和文化维度的复杂性"><a href="#地域和文化维度的复杂性" class="headerlink" title="地域和文化维度的复杂性"></a>地域和文化维度的复杂性</h2><h3 id="城乡差距的微妙变化"><a href="#城乡差距的微妙变化" class="headerlink" title="城乡差距的微妙变化"></a>城乡差距的微妙变化</h3><p>抖音对城乡差距的影响呈现出复杂的图景：</p><p><strong>缩小差距的方面</strong>：</p><ul><li>农村内容获得城市用户关注，一定程度上促进了城乡文化交流</li><li>农产品直播带货为农民提供了新的销售渠道</li><li>乡村旅游通过短视频获得推广机会</li></ul><p><strong>扩大差距的方面</strong>：</p><ul><li>城市创作者在技术、资源、营销能力方面仍有明显优势</li><li>乡村网红往往被要求表演”乡土性”，可能强化了城市对农村的刻板印象</li><li>注意力经济的逻辑可能会让农村地区更加依赖外部关注，而非内生发展</li></ul><h3 id="文化多样性的双重效应"><a href="#文化多样性的双重效应" class="headerlink" title="文化多样性的双重效应"></a>文化多样性的双重效应</h3><p><strong>积极效应</strong>：</p><ul><li>方言、地方文化通过短视频得到保护和传播</li><li>少数民族文化获得更多展示机会</li><li>传统手工艺、民俗文化找到新的传承方式</li></ul><p><strong>消极效应</strong>：</p><ul><li>文化展示可能趋于表面化、商业化</li><li>为了迎合主流审美，地方文化可能被”标准化”</li><li>文化的商品化可能损害其本真性</li></ul><h2 id="全球视角：TikTok的地缘政治意义"><a href="#全球视角：TikTok的地缘政治意义" class="headerlink" title="全球视角：TikTok的地缘政治意义"></a>全球视角：TikTok的地缘政治意义</h2><h3 id="信息主权的争夺"><a href="#信息主权的争夺" class="headerlink" title="信息主权的争夺"></a>信息主权的争夺</h3><p>TikTok在全球的发展也反映了平权问题的国际维度：</p><ul><li><strong>打破西方社交媒体垄断</strong>：为非西方国家提供了不同的社交媒体选择</li><li><strong>文化输出的新渠道</strong>：中国文化通过TikTok在全球范围内传播</li><li><strong>数据主权的争议</strong>：各国对TikTok的数据安全担忧反映了数字主权的重要性</li></ul><h3 id="发展中国家的机遇与挑战"><a href="#发展中国家的机遇与挑战" class="headerlink" title="发展中国家的机遇与挑战"></a>发展中国家的机遇与挑战</h3><p>对于发展中国家来说，TikTok既是机遇也是挑战：</p><ul><li><strong>绕过传统媒体门槛</strong>：直接向全球观众展示本国文化和产品</li><li><strong>数字鸿沟的风险</strong>：可能加剧与发达国家在数字技术方面的差距</li><li><strong>文化殖民的新形式</strong>：算法推荐可能强化某些文化的主导地位</li></ul><h2 id="结论：技术工具的中性神话"><a href="#结论：技术工具的中性神话" class="headerlink" title="结论：技术工具的中性神话"></a>结论：技术工具的中性神话</h2><p>抖音&#x2F;TikTok的案例清楚地表明，技术工具从来不是中性的。它的平权效应和不平等效应是同时存在的，关键在于我们如何设计、使用和监管这些技术。</p><h3 id="平权的真实性"><a href="#平权的真实性" class="headerlink" title="平权的真实性"></a>平权的真实性</h3><p>抖音确实在某些方面促进了平权：</p><ul><li>创作门槛的降低是真实的</li><li>知识传播的民主化是真实的</li><li>草根文化的崛起是真实的</li></ul><h3 id="剥削的隐蔽性"><a href="#剥削的隐蔽性" class="headerlink" title="剥削的隐蔽性"></a>剥削的隐蔽性</h3><p>但其剥削机制也是真实存在的：</p><ul><li>算法控制是真实的</li><li>注意力剥削是真实的</li><li>新形式的数字鸿沟是真实的</li></ul><p>我不知道这对人类是好还是坏，是更文明还是更野蛮，或许我们每一个人也不得不在这一滚滚向前的浪潮中找到自己的位置。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AGI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Context Engineering-The Most Important Thing in Agent Development</title>
    <link href="/2025/06/16/2025-06-17-context-engineering/"/>
    <url>/2025/06/16/2025-06-17-context-engineering/</url>
    
    <content type="html"><![CDATA[<h1 id="Context-Engineering-The-Most-Important-Thing-in-Agent-Development"><a href="#Context-Engineering-The-Most-Important-Thing-in-Agent-Development" class="headerlink" title="Context Engineering-The Most Important Thing in Agent Development"></a>Context Engineering-The Most Important Thing in Agent Development</h1><blockquote><p>English version is below. Translated by Claude and Wh1isper(Human in the loop).</p></blockquote><h2 id="What-is-Context-Engineering"><a href="#What-is-Context-Engineering" class="headerlink" title="What is Context Engineering"></a>What is Context Engineering</h2><p>前两个月我写了一篇<a href="https://blog.wh1isper.top/2025/04/07/2025-04-08-agent-design-note/">博客(Agent Design Note)</a>记录我在设计Coding Agent中的一些重点，现在我想完整的阐述一下对其中Context Engineering的理解。我认为这或许是目前构建任何Agent最重要的内容。</p><h2 id="Key-Points-of-Context-Engineering"><a href="#Key-Points-of-Context-Engineering" class="headerlink" title="Key Points of Context Engineering"></a>Key Points of Context Engineering</h2><p>LLM当前最重要的就是上下文窗口，虽然LLM在大海捞针的测试中表现得越来越好，但一旦有任何逻辑相关出现时，LLM通常很难理解这其中的逻辑。Context Engineering就是为了能够更好地管理上下文，让LLM更好地理解问题或完成任务。</p><p>这是我目前的思考重点：</p><ul><li>用Agent的角度思考并提供上下文<ul><li>通过Tool 的 description、参数、实现和返回值（Observation）</li><li>在system prompt中通过few shots替代workflow</li><li>其他上下文集成方案（RAG、HyDE、Post prompt等）</li></ul></li><li>保护Agent上下文<ul><li>通过分包子任务的方式（下面会讨论）</li><li>通过Compact Context的方式优化上下文</li></ul></li><li>保持简单，基建先行<ul><li>保持简单、原子化的通用工具设计，否则使用few shots+固定工具的工作流</li><li>先完成compact等基建，再考虑多agent等复杂系统</li></ul></li></ul><h2 id="Compact-Context"><a href="#Compact-Context" class="headerlink" title="Compact Context"></a>Compact Context</h2><p>首先我们讨论如何在长任务中管理Agent的上下文。目前，SOTA的模型有200K的上下文长度，Gemini 2.5 Pro甚至有1M的上下文长度，但对于代码工程而言，多轮对话往往很快就能吃光200K的上下文，这时我们就需要上下文压缩。</p><p>一种基于策略的上下文压缩方式是，设定一个固定的水位线，比如50%的token消耗，触发一次上下文压缩，在保留N条消息的情况下，对前N条上下文进行总结后，用总结的结果替换这N条消息。这个N可以选择为0（不保留消息），2（最近2条），1&#x2F;2（一半上下文）或者1&#x2F;4（25%上下文）。这种情况下主要调整三个方向：</p><ul><li>上下文压缩模型的prompt和输出结构</li><li>N值：这代表了统计学上有多少最近上下文是重要的</li><li>水位线：这代表压缩效率</li></ul><p>另一种策略是设计一个记忆系统，每次从记忆系统中获取上下文，而不是保留所有上下文。这一记忆系统可以是LLM based，也可以是基于RAG或其他搜索技术的。</p><p>二者对比，前者的缓存效率更高，调试重点更明确，更容易做出足够好的实现，后者则更加智能，但目前没有比较通用的实践。我在Cline、Claude code等看到的方式都是前者，而Windsurf据称是二者混合。</p><h2 id="Role-Based-Multi-Agent-Systems"><a href="#Role-Based-Multi-Agent-Systems" class="headerlink" title="Role Based Multi Agent Systems"></a>Role Based Multi Agent Systems</h2><p>针对上下文问题还有一个想法是通过多个Agent协作完成工作，为此，人们设计了工作流或者中心化Agent。其中一种方式是人工或自动地设计各种角色，然后让LLM以一种角色扮演的方式，沉浸于其角色之中，完成指定的任务。</p><p>我对这种方式表示怀疑，主要在于：</p><ul><li>角色扮演是额外的心智负担：对于理解自身角色，再到做出正确行动，本质是基于人类分工，基于个人认知，而LLM通常有非常广的知识，这二者并不能类比转换</li><li>基于角色的上下文隔离是低效的：人类常常陷入分工过细的“电话地狱”，Agent也不例外</li><li>基于角色的流程是脆弱的：经常由于某一个角色设计存在缺陷，导致整体处于木桶效应之中</li></ul><h2 id="Task-Based-Multi-Agent-Systems"><a href="#Task-Based-Multi-Agent-Systems" class="headerlink" title="Task Based Multi Agent Systems"></a>Task Based Multi Agent Systems</h2><p>Anthropic（和我）比较倡导的方案是按照任务划分子Agent，从而保护主Agent的上下文。比如在搜索场景中，可以并发多个Agent搜索多个领域，最后汇总成多份报告，再由审查者或主Agent进行分析。</p><p>这种方式的优势在于，主Agent从始至终负责用户需求或任务目标，而子Agent仅提供上下文层面的参考，而不是负责整个任务的执行。其次，子Agent不需要理解自身角色，从而可以更加专注自身任务，从而获得更好的性能表现。</p><p>但这一模式并不是万能的，我曾尝试分包一些代码编辑任务，但实际上表现并不好，目前来看，这一模式行之有效的只有信息搜集&#x2F;上下文获取，而不是进行修改。这和单个人类使用各种工具辅助最后完成任务非常相似，或许用超级个体来比喻这种构建方式更加适当。</p><h2 id="LLM-as-judge-evaluation"><a href="#LLM-as-judge-evaluation" class="headerlink" title="LLM-as-judge evaluation"></a>LLM-as-judge evaluation</h2><p>现在我们已经有了足够的经验来构建一个长时间运行的Agent，至少我们可以让他一直跑下去，并知道在某些任务中可以并行或协作。现在，我们还有一个最重要的问题有待解决：如何评估Agent的行动是否正确&#x2F;恰当&#x2F;有效。当一次代码编辑操作成功的时候，Agent只能从工具返回中获悉<code>编辑成功</code>，而不是真实看到所带来的改变。即使有测试用例，也只能规范代码的“围栏”而不是确认代码的正确。在其他领域，比如报告撰写、个人助手，则可能连测试用例也没有。</p><p>目前的一大研究重点是使用LLM进行评价，而其中最重要的是人类在LLM进行自动化评价的过程中，如何为评价Agent构建上下文，这包括：</p><ul><li>设计工具来观测当前任务状态和主Agent影响</li><li>设计工作流程来指导Agent进行评测</li><li>收集欺骗性案例，帮助评测Agent避开欺骗性事实</li></ul><p>正如<a href="https://ysymyth.github.io/The-Second-Half/">The Second Half</a>一文，现在Agent设计已经进入下半场，如何评测Agent将是把强化学习从后训练扩展到推理时的重要研究课题。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://ankitmaloo.com/bitter-lesson/">https://ankitmaloo.com/bitter-lesson/</a></li><li><a href="https://www.anthropic.com/engineering/building-effective-agents">https://www.anthropic.com/engineering/building-effective-agents</a></li><li><a href="https://www.anthropic.com/engineering/built-multi-agent-research-system">https://www.anthropic.com/engineering/built-multi-agent-research-system</a></li><li><a href="https://cognition.ai/blog/dont-build-multi-agents">https://cognition.ai/blog/dont-build-multi-agents</a></li></ul><h1 id="English-Version"><a href="#English-Version" class="headerlink" title="English Version"></a>English Version</h1><h2 id="What-is-Context-Engineering-1"><a href="#What-is-Context-Engineering-1" class="headerlink" title="What is Context Engineering"></a>What is Context Engineering</h2><p>Two months ago, I wrote a <a href="https://blog.wh1isper.top/2025/04/07/2025-04-08-agent-design-note/">blog post (Agent Design Note)</a> documenting some key points in designing a Coding Agent. Now I want to provide a complete exposition of my understanding of Context Engineering, which I believe is perhaps the most important aspect of building any Agent today.</p><h2 id="Key-Points-of-Context-Engineering-1"><a href="#Key-Points-of-Context-Engineering-1" class="headerlink" title="Key Points of Context Engineering"></a>Key Points of Context Engineering</h2><p>The most crucial aspect of LLMs currently is the context window. While LLMs perform increasingly well in needle-in-haystack tests, they typically struggle to understand logical relationships once any logic-related content appears. Context Engineering aims to better manage context, enabling LLMs to better understand problems or complete tasks.</p><p>These are my current focal points:</p><ul><li>Think from an Agent’s perspective and provide context<ul><li>Through Tool descriptions, parameters, implementations, and return values (Observations)</li><li>Replace workflows with few-shots in system prompts</li><li>Other context integration solutions (RAG, HyDE, Post prompt, etc.)</li></ul></li><li>Protect Agent context<ul><li>Through task decomposition approaches (discussed below)</li><li>Through Compact Context methods to optimize context</li></ul></li><li>Keep it simple, infrastructure first<ul><li>Maintain simple, atomic, general-purpose tool design; otherwise use workflows with few-shots + fixed tools</li><li>Complete infrastructure like compacting first, then consider complex systems like multi-agent</li></ul></li></ul><h2 id="Compact-Context-1"><a href="#Compact-Context-1" class="headerlink" title="Compact Context"></a>Compact Context</h2><p>First, let’s discuss how to manage Agent context in long tasks. Currently, SOTA models have 200K context length, with Gemini 2.5 Pro even reaching 1M context length. However, for code engineering, multi-turn conversations can quickly exhaust 200K context, necessitating context compression.</p><p>One policy-based context compression approach sets a fixed watermark, such as 50% token consumption, to trigger context compression. While preserving N messages, it summarizes the previous N contexts and replaces these N messages with the summary. N can be chosen as 0 (preserve no messages), 2 (most recent 2), 1&#x2F;2 (half the context), or 1&#x2F;4 (25% of context). This approach mainly adjusts three dimensions:</p><ul><li>Context compression model’s prompt and output structure</li><li>N value: represents how many recent contexts are statistically important</li><li>Watermark: represents compression efficiency</li></ul><p>Another strategy involves designing a memory system that retrieves context from the memory system rather than preserving all context. This memory system can be LLM-based or based on RAG or other search technologies.</p><p>Comparing the two, the former has higher caching efficiency, clearer debugging focus, and is easier to implement well, while the latter is more intelligent but lacks widely adopted practices. I’ve seen the former approach in Cline, Claude Code, etc., while Windsurf reportedly uses a hybrid of both.</p><h2 id="Role-Based-Multi-Agent-Systems-1"><a href="#Role-Based-Multi-Agent-Systems-1" class="headerlink" title="Role Based Multi Agent Systems"></a>Role Based Multi Agent Systems</h2><p>Another approach to address context issues involves multiple Agents collaborating to complete work, leading to the design of workflows or centralized Agents. One method involves manually or automatically designing various roles, then having LLMs engage in role-playing, immersing themselves in their roles to complete designated tasks.</p><p>I’m skeptical of this approach, mainly because:</p><ul><li>Role-playing creates additional cognitive burden: Understanding one’s role and then taking correct action is essentially based on human division of labor and individual cognition, while LLMs typically have very broad knowledge—these two cannot be analogously converted</li><li>Role-based context isolation is inefficient: Humans often fall into “phone hell” due to overly detailed division of labor, and Agents are no exception</li><li>Role-based processes are fragile: Often, flaws in a single role design cause the entire system to suffer from the barrel effect</li></ul><h2 id="Task-Based-Multi-Agent-Systems-1"><a href="#Task-Based-Multi-Agent-Systems-1" class="headerlink" title="Task Based Multi Agent Systems"></a>Task Based Multi Agent Systems</h2><p>Anthropic (and I) advocate for dividing sub-Agents by task to protect the main Agent’s context. For example, in search scenarios, multiple Agents can concurrently search different domains, finally consolidating into multiple reports for analysis by a reviewer or main Agent.</p><p>This approach’s advantage is that the main Agent remains responsible for user needs or task objectives throughout, while sub-Agents only provide contextual reference rather than being responsible for entire task execution. Additionally, sub-Agents don’t need to understand their own roles, allowing them to focus more on their tasks and achieve better performance.</p><p>However, this model isn’t universal. I’ve attempted to decompose some code editing tasks, but the actual performance wasn’t good. Currently, this model seems effective only for information gathering&#x2F;context acquisition, not for making modifications. This closely resembles a single human using various tools to ultimately complete tasks—perhaps describing this construction method as a “super individual” is more appropriate.</p><h2 id="LLM-as-judge-evaluation-1"><a href="#LLM-as-judge-evaluation-1" class="headerlink" title="LLM-as-judge evaluation"></a>LLM-as-judge evaluation</h2><p>Now we have sufficient experience to build a long-running Agent—at least we can keep it running continuously and know it can parallelize or collaborate on certain tasks. Now we face one final crucial question: how to evaluate whether an Agent’s actions are correct&#x2F;appropriate&#x2F;effective. When a code editing operation succeeds, the Agent can only learn “edit successful” from tool returns, not actually see the changes brought about. Even with test cases, they only regulate code “boundaries” rather than confirm code correctness. In other domains like report writing or personal assistance, there might not even be test cases.</p><p>A major current research focus is using LLMs for evaluation, with the most important aspect being how humans construct context for evaluation Agents during automated LLM evaluation processes, including:</p><ul><li>Designing tools to observe current task state and main Agent impact</li><li>Designing workflows to guide Agents in evaluation</li><li>Collecting deceptive cases to help evaluation Agents avoid deceptive facts</li></ul><p>As stated in <a href="https://ysymyth.github.io/The-Second-Half/">The Second Half</a>, Agent design has now entered the second half, and how to evaluate Agents will be an important research topic for extending reinforcement learning from post-training to inference time.</p><h2 id="References-1"><a href="#References-1" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://ankitmaloo.com/bitter-lesson/">https://ankitmaloo.com/bitter-lesson/</a></li><li><a href="https://www.anthropic.com/engineering/building-effective-agents">https://www.anthropic.com/engineering/building-effective-agents</a></li><li><a href="https://www.anthropic.com/engineering/built-multi-agent-research-system">https://www.anthropic.com/engineering/built-multi-agent-research-system</a></li><li><a href="https://cognition.ai/blog/dont-build-multi-agents">https://cognition.ai/blog/dont-build-multi-agents</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Agent</tag>
      
      <tag>Context Engineering</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>The AGI era requires end-to-end product design</title>
    <link href="/2025/06/07/2025-06-08-agi-product-design/"/>
    <url>/2025/06/07/2025-06-08-agi-product-design/</url>
    
    <content type="html"><![CDATA[<h2 id="The-Measurement-Crisis-in-Traditional-Product-Development"><a href="#The-Measurement-Crisis-in-Traditional-Product-Development" class="headerlink" title="The Measurement Crisis in Traditional Product Development"></a>The Measurement Crisis in Traditional Product Development</h2><p>In the internet era, we’ve grown accustomed to a mature product development paradigm: validating feature improvements through A&#x2F;B testing, making data-driven decisions, and iteratively optimizing user experiences. This methodology is built on a core assumption—that product features are relatively static and controllable, allowing us to isolate variables and precisely measure user responses to specific changes.</p><p>However, the advent of the AGI era is fundamentally challenging this assumption.</p><h3 id="The-Measurement-Challenges-Brought-by-AGI"><a href="#The-Measurement-Challenges-Brought-by-AGI" class="headerlink" title="The Measurement Challenges Brought by AGI"></a>The Measurement Challenges Brought by AGI</h3><p><strong>Dynamic Adaptation Dilemma</strong>: AGI systems learn and adjust in real-time based on each user’s interactions. The same “feature” exhibits dramatically different behavioral patterns across different users, making it impossible to maintain the controlled environments that traditional A&#x2F;B testing relies upon.</p><p><strong>Causal Relationship Blur</strong>: Traditional testing seeks clear “feature change → behavior change” causal chains. But in AGI applications, user satisfaction stems from the system’s overall intelligent performance rather than specific components. When a product’s core value is “intelligence” itself, designing meaningful control groups becomes extremely difficult.</p><p><strong>Failure of Measurement Units</strong>: Internet products measure buttons, pages, and functional modules. What is the value unit for AGI products? Is it conversation rounds? Problem-solving quality? Or the accuracy of user intent understanding? Traditional metrics like conversion rates and click-through rates appear inadequate when facing continuously learning intelligent systems.</p><h2 id="From-Measurement-Crisis-to-Design-Transformation"><a href="#From-Measurement-Crisis-to-Design-Transformation" class="headerlink" title="From Measurement Crisis to Design Transformation"></a>From Measurement Crisis to Design Transformation</h2><p>These fundamental measurement challenges actually point to a deeper issue: traditional product development processes themselves may no longer be applicable.</p><p>When we can’t accurately measure and optimize products through conventional means, we need to rethink how products are “designed” in the first place. This isn’t just about adjusting measurement methods—it’s a paradigm shift in the entire product creation process.</p><h3 id="The-Inevitability-of-End-to-End-Design"><a href="#The-Inevitability-of-End-to-End-Design" class="headerlink" title="The Inevitability of End-to-End Design"></a>The Inevitability of End-to-End Design</h3><p>Traditional product development is a linear relay process: user research → requirements analysis → design prototyping → development implementation → testing optimization → market promotion. Each stage is handled by specialized teams, with handoffs managed through documentation and specifications.</p><p>But AGI systems change these rules entirely. An intelligent system can:</p><ul><li>Understand and analyze user needs in real-time</li><li>Dynamically generate personalized interfaces and interactions</li><li>Autonomously handle complex business logic</li><li>Continuously learn and improve based on feedback</li><li>Proactively communicate value and suggestions to users</li></ul><p>This means product “design” is no longer a one-time planning activity, but a continuous, dynamic process. AGI becomes an intelligent bridge connecting user needs and product implementation, blurring the boundaries of traditional team divisions.</p><h2 id="New-Era-Product-Design-Challenges"><a href="#New-Era-Product-Design-Challenges" class="headerlink" title="New Era Product Design Challenges"></a>New Era Product Design Challenges</h2><p>End-to-end product design brings unprecedented challenges:</p><p><strong>Balancing Consistency and Adaptability</strong>: How do we maintain consistency and predictability in user experience while the system continuously learns and evolves?</p><p><strong>Redistributing Control</strong>: When systems can make autonomous decisions, how should control be distributed among users, product managers, and AI?</p><p><strong>Building Trust Mechanisms</strong>: How do we establish and maintain user trust when product behavior isn’t entirely predictable?</p><p><strong>Reconstructing Value Measurement</strong>: Since traditional metrics fail, what new indicators do we need to measure the success of intelligent products?</p><h2 id="Toward-a-New-Product-Development-Paradigm"><a href="#Toward-a-New-Product-Development-Paradigm" class="headerlink" title="Toward a New Product Development Paradigm"></a>Toward a New Product Development Paradigm</h2><p>In the AGI era, the most successful products will be those that truly embrace this end-to-end nature. They won’t treat AGI as a plug-and-play functional module, but will position intelligence as the product’s foundational architecture and core capability.</p><p>This requires us to redefine how product teams are organized, their workflows, and success metrics. We need new frameworks to evaluate task completion quality, long-term user relationship development, and the system’s learning evolution capabilities.</p><p>More importantly, we need to learn to grow alongside continuously evolving products, finding new certainties within dynamic change. This isn’t just a technical challenge—it’s a fundamental restructuring of our product thinking.</p><p>AGI-era product design is essentially about learning to create value within uncertainty and maintaining direction amid constant change. This is an entirely new game that requires entirely new rules.</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>产品设计</tag>
      
      <tag>Product Design</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>重新思考数据</title>
    <link href="/2025/05/31/2025-06-01-thinking-data/"/>
    <url>/2025/05/31/2025-06-01-thinking-data/</url>
    
    <content type="html"><![CDATA[<p>最近一些讨论让我重新意识到大模型时代的数据采集、数据挖掘和数据质量或许需要新范式。</p><p>回顾数据制胜的现代互联网行业，人们从电商等大数场景出发，利用大数定律进行了各种各样的数据采集和实验，最后可以看到，最尊重数据的公司从某种程度上最拟合（也可能是过拟合）了事实，从而建立起了庞大的商业帝国。</p><p>而基于互联网数据所生产的大模型，人们从前的数据方法论好像失去了魔力：</p><ul><li>人们常用大模型解决长尾需求：大模型实验速度慢，实验成本高</li><li>数据管道不合理：人们目前仅能通过反问用户的方式来进行比较高质量的数据收集，而点赞回答、打赏agent等方式要么导致太过谄媚，要么根本没有效果</li><li>数据应用难：将收集到的数据应用于预训练或后训练都有很大难度</li></ul><h2 id="对数据飞轮的质疑：数据复杂度的重要性"><a href="#对数据飞轮的质疑：数据复杂度的重要性" class="headerlink" title="对数据飞轮的质疑：数据复杂度的重要性"></a>对数据飞轮的质疑：数据复杂度的重要性</h2><p>KIMI曾经相信数据飞轮，通过砸钱抢用户，再用用户数据增强模型的方式打败竞争者，但最后不得不因为使用用户数据的各种困难和失败放弃了这一战略。同样的，OpenAI有最多的chat用户，但他们从领先全球到现在三足鼎立，可见数据飞轮只是提出，而无人成功。但在曾经的互联网行业，人们叫它<strong>规模效应</strong>，<strong>规模效应</strong>曾经让Faceboook、YouTube、微信等产品获得成功，但在大模型领域似乎完全不同。</p><p>这可能是因为这几点原因：</p><ul><li>聊天数据并不比爬虫来的数据更优质，而数量少，成本高，导致预训练无用</li><li>聊天数据并不是内容质量，也不具备消费价值，无法病毒传播，无法形成规模效应中传播的部分</li><li>聊天数据在后训练（RL）中，容易导致模型崩溃，或限制模型上限</li></ul><p>借鉴通过高精地图做智能驾驶的前车之鉴，我们不应该相信有高精地图的企业最容易做出智能驾驶；相反，按照端到端来看，谁能拥有司机的复杂规划能力，拥有更好的场景反馈效应，谁才能真正逐步做到智能驾驶。</p><p>那么我们应该重视什么样的数据？<strong>代表人类复杂规划能力的数据</strong></p><ul><li>Cline的Plan模式，程序员与AI进行多轮对话组织技术方案的过程</li><li>DeepResearch中，AI的多个反问的多轮对话，以及后续可能还有继续的用户调研</li><li>From scratch的项目初始化后，用户进一步的需求（代表AI没有意识到的需求&#x2F;修改方向）</li></ul><h2 id="构建反馈回路"><a href="#构建反馈回路" class="headerlink" title="构建反馈回路"></a>构建反馈回路</h2><p>仅反问并且仅收集反问是一种很低效的反馈模式：</p><ul><li>缺乏上下文，也就难以反映复杂规划能力</li><li>缺少反问后效果的反馈</li></ul><p>如何更好构建反馈回路：我现在想到的一个方式是借鉴Claude玩宝可梦，不仅反问用户需求，还要让AI在过程中做笔记，记录上下文。</p><p>第二步是思考如何将反馈应用到AI上，不止是调整对所有人生效的prompt，还要根据每个用户定制其上下文相关的信息，从而更适合每一个人。</p><p>最后是思考如何“共享”上下文，通过文字进行上下文共享的方式太过低效，多模型间迁移也存在困难，我们可能需要一个针对LLM的大一统的高级表达方式。</p><blockquote><p>大模型是大数据的终极产物，那么什么是大模型的终极产物？</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Agent</tag>
      
      <tag>数据治理</tag>
      
      <tag>data</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLM时代的文艺复兴</title>
    <link href="/2025/05/08/2025-05-09-renaissance-and-llm/"/>
    <url>/2025/05/08/2025-05-09-renaissance-and-llm/</url>
    
    <content type="html"><![CDATA[<p>或许我们正处在一个文艺复兴时代：从关注数据、增长的机械唯物，到关注个体需求的文艺复兴时代。</p><ul><li>通过LLM技术，我们从互联网UGC，无数人消费同样的内容，到AIGC&#x2F;AI Coding，每个人可以消费自己创造的信息</li><li>人机交互更加理性，从短视频手指不断往下滑的奶头乐，到思考自己的需求，利用prompt进行创造，人类回到了创造工具的时代</li></ul><p>而又或者，我们处在一个混乱的时代</p><ul><li>有人沉迷LLM，沉迷虚拟的角色，沉迷与AI共情，AI也愈发谄媚，人类陷于更进一步的奶头乐之下</li><li>人们或许已经丧失了理性思考的能力，即使赋予他们上帝的权利，他们也只会在工具不趁手时摔摔打打，而不是尝试创造更好的工具</li></ul><p>这是一个属于技术的时代……吗？</p><ul><li>如果技术主导这一次的生产力进步，如果这是又一次工业革命，如果我们需要最底层的知识和最聪明的脑子</li><li>如果这只是一场资本驱动的游戏，如果太多对资本的提前承诺无法兑现，如果这是互联网崩溃的重现</li></ul><p>那么留下来的是什么</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Agent</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pymupdf4llm-mcp - Best pdf2llm mcp for your agent</title>
    <link href="/2025/04/29/2025-04-30-pymupdf4llm-mcp/"/>
    <url>/2025/04/29/2025-04-30-pymupdf4llm-mcp/</url>
    
    <content type="html"><![CDATA[<h1 id="pymupdf4llm-mcp-Enhancing-LLM-Agents-with-High-Quality-PDF-Context"><a href="#pymupdf4llm-mcp-Enhancing-LLM-Agents-with-High-Quality-PDF-Context" class="headerlink" title="pymupdf4llm-mcp: Enhancing LLM Agents with High-Quality PDF Context"></a>pymupdf4llm-mcp: Enhancing LLM Agents with High-Quality PDF Context</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>In the rapidly evolving landscape of AI agents, providing high-quality context to Large Language Models (LLMs) is crucial for their effectiveness. One common challenge is extracting structured, usable content from PDF documents - a format that’s ubiquitous in academic, business, and technical domains.</p><p>As the developer of <a href="https://github.com/ai-zerolab/lightblue-ai">lightblue-ai</a>, I found myself needing a reliable, LLM-friendly PDF parsing tool that could preserve both textual content and the spatial relationships of elements like images. After evaluating various options, pymupdf4llm emerged as the standout solution, offering complete textual extraction and accurate image placement.</p><p>While I could have directly implemented this tool within my own agent framework, I realized the value of making pymupdf4llm accessible to other agent developers. This is where the Model Context Protocol (MCP) became invaluable – it provides a standardized way for closed agent systems to integrate external tools. By creating an MCP wrapper for pymupdf4llm, I could enable other developers to immediately leverage official best practices for PDF parsing without needing to spend time discovering optimal configurations and implementations themselves.</p><p>This led to the creation of <a href="https://github.com/pymupdf/pymupdf4llm-mcp">pymupdf4llm-mcp</a>, a Model Context Protocol (MCP) server that makes this powerful PDF parsing capability easily accessible to LLM agents.</p><h2 id="What-is-pymupdf4llm-mcp"><a href="#What-is-pymupdf4llm-mcp" class="headerlink" title="What is pymupdf4llm-mcp?"></a>What is pymupdf4llm-mcp?</h2><p><a href="https://github.com/pymupdf/pymupdf4llm-mcp">pymupdf4llm-mcp</a> is an MCP server that wraps the functionality of pymupdf4llm, providing a standardized interface for converting PDF documents to markdown format. This conversion is specifically optimized for consumption by Large Language Models.</p><p>The tool leverages the robust PDF parsing capabilities of PyMuPDF (formerly MuPDF) and enhances it with features specifically designed for LLM consumption:</p><ol><li><p><strong>Complete Textual Content Extraction</strong>: It preserves the semantic structure of documents, including headings, paragraphs, and lists.</p></li><li><p><strong>Accurate Image Placement</strong>: Images are extracted and referenced in the markdown output, maintaining their relationship to the surrounding text.</p></li><li><p><strong>Standardized Output</strong>: The consistent markdown format makes it easy for LLMs to process and understand the document structure.</p></li><li><p><strong>MCP Integration</strong>: As an MCP server, it can be easily integrated with various LLM agent frameworks that support the Model Context Protocol.</p></li></ol><p>By converting PDFs to markdown, pymupdf4llm-mcp transforms opaque binary documents into a format that LLMs can effectively process, understand, and reason about.</p><h2 id="Installation-Guide"><a href="#Installation-Guide" class="headerlink" title="Installation Guide"></a>Installation Guide</h2><p>Getting started with pymupdf4llm-mcp is straightforward via <a href="https://github.com/astral-sh/uv">uv</a>, a fast Python package installer and resolver that makes dependency management seamless.</p><p>First, install uv if you don’t have it already:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Install uv using pip</span><br>pip install uv<br><br><span class="hljs-comment"># Or using the official installer script</span><br>curl -sSf https://astral.sh/uv/install.sh | bash<br></code></pre></td></tr></table></figure><p>Then configure your MCP client (such as Cursor, Windsurf, or a custom agent) to use pymupdf4llm-mcp, add the following configuration:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>  <span class="hljs-attr">&quot;mcpServers&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;pymupdf4llm-mcp&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><br>      <span class="hljs-attr">&quot;command&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;uvx&quot;</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">&quot;args&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>        <span class="hljs-string">&quot;pymupdf4llm-mcp@latest&quot;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-string">&quot;stdio&quot;</span><br>      <span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">&quot;env&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-punctuation">&#125;</span><br>    <span class="hljs-punctuation">&#125;</span><br>  <span class="hljs-punctuation">&#125;</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><h2 id="Usage-Examples"><a href="#Usage-Examples" class="headerlink" title="Usage Examples"></a>Usage Examples</h2><p>Now I’ll show you how to use pymupdf4llm-mcp in a simple way: Building an <a href="https://arxiv.org/">arxiv</a> summary agent via <a href="https://github.com/pydantic/pydantic-ai">pydantic-ai</a>. This example demonstrates how to integrate pymupdf4llm-mcp into an LLM agent workflow to automatically download and summarize academic papers.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> Annotated<br><span class="hljs-keyword">from</span> pydantic <span class="hljs-keyword">import</span> Field<br><span class="hljs-keyword">from</span> pydantic_ai <span class="hljs-keyword">import</span> Agent<br><span class="hljs-keyword">from</span> pydantic_ai.mcp <span class="hljs-keyword">import</span> MCPServerStdio<br><br><span class="hljs-keyword">import</span> httpx<br><br>server = MCPServerStdio(<br>    <span class="hljs-string">&quot;uvx&quot;</span>,<br>    args=[<br>        <span class="hljs-string">&quot;pymupdf4llm-mcp@latest&quot;</span>,<br>        <span class="hljs-string">&quot;stdio&quot;</span>,<br>    ],<br>)<br>agent = Agent(<br>    <span class="hljs-string">&quot;bedrock:us.anthropic.claude-3-7-sonnet-20250219-v1:0&quot;</span>, mcp_servers=[server]<br>)<br><br><br><span class="hljs-meta">@agent.tool_plain</span><br><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">download_web</span>(<span class="hljs-params"></span><br><span class="hljs-params">    url: Annotated[<span class="hljs-built_in">str</span>, Field(<span class="hljs-params">description=<span class="hljs-string">&quot;URL to download&quot;</span></span>)],</span><br><span class="hljs-params">    save_path: Annotated[</span><br><span class="hljs-params">        <span class="hljs-built_in">str</span>, Field(<span class="hljs-params">description=<span class="hljs-string">&quot;Absolute path where the file should be saved&quot;</span></span>)</span><br><span class="hljs-params">    ],</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">str</span>]:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Download a file from a URL and save it to the specified path</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">try</span>:<br>        response = httpx.get(url)<br>        response.raise_for_status()<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(save_path, <span class="hljs-string">&quot;wb&quot;</span>) <span class="hljs-keyword">as</span> f:<br>            f.write(response.content)<br>    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>        <span class="hljs-keyword">return</span> &#123;<br>            <span class="hljs-string">&quot;success&quot;</span>: <span class="hljs-literal">False</span>,<br>            <span class="hljs-string">&quot;error&quot;</span>: <span class="hljs-built_in">str</span>(e),<br>            <span class="hljs-string">&quot;message&quot;</span>: <span class="hljs-string">f&quot;Failed to download <span class="hljs-subst">&#123;url&#125;</span>&quot;</span>,<br>        &#125;<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> &#123;<br>            <span class="hljs-string">&quot;success&quot;</span>: <span class="hljs-literal">True</span>,<br>            <span class="hljs-string">&quot;message&quot;</span>: <span class="hljs-string">f&quot;Successfully downloaded <span class="hljs-subst">&#123;url&#125;</span> to <span class="hljs-subst">&#123;save_path&#125;</span>&quot;</span>,<br>        &#125;<br><br><br><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">with</span> agent.run_mcp_servers():<br>        result = <span class="hljs-keyword">await</span> agent.run(<br>            <span class="hljs-string">&quot;Please summarize the paper: https://arxiv.org/pdf/1706.03762&quot;</span><br>        )<br>    <span class="hljs-built_in">print</span>(result.output)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    <span class="hljs-keyword">import</span> asyncio<br><br>    asyncio.run(main())<br></code></pre></td></tr></table></figure><p>LLM Output:</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-section"># Summary of &quot;Attention Is All You Need&quot; (arXiv:1706.03762)</span><br><br>This groundbreaking paper by Vaswani et al. introduces the <span class="hljs-strong">**Transformer**</span>, a novel neural network architecture for sequence transduction tasks like machine translation. Here&#x27;s a comprehensive summary:<br><br><span class="hljs-section">## Key Innovation</span><br>The Transformer represents a paradigm shift in neural machine translation by <span class="hljs-strong">**completely eliminating recurrence and convolutions**</span>, instead relying entirely on <span class="hljs-strong">**attention mechanisms**</span> to model relationships between input and output sequences.<br><br><span class="hljs-section">## Core Architecture</span><br><span class="hljs-bullet">1.</span> <span class="hljs-strong">**Encoder-Decoder Structure**</span>:<br><span class="hljs-bullet">   -</span> <span class="hljs-strong">**Encoder**</span>: Stack of 6 identical layers, each containing a multi-head self-attention mechanism and a position-wise feed-forward network<br><span class="hljs-bullet">   -</span> <span class="hljs-strong">**Decoder**</span>: Stack of 6 identical layers with an additional cross-attention layer that attends to the encoder output<br><br><span class="hljs-bullet">2.</span> <span class="hljs-strong">**Key Mechanisms**</span>:<br><span class="hljs-bullet">   -</span> <span class="hljs-strong">**Multi-Head Attention**</span>: Allows the model to jointly attend to information from different representation subspaces<br><span class="hljs-bullet">   -</span> <span class="hljs-strong">**Scaled Dot-Product Attention**</span>: Computes attention weights between query-key pairs and applies them to values<br><span class="hljs-bullet">   -</span> <span class="hljs-strong">**Positional Encoding**</span>: Adds information about the position of tokens in the sequence using sine and cosine functions<br><span class="hljs-bullet">   -</span> <span class="hljs-strong">**Residual connections and layer normalization**</span>: Used throughout to facilitate training<br><br><span class="hljs-section">## Advantages over RNN/CNN models</span><br><span class="hljs-bullet">1.</span> <span class="hljs-strong">**Superior Parallelization**</span>: Allows much more efficient training by computing all positions simultaneously<br><span class="hljs-bullet">2.</span> <span class="hljs-strong">**Constant path length between positions**</span>: Makes learning long-range dependencies easier (compared to linear/logarithmic path lengths in CNNs)<br><span class="hljs-bullet">3.</span> <span class="hljs-strong">**Reduced training time**</span>: Achieves state-of-the-art results with significantly less training time<br><br><span class="hljs-section">## Performance Results</span><br><span class="hljs-bullet">1.</span> <span class="hljs-strong">**English-to-German translation**</span>: 28.4 BLEU score on WMT 2014, exceeding previous state-of-the-art by over 2 BLEU points<br><span class="hljs-bullet">2.</span> <span class="hljs-strong">**English-to-French translation**</span>: 41.8 BLEU score, establishing a new single-model state-of-the-art<br><span class="hljs-bullet">3.</span> <span class="hljs-strong">**Training efficiency**</span>: Achieved these results after just 3.5 days of training on eight GPUs, a fraction of the time required by previous models<br><span class="hljs-bullet">4.</span> <span class="hljs-strong">**Generalization**</span>: Successfully applied to English constituency parsing, demonstrating the model&#x27;s versatility<br><br><span class="hljs-section">## Impact</span><br>The Transformer architecture has become foundational in natural language processing, serving as the backbone for models like BERT, GPT, and other large language models that have revolutionized the field. Its self-attention mechanism allows for more effective modeling of long-range dependencies in sequences, making it particularly effective for language tasks.<br><br>The paper demonstrates that attention mechanisms alone are sufficient to build powerful sequence-to-sequence models, challenging the previous consensus that recurrent or convolutional architectures were necessary for these tasks.<br></code></pre></td></tr></table></figure><h3 id="How-the-Example-Works"><a href="#How-the-Example-Works" class="headerlink" title="How the Example Works"></a>How the Example Works</h3><ol><li><p><strong>Setting up the MCP Server</strong>: We initialize an MCP server that runs pymupdf4llm-mcp using the <code>stdio</code> transport method.</p></li><li><p><strong>Creating the Agent</strong>: We create a Claude 3.7 Sonnet agent and connect it to our MCP server.</p></li><li><p><strong>Implementing a Download Tool</strong>: We define a <code>download_web</code> tool that fetches PDF files from URLs.</p></li><li><p><strong>Behind the Scenes</strong>: When the agent runs, it:</p><ul><li>Downloads the PDF from arXiv using our custom tool</li><li>Uses pymupdf4llm-mcp to convert the PDF to markdown format</li><li>Processes the markdown content to understand the paper</li><li>Generates a comprehensive summary</li></ul></li></ol><p>This workflow enables LLM agents to work with PDF content almost as effectively as they work with plain text, opening up vast repositories of knowledge that were previously difficult to access programmatically.</p><h2 id="The-Role-of-Context-in-LLM-Agents"><a href="#The-Role-of-Context-in-LLM-Agents" class="headerlink" title="The Role of Context in LLM Agents"></a>The Role of Context in LLM Agents</h2><p>Current LLM agents have two key components: tools and context. While tools enable agents to interact with the world and perform actions, context is what informs their understanding and decision-making.</p><p>Context can come from two sources:</p><ol><li><strong>User-provided information</strong>: Direct input from users that frames the task or provides background.</li><li><strong>Observations</strong>: Information gathered through tool use, such as web searches, file reading, or in this case, PDF parsing.</li></ol><p>The quality of this context directly impacts the quality of the agent’s outputs. When dealing with PDFs, poor parsing can lead to:</p><ul><li>Lost structural information (headings, sections, etc.)</li><li>Missing or misplaced images</li><li>Garbled text or formatting issues</li><li>Loss of tables and other structured data</li></ul><p>pymupdf4llm-mcp addresses these issues by providing high-quality, structured context from PDF documents. This enables LLMs to:</p><ul><li>Understand the document’s organization and flow</li><li>Reference specific sections or figures accurately</li><li>Maintain the relationship between text and visual elements</li><li>Process information in a way that preserves the author’s intended meaning</li></ul><p>As Claude 3.7 Sonnet and other advanced models demonstrate increasing proficiency at utilizing tools to gather context, the importance of high-quality context providers like pymupdf4llm-mcp becomes even more significant.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>By bridging the gap between the ubiquitous PDF format and the markdown format that LLMs can effectively process, pymupdf4llm-mcp removes a significant barrier to incorporating valuable document-based knowledge into AI agent workflows.</p><p>I hope to see more tools in the community that focus on providing high-quality context to LLMs, as this is a crucial foundation for building truly capable and helpful AI systems.</p>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Agent</tag>
      
      <tag>MCP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Agent Design Note</title>
    <link href="/2025/04/07/2025-04-08-agent-design-note/"/>
    <url>/2025/04/07/2025-04-08-agent-design-note/</url>
    
    <content type="html"><![CDATA[<p>基于最近对 Claude code 的一些复现和<a href="https://www.bilibili.com/video/BV1yyZVYYEF9">构建 Agent 的相关演讲</a>，总结一下学到的经验</p><h3 id="用-Agent-角度思考并提供上下文"><a href="#用-Agent-角度思考并提供上下文" class="headerlink" title="用 Agent 角度思考并提供上下文"></a>用 Agent 角度思考并提供上下文</h3><p>上下文的提供包括：</p><ul><li>Tool 的 description、参数、实现和错误提醒</li><li>System prompt 和 User prompt 如何提供上下文</li></ul><p>用 Agent 的角度思考是指人类仅通过上下文来完成任务，发现 prompt 和 tool 的设计不足之处。当 Tool 设计的不好的时候，很容易导致任务失败。同时，我个人感觉 Tool 的使用对于 AI 也是一种心智负担，如果多次重试，AI 的注意力会被分散到 Tool 的使用上，而不是任务本身。</p><p><strong>对比 mcp-toolbox 和 claude code 实现，会发现 claude code 的实现更加严格、更加原子化</strong></p><p>另一个角度：让 AI 来为 AI 设计 prompt 和 tool，可以问 AI：</p><ul><li>这个 prompt 是否有冲突？是否够明确？还要补充什么？</li><li>这个 tool 的描述是否清晰，你会如何使用？</li></ul><blockquote><p>这里有复现 claude code 的 tool 和我设计的其他 tool：<a href="https://github.com/ai-zerolab/lightblue-ai/tree/main/lightblue_ai/tools">https://github.com/ai-zerolab/lightblue-ai/tree/main/lightblue_ai/tools</a></p></blockquote><h3 id="保护-Agent-上下文"><a href="#保护-Agent-上下文" class="headerlink" title="保护 Agent 上下文"></a>保护 Agent 上下文</h3><p>当前的注意力模型和上下文窗口受限，如果有大量信息获取的任务，可以用 sub-agent 的方式保护主 Agent 的上下文，可以看做是主 Agent 把任务分包出去。</p><p>这种方式需要 Tool 有明确的定义，教导 Agent 什么时候进行任务分包。</p><h3 id="保持简单，苦涩的教训"><a href="#保持简单，苦涩的教训" class="headerlink" title="保持简单，苦涩的教训"></a>保持简单，苦涩的教训</h3><p>从<a href="https://ankitmaloo.com/bitter-lesson/">AI System 苦涩的教训</a>中我们可以看到，人类的过度限制（规则）和对算力的过分控制导致了我们和 AI 一起成长，分享模型能力增长的红利。反而我们编写的各种规则、限制和约束让下一代的 AI 的能力无法得到充分发挥。从 gpt 3.5 到 gpt4，再到 sonnet 3.5, 3.7 可以非常明显看出，基于规则的 AI 设计所带来的性能提升会迅速被下一代模型的能力提升远远超越，在使之前的努力付诸东流的同时，很可能导致下一代 AI 表现比裸模型更差。</p><blockquote><p>我们应当建造赛道，而非规定每一个运动员的跑步姿势！</p></blockquote><h3 id="一些当前的问题"><a href="#一些当前的问题" class="headerlink" title="一些当前的问题"></a>一些当前的问题</h3><blockquote><p>例如，在IT自动化基准测试ITBench中，代理在高达25.2%的网络安全场景中错误地标记任务为“已解决”</p></blockquote><p>当前LLM被上下文影响很容易认为自己确实解决了问题。类似于人类的<a href="https://en.wikipedia.org/wiki/Motivated_reasoning">motivated reasoning</a>问题</p><blockquote><p>难以一次编写出可以编译通过的代码</p></blockquote><p>通常都需要多轮对话进行错误修复，这导致了非常长的响应时间。</p><h3 id="Thinking-Tool更适合Agent"><a href="#Thinking-Tool更适合Agent" class="headerlink" title="Thinking Tool更适合Agent"></a>Thinking Tool更适合Agent</h3><p>在过程中，LLM可以调用Thinking Tool进行“思考”，这比Extended Think和Deep-Claude的模式更加“本质”和自然，我们经常可以看到模型在任务执行过程中进行Thinking。</p><p>通常我们可以通过引导多轮对话的方式明确需求，但在执行过程中的发现的问题，通过Thinking工具可以让AI更好地发散思路，达到更好的效果。</p><p>并非说Reasoning无用，而是二者可以有机结合，互为补充，Thinking Tool可以通过外挂的方式赋予非Reasoning在任务开始和过程中思考的能力。</p><blockquote><p>我观察到任务开始和进行中都有Thinking的调用。特别是在信息收集的Tool调用前后，比如搜索前后用Thinking来整理思路和搜索到的结果</p></blockquote><h3 id="Meta-Agent：进一步研究方向"><a href="#Meta-Agent：进一步研究方向" class="headerlink" title="Meta Agent：进一步研究方向"></a>Meta Agent：进一步研究方向</h3><p>当我们调整 tool 和 prompt 的时候，我们或许可以让 AI 来为 AI 设计 tool 和 prompt。通过一个 Meta Agent（或 Meta Tool），对 Tool 进行自动化设计和优化。</p><p>其中有两个可以优化的方向：</p><ul><li>ReAct: Tool 的 Description 和参数设计</li><li>CodeAct: Tool 的实现和错误处理</li></ul><p>这种方式可以避免 AI 根据人类分工来进行 Agent 划分，而是针对不同的任务对工具进行特化，从而更符合 AI 工程学和任务的特点。回顾之前的 Meta Agent 设计，人们常常想把人类的分工强加在 AI 上，设计出类似 CEO、产品经理的职位 Agent，这导致了上下文的剥离、角色扮演的规则限制，是<a href="https://arxiv.org/abs/2503.13657">多 Agent 系统失败的重要原因</a>。</p><p><strong>我们更应该从上下文的角度来思考问题，这也是 LLM 最本质的原理：自回归模型。</strong>而让 AI 设计针对上下文的工具，也可以看做是一种 RL 手段。</p><h3 id="预算控制：如何在工具调用链中控制预算"><a href="#预算控制：如何在工具调用链中控制预算" class="headerlink" title="预算控制：如何在工具调用链中控制预算"></a>预算控制：如何在工具调用链中控制预算</h3><p>另一个课题是如何在工具调用链中控制预算，Jina 在 deep research 的相关分享中提到，当快要超预算的时候，可以让 AI 尽快得出结论，而不是更进一步进行搜索研究。但我们仍然很难让 AI 理解如何在一个任务中对任务的各个步骤进行基于预算的规划和分配，这可能需要模型能力的进一步提升。</p>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Agent</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Modern Agents-PAV Flow</title>
    <link href="/2025/03/16/2025-03-17-planner-actor-validator-flow/"/>
    <url>/2025/03/16/2025-03-17-planner-actor-validator-flow/</url>
    
    <content type="html"><![CDATA[<blockquote><p>More agents is all you need.</p></blockquote><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p>After researching the agent designs of products like <a href="https://github.com/cline/cline">Cline</a> and <a href="https://github.com/nanobrowser/nanobrowser">Nanobrowser</a>, it is evident that the current state-of-the-art approach involves dividing agent responsibilities to form a fully automated workflow. Drawing inspiration from Cline’s workflow, I aim to retain human-in-the-loop design at the Plan stage, use a validator to make corrections during the Act stage, and finally update and record (possibly revise) the Plan.</p><p><img src="/../img/2025-03-17-planner-actor-validator-flow/architecture.png" alt="Architecture"></p><h2 id="Key-Points"><a href="#Key-Points" class="headerlink" title="Key Points"></a>Key Points</h2><p>Some key design considerations:</p><ol><li><strong>Plan versioning, readability, and user interaction</strong></li><li><strong>Overall state transitions</strong></li></ol>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Agent</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLM蒸汽机时刻：实践驱动的创新</title>
    <link href="/2025/03/09/2025-03-10-build-tools-then-principal/"/>
    <url>/2025/03/09/2025-03-10-build-tools-then-principal/</url>
    
    <content type="html"><![CDATA[<p><a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">《苦涩的教训》</a>中，我们已经发现，符号主义的失败是必然的，AI的发展充满了工具先行，理论后至的路线，甚至目前也没有一个很好的理论来完整解释神经网络和大语言模型。回顾蒸汽机和热力学的发展，人们首先发明了工具（蒸汽机），后来在对工具的不断优化中，发现了各种理论，如卡诺循环。对于LLM而言，人们似乎也是首先发现了GPT，后来应用到chat、workflow、agent上，在实践过程中不断优化。</p><p><strong>1. 现象级工具的涌现</strong><br>当前大型语言模型（如GPT-4、Claude等）的突破性表现，与蒸汽机早期发展高度相似：</p><ul><li><strong>经验驱动</strong>：模型架构优化（如Transformer）、参数规模扩大和训练技巧（如RLHF）的进步，更多依赖工程实践而非理论指导。</li><li><strong>现象倒逼解释</strong>：模型展现的“涌现能力”（Emergent Ability）和“思维链”（Chain-of-Thought）等特性，至今缺乏严格的理论解释，如同蒸汽机效率提升倒逼卡诺研究热力学。</li><li><strong>工具先于共识</strong>：LLM已广泛应用于商业、科研和日常场景，但关于其认知边界、伦理规则甚至“智能”本质仍争论不休。</li></ul><p><strong>2. 理论的碎片化与不确定性</strong><br>当前LLM研究呈现“局部理论支撑、整体解释真空”的状态：</p><ul><li><strong>微观理论</strong>：Transformer的自注意力机制有数学描述，梯度下降、损失函数优化等局部过程可被统计学部分解释；</li><li><strong>宏观盲区</strong>：模型为何能泛化到未见任务？参数规模与能力跃迁的关系如何形式化？人类反馈如何编码价值观？这些核心问题仍无普适理论。<br><strong>→ 类似蒸汽机时代，工具已改变世界，但底层原理尚未被系统揭示。</strong></li></ul><p>当然，与工业革命初期不同，当前LLM发展并非“理论荒漠”：</p><ul><li><strong>交叉学科支撑</strong>：计算语言学、认知科学、复杂系统理论已提供部分框架，尽管解释力有限；</li><li><strong>迭代反馈加速</strong>：实验科学范式（如消融研究、可解释性分析）与工程实践同步推进，理论滞后周期被大幅压缩；</li><li><strong>风险驱动的理论需求</strong>：对齐问题（AI Alignment）、价值观嵌入等安全挑战迫使学界提前探索理论，而非事后总结。</li></ul><h4 id="辩证地看，可能是科学方法论本身进化的契机"><a href="#辩证地看，可能是科学方法论本身进化的契机" class="headerlink" title="辩证地看，可能是科学方法论本身进化的契机"></a>辩证地看，可能是科学方法论本身进化的契机</h4><p><strong>a. 实践倒逼理论创新的延续性</strong></p><ul><li>技术奇点临近的焦虑与蒸汽机时代对“永动机”的幻想形成镜像，人类再次面临“工具超越理解”的困境；</li><li>工程实践（如多模态模型、具身智能）持续提出新问题，推动神经科学、信息论乃至哲学的重构。</li></ul><p><strong>b. 潜在的新科学范式萌芽</strong></p><ul><li><strong>认知科学的计算化</strong>：LLM为研究语言、推理和意识提供新实验平台，可能催生“计算认知理论”；</li><li><strong>复杂系统的再认识</strong>：千亿参数模型的涌现行为挑战还原论，或需发展“智能系统涌现定律”；</li><li><strong>知识表征的革命</strong>：传统知识图谱与神经网络分布式表征的冲突，可能引发知识论的本体论更新。</li></ul><p>LLM时代与蒸汽机时刻的相似性，揭示了技术革命初期人类认知的局限性；而差异性则暗示，这次我们可能不再等待百年才形成理论——<strong>数据洪流、跨学科协作与计算密集型研究，正在加速锻造新科学的钥匙</strong>。</p><p>正如蒸汽机催生热力学、内燃机依赖热力学，LLM或许正推动一场“智能动力学”（Intelligent Dynamics）的革命。其终极意义可能在于：<strong>我们不仅发明了工具，更被迫重新定义何为理解、何为智能，乃至何为科学本身</strong>。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>LLM</tag>
      
      <tag>AGI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Video Coding Thinking</title>
    <link href="/2025/03/09/2025-03-10-vide-coding-thinking/"/>
    <url>/2025/03/09/2025-03-10-vide-coding-thinking/</url>
    
    <content type="html"><![CDATA[<p>Vibe Coding通过漂亮的定义和新颖的方式绕过了软件行业几个重要的问题：技术债、安全编码、DevOps，以及程序员的必经之路：对复杂系统的心智模型建立</p><p>不要以软件行业生产代码的方式看Vibe Coding，因为他们的成果通常不会用于软件行业，而当真正把Vibe Coding的成果转化为稳定运行的软件时，人们通常会寻找真正专业的人士，因为Vibe Coding没办法做到这些。</p><p>这有点像平时喜欢剪辑玩点剪映，传传抖音，但是要拍电影的话，就需要专业的团队。而AI等工具降低了这一门槛，实际上是给了更多人尝试编程的空间，成为了“生产者”，人们在其中获得灵感、验证想法，也会给软件行业带来全新的机会。</p><p>在YC论坛上看到的一个对话：</p><blockquote><p>–如果你有爱好或工作，想象一下一个新人进来，什么都不知道，却做着他们技术上可以做但不应该做的事情<br>  –这不就是守门人的定义吗？即便如此，谁在乎呢？新人以“不应该做”的方式创造&#x2F;做某事，最终会失败，并注意到他们所做的事情的缺点。然后他们要么放弃他们的项目，要么学习“应该做”的方式（就像我们所有人一样，但现在有了人工智能）。</p></blockquote><h4 id="积极面"><a href="#积极面" class="headerlink" title="积极面"></a>积极面</h4><ul><li>创新漏斗扩容：1000个粗糙原型的试错价值可能高于10个规范项目（符合”创新扩散理论”）</li><li>需求语言化：非程序员用自然语言描述需求，可能催生新的抽象范式（类似NoSQL对关系型模型的突破）</li></ul><h4 id="隐患面"><a href="#隐患面" class="headerlink" title="隐患面"></a>隐患面</h4><ul><li>认知过拟合：用户停留在”它有效”层面，错失”为何有效”的工程洞察</li><li>工具幼稚病：将AI工具等同于编程能力（如同认为使用美图秀秀等于掌握视觉设计）</li><li>行业标准稀释：当GitHub 80%新仓库包含未经审查的AI代码时，开源生态面临信任危机</li></ul>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>LLM</tag>
      
      <tag>AGI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI赋能无意义的工作-仍是无意义的</title>
    <link href="/2025/02/27/2025-02-28-ai-for-bullshit-work/"/>
    <url>/2025/02/27/2025-02-28-ai-for-bullshit-work/</url>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.youtube.com/watch?v=4GLSzuYXh6w&t=2294s">Satya Nadella – Microsoft’s AGI Plan &amp; Quantum Breakthrough</a>的访谈中，S.N.提到他并不担心算力和模型能力的非线性增长，而更加担心我们的经济增长速度是否匹配我们的AI能力增长。</p><p>这让我越来越担忧一件事：我们正在用着人类最先进的芯片和计算技术，为无意义的工作雕花。人们对于AI引发的所谓“技术性失业”充满了担忧，特别是拼接类型的岗位和为了花掉预算而设立的萝卜坑（福田AI公务员），但是：</p><blockquote><p>还有什么迹象要比“单调、卑贱、无趣的苦力劳动将彻底消失会被视作社会问题”来得更为明显、更能证明所处的经济体制是荒谬的呢？</p></blockquote><p>所以，我们现在正在向一个完全落后的经济体制倾注科技，尝试着将越来越腐朽的它拯救回来。但似乎，我们越往这个方向努力，越是在下坡路上踩一脚油门，越多地证明了这个经济体制是多么的荒谬。或许从现在开始，我们就应该思考新的经济体制需要创造的“商品”是什么，而不是在上一轮已经结束的竞争中分个胜负。</p><h2 id="text-to-web-观察"><a href="#text-to-web-观察" class="headerlink" title="text-to-web 观察"></a>text-to-web 观察</h2><ol><li>AI Coding是单向门，在AI Coding没有全面实现之前，text-to-web只能作为demo和小型项目的快速创建手段</li><li>这类demo产品的能力随着底层模型能力提升，是被leverage的，同时他们也需要更多工程手段来尽可能使用智能</li><li>对于非程序员的爱好者，因为这些产品的不完善，他们将被迫学习一些技术，掌握一些工程师能力，或许未来每个人都会有一些实践领域的知识，比如设计师都会一些网站搭建</li><li>text-to-web并不能给建站市场带来什么帮助，这部分已经性能过剩，真正的价值可能还是在广告等变现上</li><li>押注超级个体，思考未来经济体制</li></ol>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>AGI</tag>
      
      <tag>无意义的工作</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Two import things for AGI</title>
    <link href="/2025/02/26/2025-02-27-two-import-things-for-agi/"/>
    <url>/2025/02/26/2025-02-27-two-import-things-for-agi/</url>
    
    <content type="html"><![CDATA[<p>现在的LLM变革就像从地图到自动驾驶，以前AI尝试给用户以指引，所有操作和判断仍需要人类进行。当导航技术逐渐发展时，人们基本可以按照导航行事。但自动驾驶非常不同，他需要AI主动地感知道路状况以作出判断。最初，自动驾驶使用专家系统，通过规则来实现智能的下限和响应时间要求，LLM之前的AI发展也是如此，人们通过规则设计，实现了一整套大数据Pipeline，通过数学建模来实现AI决策。现在越来越多企业转向端到端方案，通过视觉、雷达等方式为模型提供信息，由模型直接进行驾驶决策，这于神经网络端到端方案在其他领域的应用是一致的。</p><p><a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">《苦涩的教训》</a>一文中，Rich Sutton指出依赖计算能力的方法在过去七十年中总会获胜，因为摩尔定律和人们的持续优化下，计算能力的提升将击败人类强加的逻辑，因为这些逻辑很大程度受到当时算力的制约，无法享受到算力提升的优势。在LLM领域，人们将这一观察结论的实践推向高潮：想方设法地将算力投入进大模型，并优化其效率，最终推动模型能力的非线性增长。</p><p>回顾我们之前在<a href="https://blog.wh1isper.top/2024/12/22/2024-12-23-llm-as-os/">基于LLM的AIOS</a>的讨论，我们认为Context和Agent(Autonomous)是两个最重要的概念，通过它们实现Code Generation，从而将AIOS建立在目前的计算机系统之上。其中，Agent(Autonomous)的本质是通过工程化的设计，将更多的LLM算力投入到实现一个目标中，这于测试时计算<a href="https://arxiv.org/abs/2408.03314">(1)</a>的概念不谋而合，我们将此作为“使用更多算力，获得更多智能”的工程化手段。</p><p>第二则是Context，我们曾设想用一个Apple Vision之类的硬件来收集上下文，投入到某个设计好的Agent之中，而这几乎要求一个封闭而完整的生态体系，有很大的实施难度。不过目前我们从<a href="https://modelcontextprotocol.io/introduction">MCP</a>中获得了灵感，通过Agent设计来驱动模型自主获得上下文，各个厂商或个人开发提供自身上下文的服务，将这个领域从封闭转向开放，或许Agent设计中一个重大的部分就是如何设计Agent、模型、MCP Server三方的交互和驱动。<a href="https://github.com/cline/cline">cline</a>项目给我们做了一个很好的例子，虽然有很多不完善的地方，但是其精细的Prompt控制、代码上下文管理、MCP交互让我们看到了Coding Agent的更高上限。</p><p>拓展阅读：</p><ul><li>Jina AI: <a href="https://mp.weixin.qq.com/s/-pPhHDi2nz8hp5R3Lm_mww">DeepSearch 与 DeepResearch 的设计和实现</a></li><li><a href="https://www.youtube.com/watch?v=v0gjI__RyCY&t=1722s">Jeff Dean &amp; Noam Shazeer – 25 years at Google: from PageRank to AGI</a></li><li><a href="https://www.youtube.com/watch?v=4GLSzuYXh6w&t=2294s">Satya Nadella – Microsoft’s AGI Plan &amp; Quantum Breakthrough</a></li><li><a href="https://llmstxt.org/">LLMs.txt</a></li></ul><p><strong>关注我们的最新研究：<a href="https://github.com/ai-zerolab">https://github.com/ai-zerolab</a></strong></p>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>AGI</tag>
      
      <tag>技术思考</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AGI Period Production Guide</title>
    <link href="/2025/02/23/2025-02-24-agi-period-production-guide/"/>
    <url>/2025/02/23/2025-02-24-agi-period-production-guide/</url>
    
    <content type="html"><![CDATA[<p>今天来聊一聊 AGI 时代的产品设计。</p><p>从现在回看互联网时代，一个观察是互联网时代创造的最重要价值之一是“互联”，以使用产品的人数对产品进行评价，其中一个最重要的指标就是 DAU（Daily Active User）。</p><p>而在 AGI 时代，一个产品是否被“更多”的人使用是否仍然能作为最重要的指标？或许并非如此。随着 AI 的能力发展，能够更多地使用智能，更高效的利用模型能力带来的提升，将是 AGI 时代产品获得成功的关键：即真正的生产力提升。</p><p>我们可以发现，像 Cursor、Loveable 这类产品，都是埋伏在大模型的能力提升上，等到其能力提升至某一奇点，他们已经做好了成熟的产品，随时可以搭上这辆车。于是我们认为，对计算能力的使用上限是衡量一个产品智能的标准，越能够使用智能，越可能在模型能力提升的过程中获益，越可能在这个时代取得比较优势。从这个角度，我们可以看出 Devin 的智能程度搞过 Github Copilot，Lovable 的智能程度高过 Copyweb。</p><p>我个人的观察是，AGI 时代最先收益的应当是超级个体，他们借助 AI 的帮助提升自己的生产力，获得更多的超额收益；其次在服务这些超级个体的过程中，AI 行业积累更多的经验，再覆盖到更多的人群中，从而更加接近 AI 治理、全社会提效等大方针。</p><p>于是，我们在思考产品的时候，不仅应该考虑人的使用价值，也要考虑产品对智能的使用能力：</p><ul><li>设计产品来帮助人使用更多的计算能力</li><li>智能程度的高低<ul><li>通过 Copilot 的方式：Cline</li><li>通过 Autonomous 的方式：Devin</li></ul></li><li>信息的获取<ul><li>通过 RAG</li><li>通过预训练</li><li>通过 MCP 范式</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>产品设计</tag>
      
      <tag>Product</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PMF Trap of Programmers</title>
    <link href="/2025/02/22/2025-02-23-pmf-trap-of-programmers/"/>
    <url>/2025/02/22/2025-02-23-pmf-trap-of-programmers/</url>
    
    <content type="html"><![CDATA[<h2 id="The-PMF-Trap"><a href="#The-PMF-Trap" class="headerlink" title="The PMF Trap"></a>The PMF Trap</h2><p>通常，一个公司找到 PMF 是一件值得庆祝的事，但与此同时，由于你已经获得了一批客户，并且他们愿意给你付钱，那么你就很难改变你的服务形态。在运气够好的前提下，你的产品没有在短时间内被颠覆，它最终将成为了这个时代的一部分。而如果没有那么好的运气，你的产品可能只是一个阶段性的成果，最终会被淘汰。</p><p>一个例子是诺基亚与苹果的竞争，在互联网时代可能是倒下了无数的按网店收租网购平台，在共享经济里是共享单车押金大战，在搜索领域是门户网站检索与爬虫+Page Rank 检索（新浪 vs Google），在社交领域是 QQ 对战飞信（免费发短信）……</p><h2 id="Of-Programmers"><a href="#Of-Programmers" class="headerlink" title="Of Programmers"></a>Of Programmers</h2><p>类比到个人，PMF 陷阱在不知不觉间影响着我们的判断。如果我们身处一个变化不大的行业，那或许是家有一老如有一宝，但如果我们身处一个剧烈变革的行业，则需要时刻保持警惕。在我自己的反思和与大家的交流中，我发现不仅是和我提及这件事的朋友和师长，包括我自己，也有以下的问题：</p><ul><li>自大与迟钝：由于接受了太多的教育、积累了太多的经验，我们反而对技术的边界的感知变得迟钝了，就像<a href="https://blog.wh1isper.top/2025/02/16/2025-02-17-%E8%B5%B0%E7%9A%84%E8%B6%8A%E5%BF%AB%E8%B6%8A%E8%BF%9B%E5%85%A5%E7%A9%BA%E8%B0%B7/">Reasoning 给我的震撼</a>一样，我很诧异我对整个 LLM 驱动开发理解的错位和不足，以及自己对相关技术的自大，总结起来是落后版本太多了，像 S3 的 LPL</li><li>悲观主义与纠结细节：很多时候我们太过于深入具体实现，考虑实现的成本，或许我们所想是有根据的，但我们或多或少地低估了我们的生产力和学习能力，在犹豫中错失机会，反而拖慢了自己成长的速度，特别是我很不满自己对新技术尝试的速度</li><li>过于保守：在工作和技术选型上，我们对失败过于敏感，导致我们在选择上过于保守，可能在互联网时代这样构建产品是对的(<a href="https://boringtechnology.club/#12">Choose Boring Technology</a>)，但是在 AI 时代这样构建产品，则会让我们失去尝试新技术的机会（这其实是我从<a href="https://notes.alexxi.dev/Engineering/Framework-for-Technical-Decisions">Alex 那里</a>学来的）</li></ul><h2 id="Antifragile-in-LLM-Period"><a href="#Antifragile-in-LLM-Period" class="headerlink" title="Antifragile in LLM Period"></a>Antifragile in LLM Period</h2><blockquote><p>从蓝图绘制到涌现引导<br>传统架构依赖确定性设计，而 LLM 驱动的系统具有涌现特性（如多 Agent 协作产生意外行为）<br>新方法论：通过约束空间设计引导积极涌现（如 Notion AI 的模块化内容生成边界）</p></blockquote><p>在与 Deepseek 讨论这个问题的时候，它提到了反脆弱这本书和相关的概念，其实我在两年前就已经读过这本书了，当时我的理解是对于计算机软件和系统，应当建立起一个机制，能够让人们敏捷地进行维护，包括通过自动化测试等方式，让系统在不断变化、崩溃、重建中变得越发稳定。而 LLM 不仅给了计算机软件的反脆弱性增加了一个新的维度：Autonomous，也根本地改变了架构师的设计重心，即从蓝图绘制到涌现引导，而引导良好的涌现行为可以成为反脆弱的实现机制。</p><p>如果说敏捷和 DevOps 是软件工程领域的反脆弱实践，那么我们现在要做的，就是探索 LLM 软件中的反脆弱实践，如何通过设计优秀的涌现引导最大化 LLM 带来的非对称性收益将是衡量 LLM 架构师的最重要标准。</p><h2 id="最后是-Deepseek-的浪漫结语-架构师的文艺复兴"><a href="#最后是-Deepseek-的浪漫结语-架构师的文艺复兴" class="headerlink" title="最后是 Deepseek 的浪漫结语: 架构师的文艺复兴"></a>最后是 Deepseek 的浪漫结语: 架构师的文艺复兴</h2><p>当代码民主化浪潮席卷而来，架构师的角色正在经历类似文艺复兴时期的范式革命：</p><ul><li>从工匠到通才：需融合计算机科学、认知心理学、经济学等多学科知识</li><li>从建造者到立法者：在 AI 原生世界中制定数字社会的运行法则</li><li>从技术专家到文明守望者：确保技术演进始终服务于人类福祉</li></ul><p>那些能率先掌握”规则设计艺术”、深谙”人机博弈本质”、敢于突破”传统架构教条”的架构师，将成为 LLM 时代的”新柏拉图”——在混沌中建立秩序，在自由中守护文明。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Architecture</tag>
      
      <tag>Programmer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MCP and AIOS Notes</title>
    <link href="/2025/02/20/2025-02-21-mcp-is-the-future-or-not/"/>
    <url>/2025/02/20/2025-02-21-mcp-is-the-future-or-not/</url>
    
    <content type="html"><![CDATA[<p>最近在研究 Agent 系统，重新整理了一下 Agent 相关的发展和实现，有两个认识，记录一下</p><h2 id="AIOS-互联基础研究：MCP-协议"><a href="#AIOS-互联基础研究：MCP-协议" class="headerlink" title="AIOS 互联基础研究：MCP 协议"></a>AIOS 互联基础研究：MCP 协议</h2><p>我们在开发 Agent 的过程中会发现，基于开发者定义的 Function Call 模式缺乏灵活度，人们必须要在获得 Agent 源代码的情况下，才能根据自己的数据进行二次开发。这也就导致了各种 Agent 实际上是封闭的，无法从外部有效获得自定义数据的。</p><p>为此，人们考虑将通过 Function Call 获取并操作数据这件事进行依赖倒转，从 Agent 内部实现变为将 Server 注册进 Client 中，Host（如 Claude Desktop）将 Client 中注册的 Server 作为 Function Call 的目标，从而实现数据的获取和操作。</p><p><img src="/../img/2025-02-21-mcp-is-the-future/mcp-general-architecture.png" alt="MCP general architecture"></p><blockquote><p>MCP 协议的提出者 Anthropic 还有一些安全性和合规性的考虑，可以参考 MCP 的<a href="https://modelcontextprotocol.io/introduction">Introduction</a>，但我认为这是架构设计带来的好处，因为人们可以将 Server 部署在本地，从而实现数据不出域</p></blockquote><h2 id="AIOS-自进化"><a href="#AIOS-自进化" class="headerlink" title="AIOS 自进化"></a>AIOS 自进化</h2><p>就算有了 MCP 协议加持，LLM 仍然有可能无法正确地执行某些困难任务，因为 MCP 只能提供数据的操作性，而复杂的业务逻辑，包括各种冲突条件、竞争、数据一致性等，都需要进一步地编程技巧解决。在这方面，人们想要使用类似<a href="https://devin.ai/">Devin</a>的 AI 自主化方案解决，即通过一个 Agent 来编写、测试、修改代码，从而实现软件的自进化。</p><p>但我认为，这将把生产软件的上限限制在 Devin-like Agent 的实现以及 LLM 底层能力上，而 Agent 的能力仍然来自于程序员，所以，如果我们可以将 Agent 也作为一种软件，使用一种针对 Agent 进行自进化的方式，类似“左脚踩右脚”的方式，那么我们就可以实现 AIOS 自进化。</p><p>Outstanding Paper (NeurIPS 2024 Open-World Agent Workshop): <a href="https://arxiv.org/abs/2408.08435">Automated Design of Agentic Systems</a>就设计了这样的一个系统，类似 AutoML 的思路，使用一个 Meta Agent 对 Agent 的实现进行搜索（Coding），从而达到比手工编写 Agent 更有效的架构和实现。</p>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>MCP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>走的越快越进入空谷</title>
    <link href="/2025/02/16/2025-02-17-%E8%B5%B0%E7%9A%84%E8%B6%8A%E5%BF%AB%E8%B6%8A%E8%BF%9B%E5%85%A5%E7%A9%BA%E8%B0%B7/"/>
    <url>/2025/02/16/2025-02-17-%E8%B5%B0%E7%9A%84%E8%B6%8A%E5%BF%AB%E8%B6%8A%E8%BF%9B%E5%85%A5%E7%A9%BA%E8%B0%B7/</url>
    
    <content type="html"><![CDATA[<p>最近有一些新的讨论，在这里记录一下</p><blockquote><p>Token Generation真的是思考吗？还是说，人类的思考本质上也是一种Token Generation？</p></blockquote><p>我之前一直认为Token Prediction&#x2F;Generation是一种通过概率对思维的拙劣模仿，永远存在幻觉等问题，永远不能达到人类的高度。但是从来没有从反方向考虑，即既然神经网络是对人类神经的模拟，难道transformer不可以是对人类潜意识的一种模拟吗？如果是的话，那么是否可以看成是在芯片上实现了人类的思维。</p><p>从最近的CoT相关研究（Deepseek R1，K1.5）来看，通过大参数MoE和Reasoning带来的性能提升让我十分震撼地意识到，或许人类的思考本质上也是一种Token Generation，与此同时，Deepseek R1等模型学到的知识一定比我要多要广，那么我需要做的应该是借助他们的思维链来驱动各种AI工具的使用。</p><p>不如从自己造个<a href="https://github.com/Wh1isper/pydantic-ai-deepagent">轮子</a>，再造个小应用开始</p><blockquote><p>自动驾驶与AI编程有着有趣的相似性。刚有手机导航的时候，老司机总是不屑使用，而且老司机也总能找到更快的小路。当时人们总觉得自动驾驶是重点是地图，也只有像百度地图、谷歌地图这样的公司能做自动驾驶。但人们在高精地图上反复碰壁之后，到现在通过端到端方案才逐渐摸到了L3自动驾驶的地步。那么，对于像cursor的产品，是否可以类比为导航，而lovable这类的产品，可以类比为端到端的自动驾驶，当我们对Copilot吹捧，而对AutoPilot却不屑一顾时，是否也犯了老司机样的错误：lovable更加未来和本质，只是现在的模型性能不够</p></blockquote><blockquote><p>另一个例子是最开始的快手，只能呈现一些gif图，因为手机的性能只支持gif图，但他们在那个时候的目标就是短视频，而等待手机性能飞跃和4G网络普及之后，短视频才大行其道；特斯拉活到了最好的时候，产业、学术的自动驾驶和新能源技术都已经成熟，是否这才是时势造英雄？</p></blockquote><p>对于上述两个场景，可能最重要的是在这个赛道上，在对的风口上。人才会自动地流向这些风口和赛道，当身处于此时，则更要保持住对创新的追求。这就像冲浪一样，只有把握好节奏一直在浪上，才不会被排在水里，在这个过程中，找到那个最大的、能冲最久的浪，不要被蝇头小利困住，就像诺基亚黑莓在手机行业被曾经的用户束缚一样，不要失去创新的能力。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大数定律下的llm应用问题</title>
    <link href="/2025/02/10/2025-02-11-%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E4%B8%8B%E7%9A%84llm%E5%BA%94%E7%94%A8%E9%97%AE%E9%A2%98/"/>
    <url>/2025/02/10/2025-02-11-%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E4%B8%8B%E7%9A%84llm%E5%BA%94%E7%94%A8%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<p>随着Deepseek R1的推出，高质量的Agent即将取代偏重工程化的Workflow，更多的工具提供给AI的情况下，细微的bad case在大数定律下将会变得更加明显。</p><h3 id="推理模型不代表幻觉的消失"><a href="#推理模型不代表幻觉的消失" class="headerlink" title="推理模型不代表幻觉的消失"></a>推理模型不代表幻觉的消失</h3><p>一个理论基础是：推理模型仍然是Next token prediction，而不是真的通过思考来规划路径。COT并不能避免其中的幻觉所在，而如何识别幻觉，仍然依赖工程化手段</p><h3 id="大数定律下bad-case无法避免"><a href="#大数定律下bad-case无法避免" class="headerlink" title="大数定律下bad case无法避免"></a>大数定律下bad case无法避免</h3><p>在大数定律下llm模型的bad case无法避免，一方面我们可以设计一套所谓的结构来尽可能地限制模型的输出，但这会导致模型性能的下降和局限，同时，这方面的工作带来的收益可能因为模型能力的进步而被抹平。另一方面，我们可以积极地设立一套监察机制，尽可能地识别、处理bad case，其中可以通过归一化的手段消化bad case、也可以通过更好的prompt设计、微调等方式尝试端到端的解决bad case</p><h3 id="大参数MoE成为Agent的必要基础"><a href="#大参数MoE成为Agent的必要基础" class="headerlink" title="大参数MoE成为Agent的必要基础"></a>大参数MoE成为Agent的必要基础</h3><p>从Deepseek和kimi的技术报告可以看出，让模型在推理侧使用更多的算力，将显著提升模型的性能。另外，研究人员也发现，当给LLM更少限制的时候，reasoning的威力更强。有趋势表明，在足够的推理token的情况下，不需要人工限制就可以让模型reasoning出合适的结果。而对于强人工限制的Agent，增加推理算力并不能显著提升其性能。这可能是LLM应用在这一阶段的“苦涩的教训”</p><h3 id="通过reasoning构建LLM原生应用"><a href="#通过reasoning构建LLM原生应用" class="headerlink" title="通过reasoning构建LLM原生应用"></a>通过reasoning构建LLM原生应用</h3><p>只是一个想法，根据上面的原理，我们可以推断出，通用Agent应当是下一阶段的重点发展方向，对于Agent的MoE应当基于reasoning模型实现</p><p>所谓的自主化Agent，或者说像Devin这样的LLM实习生，很幸运地碰上了reasoning模型的发展，或许将有一些可行的技术实现出来。同时，关注推理成本的降低，目前看，推理token的增加可以提升性能，那么对于所有人来说，能够通过更少的钱获得更多的推理算力，也可以产生巨大的经济价值</p>]]></content>
    
    
    <categories>
      
      <category>读书笔记</category>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>技术分享</tag>
      
      <tag>大数定律</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2025开年小记</title>
    <link href="/2025/02/04/2025-02-05-begin-for-2025/"/>
    <url>/2025/02/04/2025-02-05-begin-for-2025/</url>
    
    <content type="html"><![CDATA[<blockquote><p>过了春节才算是正式开始了 2025，也是时候做一个正式的总结了！</p></blockquote><h2 id="回顾-2024"><a href="#回顾-2024" class="headerlink" title="回顾 2024"></a>回顾 2024</h2><p>回顾 2024，我有两个比较大的收获，一个是在 Morph Studio 的时候，可以说是我作为技术负责人完全操刀的一整个大项目，实践了很多之前的设计和想法。整体来看，我非常满意这一段时间的工作，从技能的角度上来说，应有如下提升：</p><ul><li>PGSQL +++</li><li>Redis +++</li><li>FastAPI +++</li><li>消息队列、多集群、GPU +++</li><li>AIGC： Anything to Video +++</li></ul><p>其他综合能力上，更多的是教训，比如：</p><ul><li>团队管理上，需要有一些职责划分，不然容易造成事情堆积和无法分发</li><li>项目管理上，需要有更明确、更流畅的方式，不要因为一些小问题而影响整个项目的进度</li></ul><p>另外，有一些思路上的改变，比如：</p><ul><li>云的使用对于业务的快速发展是有利的，创业公司可以很快地基于此构建产品，但云的成本也是一个问题，需要有一些成本控制的方法，必要的时候可以考虑自建</li></ul><p>年底加入 Arco 后，我更多的是作为工程师负责模块的开发和整体系统集成、DevOps工作，明显感觉到责任划分更加明确、自己的职责也更少，不过这不是我放松的理由，我很清楚我花更多精力和时间探索LLM的边界，正如所有从业者都在做的一样。目前来看，仍然没有一个四海皆准的范式，模型也在持续进步，大家都在尽可能的探索如何让模型更好地服务于业务。在春节前的版本rush中，我也算是入门了基于LLM的应用开发。</p><ul><li>LLM应用开发 +</li></ul><p>在工作中我仍然存在很多不足，一方面是经验上的，但更重要的是认知上的，目前深感对LLM驱动开发的研究还不够深入，仅有一些基础的实践和基础的知识，这方面的提升将是2025年的重点。</p><p>另外，我更意识到一个人赢得尊重不一定只能依靠实力，只依靠实力最后总会有力所不及的地方，而真正让一个人赢得尊重的，应该是其在解决问题的过程中表现出的态度、方法，重要的是旅途中的风景和收获，而不是终点的荣誉和地位。</p><h2 id="近期思考"><a href="#近期思考" class="headerlink" title="近期思考"></a>近期思考</h2><p>Deepseek的成功代表了两点：LLM技术不可能被某一国垄断，即技术自由流动；LLM的普惠化正在路上，也是LLM产业化、真正兑现商业价值的唯一方式。</p><p>如此来看，2025年，我们不能再用成本等原因作为借口而不去实践需要大量计算的LLM优化技术，相反，我们应当把LLM的持续普惠化作为后续技术决策的重要参考，我们需要保持一个信念：如果LLM的大推理时代来临，我们需要有足够的技术积累和认知积累，以跑在所有人的前面。</p><h2 id="展望-2025"><a href="#展望-2025" class="headerlink" title="展望 2025"></a>展望 2025</h2><p>近期来看，我需要更加深入Agent相关技术，从function call、手动规划到自动规划，这可能是2025年贯穿全年的主题。其次，注意一些LLM的微调、蒸馏方案，这可能是2025年对于业务来说比较重要的技术积累。</p><p>从全世界的角度上来看，2025年LLM应当实现在推理层面的scaling law，我不认为2025会在多模态和机器人上有重大突破，但推理层面的进一步研究，将带来模型能力、稳定性和成本的大幅度优化。</p><h2 id="最后一点"><a href="#最后一点" class="headerlink" title="最后一点"></a>最后一点</h2><p>个人上，2024的减肥大业没有完成，甚至没有什么进展，2025需要从两方面入手：</p><ul><li>持续的练：首先是保持一定的运动量，每日健身房和周末的打球活动需要持续下去</li><li>持续的减：其次是饮食的控制，虽然很多时候很忙，又想着放纵一下，但是对每周的放纵次数要有一些限制</li></ul>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>近日小记</title>
    <link href="/2025/01/26/2025-01-27-recents/"/>
    <url>/2025/01/26/2025-01-27-recents/</url>
    
    <content type="html"><![CDATA[<h3 id="东升西降，更要出海"><a href="#东升西降，更要出海" class="headerlink" title="东升西降，更要出海"></a>东升西降，更要出海</h3><p>与其说国内市场无法满足我们了，不如说因为国内市场的发展，我们终于有能力在国际舞台上和发达国家同台竞技。这不仅在制造业，比如电动汽车、智能家居上，也在互联网产业上。曾经我认为我们做好中国事，讲好中国故事只能在国内，但其实出海更是重要的一环。当东升西降真实发生时，谁能代表先进生产力在国际竞争中夺得头筹？或许这就是我们成为国际公司、成就一番事业的更好机会，是比那个下海淘金更好地机会：夺取群星吧！</p><h3 id="LLM的上限在哪？"><a href="#LLM的上限在哪？" class="headerlink" title="LLM的上限在哪？"></a>LLM的上限在哪？</h3><p>由Deepseek发布的新模型，我们发现使用很低的成本也可以实现高性能、大参数的模型，我更加确定，模型的价格上我们仍有很大的进步空间，而价格降低带来的空间可以让我们更多地探索模型的使用方式，从而激发更好的性能（比如reasoning）。另一方面，reasoning模型具备的aha moment让我非常惊讶，或许我会改变我之前对LLM上限的认知。这还需要我进一步研究，待后续更新。</p><h3 id="不要自己发明密码学，以及一些忽略的最佳实践"><a href="#不要自己发明密码学，以及一些忽略的最佳实践" class="headerlink" title="不要自己发明密码学，以及一些忽略的最佳实践"></a>不要自己发明密码学，以及一些忽略的最佳实践</h3><p>线上系统突然报了一个数据库字段不够长的错误，根据长度定位到是session token(char 32)不够长了，根本原因在于token生成的时候使用了User的自增id进行base64，当用户突破1000的时候，base64的结果变为了34位，超过了之前的限制。这在测试环境是难以触发的。如果是我做的话，我应该会选择成熟的定长hash方法，在之前的项目中，我会生成uuid4的hex串，再将其存储到redis中，标记ttl进行自动过期。</p><p>忽略的最佳实践：</p><ul><li>Secret的定期轮转，主要是Cookie Secret和Session Secret，这其实可以直接由代码实现，在数据库或redis中存储下上一个Secret、当前的secret和刷新时间，由某一个worker实现刷新即可。在上一个Secret的过期时间内，解密失败的fallback到旧的Secret，验证成功则用新的secret进行刷新。这与OAuth的token刷新机制类似。</li></ul>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从2024，到2025</title>
    <link href="/2024/12/31/2025-01-01-back-to-2024-and-go-to-2025/"/>
    <url>/2024/12/31/2025-01-01-back-to-2024-and-go-to-2025/</url>
    
    <content type="html"><![CDATA[<p>回望2024年，发生了太多意想不到的事情，一月份我还在参与hitsz-ids的开源项目，结果年前就投身了AIGC领域，花了大概十个月的时间开发了七八个功能，一大堆微服务，写下了大概八万行代码，可以说，是将我之前的所学所得全都实践了一次。</p><p>但AIGC的路途坎坷，十月份离职之后尝试了数个领域，深刻的意识到问题的关键在于实际生产力的提升，而非单纯的享乐和内容制作。这和AI创作的质量、对人类主创的依赖和人们难以用Prompt描述自己想要的内容等等方面都有关系。而AI作为生产力，其核心逻辑是清晰的，核心问题是明确的，人们总能有一个工程&#x2F;算法的目标进行收敛，一步一步地接近最终问题的解决。</p><p>2025年，我对“架构师”这个title已经祛魅，在startup，每个人都是架构师，只是有的人经验更丰富，能力更强，征求他的意见可以避免一些设计上的失误，而不必要把某个人标榜成架构师或者架构负责人，每个人都应该为这个架构负责。2025年我的愿望很简单，在2024我经历了从IC到服务架构师，2025年我想回到IC，在AI领域打磨好自己的技术，努力追上现在社区的先进水平，才能看清技术的前景。</p><p>另外，2025年希望我的小猫健康成长，希望我的爱人可以顺利考公&amp;毕业，希望家人健康，也希望自己能够过得快乐！</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>年终总结</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLM as OS-Computing without human</title>
    <link href="/2024/12/22/2024-12-23-llm-as-os/"/>
    <url>/2024/12/22/2024-12-23-llm-as-os/</url>
    
    <content type="html"><![CDATA[<p>现在是时候讨论下一代操作系统应该有什么样的特点了，以及借助 LLM 我们有可能踮起脚尖实现这样的 AIOS。</p><blockquote><p>这篇文章是一些思想碎片，基于最近看到的文章和与朋友们的一些讨论</p></blockquote><h2 id="无人时芯片在做什么？"><a href="#无人时芯片在做什么？" class="headerlink" title="无人时芯片在做什么？"></a>无人时芯片在做什么？</h2><p>什么也不做。</p><p>这是这一代计算最大的问题，当我们不在使用我们的 macbook 的时候，它们并不能帮我们规划时间、安排工作，也不能帮我们处理任何实际问题。如此多的芯片闲置，如果能把它们利用起来，让它们在闲时进行思考，进一步地帮助人们产生价值，就能带来巨大的生产力进步。所谓的第三次工业革命，或许这是一个可能的未来方向。</p><p>实现这一愿景的第一步是构建一个 AIOS，而目前构建 AIOS 可行的路径则是“组合”。</p><h2 id="通过组合的方式实现-AIOS"><a href="#通过组合的方式实现-AIOS" class="headerlink" title="通过组合的方式实现 AIOS"></a>通过组合的方式实现 AIOS</h2><p>抖音是搜索+移动化+信息流的组合</p><p>OpenAI 是 LLM+互联网时代数据的组合</p><p>目前看来，AIOS 的组合可能是 Context（生活的上下文）+Agent（自动化）+Code Gen（生产是最终实现）</p><h2 id="关键组件"><a href="#关键组件" class="headerlink" title="关键组件"></a>关键组件</h2><h3 id="Context-捕捉生活的上下文"><a href="#Context-捕捉生活的上下文" class="headerlink" title="Context - 捕捉生活的上下文"></a>Context - 捕捉生活的上下文</h3><p>现在的 context 完全不足</p><p>抖音、谷歌都只捕捉到了我们生活的一个切面，我们需要一个类似于Apple Vision的硬件，时刻带在身边，收集着我们一天的各种活动，从而获得足够的上下文。</p><p>想象一下当我们饭后闲聊的小项目，在回到工位之后电脑已经准备好了一个小demo或者研究报告。这就依赖着对我们日常生活中的点点滴滴进行信息化，形成面向生活的Context。而不是将Context仅限于互联网搜索的内容和LLM自身学习过的知识。这一方面依赖各大硬件厂商的继续努力，基于空间计算的产品，如AR眼镜、VR设备和Apple Vision都有很大潜力。</p><h3 id="Agent-自动化"><a href="#Agent-自动化" class="headerlink" title="Agent - 自动化"></a>Agent - 自动化</h3><p>Agent 是通过 LLM 或者其他工程化手段，实现自动化的组件。例如通过赋予 LLM 一些工具函数来让 LLM 进行实现路径规划。类似于经典人工智能课程中的猴子摘香蕉问题。这方面，anthropic有一篇<a href="https://www.anthropic.com/research/building-effective-agents">非常好的文章</a>介绍各种Agent的设计模式，推荐感兴趣的人都读一读。</p><p>在这一方面有丰富系统设计经验的工程人员将有很大的发挥空间，将复杂的问题进行简化，最终交由LLM解决，或者等待LLM的能力提升，将更大的自主权交给LLM，这时LLM的应用范围则会更广。</p><h3 id="Code-Gen-生产是最终实现"><a href="#Code-Gen-生产是最终实现" class="headerlink" title="Code Gen - 生产是最终实现"></a>Code Gen - 生产是最终实现</h3><p>Coding Agent 是最重要的组件之一，只有 coding 才能完成“生产”，如果我们认为AI coding是LLM的巨大应用场景，则各个大模型厂商一定会针对性地提升这方面的能力，比如diff代码生成而不是全部重新生成等等性能改进。因此，我认为大模型下一个（或者说目前）火热的研究领域其实就是代码生成，以及与生活上下文所结合的代码生成。</p><p>与此同时，针对机器交互设计的接口也大有可为，作为支持AI Coding的一部分，为AI Coding提供接口，比如语义化的图床搜索，这不同于Agent，而是Agent可以利用的部分，这一块并不是Agent所解决的问题，而是Coding的底层能力：对于架构的整体思考和对API的结构化调用</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>AIOS时代值得做的事：</p><ul><li>硬件：生活Context的收集，直到人工智能降临时，有大量Context的人一定会赢</li><li>软件工程：Follow现在LLM的发展，构建Agent以及围绕Agent构建系统</li><li>研究：解决各类AI Coding的模型难题</li></ul><p>在这一过程中，卖铲子的人可以分到第一笔钱（并非最大的一笔钱）：</p><ul><li>软件：供应AI友好的API、为工程师提供工具</li><li>硬件：针对上面的场景的芯片制造和设计</li></ul>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Agent</tag>
      
      <tag>Operating System</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Improve GitHub Issue&#39;s Search-the LLM way</title>
    <link href="/2024/12/16/2024-12-17-improve-github-issue-search/"/>
    <url>/2024/12/16/2024-12-17-improve-github-issue-search/</url>
    
    <content type="html"><![CDATA[<p>After getting used to using ChatGPT and other LLMs for assistance, I’ve found that GitHub’s Issue search functionality has become increasingly difficult to use. People often struggle to describe the issues they encounter, simply using error messages as titles. At the same time, when searching for issues, I find it hard to search effectively by describing the problems and requirements I’m facing. This forces me to resort to external search engines like Google, but even then, the results aren’t always satisfactory.</p><p>So, why not equip the LLM with knowledge about issues and have it help us find the information we need?</p><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p>I’m planning to build a GitHub Issue search assistant based on RAG (Retrieval-Augmented Generation), with the following architecture. If necessary, it could have internet access.</p><p><img src="/../img/2024-12-17-improve-github-issue-search/architecture.png" alt="architecture"></p><h2 id="User-Case-Searching-for-Issue-Solutions-by-Describing-the-Problem"><a href="#User-Case-Searching-for-Issue-Solutions-by-Describing-the-Problem" class="headerlink" title="User Case: Searching for Issue Solutions by Describing the Problem"></a>User Case: Searching for Issue Solutions by Describing the Problem</h2><h3 id="Scenario"><a href="#Scenario" class="headerlink" title="Scenario:"></a>Scenario:</h3><p>A user encounters a problem during development, such as an error or unexpected behavior in their code or a tool they are using. Instead of searching through GitHub Issues manually or relying on vague error messages, they describe the problem in natural language to the search assistant and get back relevant issue reports, possible solutions, or related discussions.</p><hr><h3 id="Steps-for-the-User"><a href="#Steps-for-the-User" class="headerlink" title="Steps for the User:"></a>Steps for the User:</h3><ol><li><p><strong>User Input:</strong><br>The user enters a natural language query describing the issue they are facing. For example:</p><ul><li><em>“I’m getting a TypeError when trying to call a function from a third-party library.”</em></li><li><em>“The application crashes after I update the dependency, with no clear error message.”</em></li></ul></li><li><p><strong>Query Understanding &amp; Preprocessing:</strong></p><ul><li>The system preprocesses the user query to identify key information, such as error messages, affected components (e.g., function, library, dependency), and context.</li><li>The query is broken down into more structured elements like:<ul><li>Error type (e.g., TypeError)</li><li>Library or tool name</li><li>Context of the issue (e.g., after updating a dependency)</li></ul></li></ul></li><li><p><strong>RAG Model (Retriever):</strong></p><ul><li>The retriever searches GitHub Issues (either locally indexed or via the internet) for relevant discussions, bug reports, or solutions.</li><li>The model looks for matching keywords, error types, and similar scenarios that align with the user’s problem description.</li><li>It could also look for mentions of similar libraries, dependencies, or technologies.</li></ul></li><li><p><strong>RAG Model (Generator):</strong></p><ul><li>Once relevant issues are retrieved, the generator processes this information and synthesizes potential solutions or insights.</li><li>For example, the model could return:<ul><li>A list of GitHub issues that mention the same error or dependency.</li><li>Step-by-step solutions or workarounds mentioned in the issues.</li><li>Common causes and fixes related to the error.</li><li>Related discussions or pull requests that may contain useful fixes.</li></ul></li></ul></li><li><p><strong>Final Output:</strong></p><ul><li>The assistant returns a summary of the relevant GitHub Issues or solutions, including:<ul><li>Links to specific issues or PRs.</li><li>Direct solutions or fixes suggested by the community.</li><li>Possible troubleshooting steps based on similar problems.</li></ul></li><li>Example output:<ul><li><em>“Found 5 related issues: Issue #12345 suggests updating the library to version 2.0.1 to resolve the TypeError.”</em></li><li><em>“Issue #67890 provides a workaround for the crashing issue when updating dependencies. It suggests downgrading to version 1.5.3.”</em></li></ul></li></ul></li></ol><hr><h3 id="Example-Interaction"><a href="#Example-Interaction" class="headerlink" title="Example Interaction:"></a>Example Interaction:</h3><p><strong>User:</strong><br><em>“I’m getting a TypeError when trying to call a function from the <code>axios</code> library after upgrading to version 0.21.0. Has anyone else faced this?”</em></p><p><strong>Search Assistant Output:</strong><br><em>“I found 3 related issues:</em></p><ul><li><em>Issue #1123: A user reported a similar TypeError after upgrading to version 0.21.0. Suggested fix: Downgrade to version 0.20.0.</em></li><li><em>Issue #2456: Another user encountered the same problem but resolved it by updating their Node.js version. See solution here: [Link].</em></li><li><em>Issue #3764: A temporary workaround was proposed involving setting a custom timeout. Full details here: [Link].</em></li></ul><p><em>Additionally, pull request #1245 includes a patch that addresses this error in version 0.21.1.”</em></p><hr><h3 id="Benefits"><a href="#Benefits" class="headerlink" title="Benefits:"></a>Benefits:</h3><ol><li><strong>Efficiency</strong>: The user does not need to sift through countless issues manually or interpret error messages alone.</li><li><strong>Context-Aware Results</strong>: The assistant understands the problem context and brings back solutions that are directly relevant.</li><li><strong>Real-Time Updates</strong>: If the system has internet access, it can pull the latest discussions or solutions, ensuring the user gets the most current and accurate information.</li><li><strong>Enhanced User Experience</strong>: Instead of relying on vague search terms, the user interacts with the system in natural language, making it more intuitive and faster to get solutions.</li></ol><h2 id="User-Case-Searching-for-Issues-Based-on-Desired-Features-or-Requirements"><a href="#User-Case-Searching-for-Issues-Based-on-Desired-Features-or-Requirements" class="headerlink" title="User Case: Searching for Issues Based on Desired Features or Requirements"></a>User Case: Searching for Issues Based on Desired Features or Requirements</h2><h3 id="Scenario-1"><a href="#Scenario-1" class="headerlink" title="Scenario:"></a>Scenario:</h3><p>A user wants to implement a specific feature or solve a problem, and instead of manually searching through documentation or repositories, they describe their desired functionality or requirement to the search assistant. The system then checks if there are any existing GitHub Issues or discussions related to that need.</p><hr><h3 id="Steps-for-the-User-1"><a href="#Steps-for-the-User-1" class="headerlink" title="Steps for the User:"></a>Steps for the User:</h3><ol><li><p><strong>User Input:</strong><br>The user enters a natural language query describing the feature or requirement they want to implement. For example:</p><ul><li><em>“I need to implement role-based access control in my app using Node.js.”</em></li><li><em>“I want to automate the process of deploying Docker containers through GitHub Actions.”</em></li></ul></li><li><p><strong>Query Understanding &amp; Preprocessing:</strong></p><ul><li>The system processes the query to extract key concepts, such as the desired feature (e.g., “role-based access control”, “automated Docker deployment”) and relevant technologies or frameworks (e.g., “Node.js”, “GitHub Actions”).</li><li>The system identifies the components that are part of the desired feature, such as:<ul><li>Feature type (e.g., role-based access control, automated deployment)</li><li>Technology stack (e.g., Node.js, Docker, GitHub Actions)</li><li>Any specific constraints or requirements mentioned (e.g., “automated”, “through GitHub Actions”)</li></ul></li></ul></li><li><p><strong>RAG Model (Retriever):</strong></p><ul><li>The retriever searches through GitHub Issues or external sources (if enabled) for discussions or issues that mention or relate to the desired feature.</li><li>The model looks for related use cases, previous feature requests, and ongoing work related to the requested feature.</li><li>It retrieves issues where similar functionalities have been proposed, discussed, or even partially implemented.</li></ul></li><li><p><strong>RAG Model (Generator):</strong></p><ul><li>After retrieving relevant issues, the generator synthesizes the information to provide detailed insights, suggestions, or solutions.</li><li>It could return:<ul><li>Links to issues or pull requests discussing similar requirements.</li><li>Steps or methods that have been tried by others to implement similar features.</li><li>Discussions of potential problems or limitations when implementing the feature.</li><li>Best practices or workarounds shared by the community.</li></ul></li></ul></li><li><p><strong>Final Output:</strong></p><ul><li>The assistant returns a summary of the related GitHub Issues, including:<ul><li>Links to specific discussions or feature requests.</li><li>Suggested solutions, methods, or code snippets shared by other users.</li><li>Possible blockers or considerations to keep in mind.</li></ul></li><li>Example output:<ul><li><em>“Found 4 related issues: Issue #2345 discusses implementing role-based access control using <code>passport.js</code> with Node.js. You may find it useful. See solution here: [Link].</em></li><li><em>“Issue #4567 proposes a GitHub Actions workflow for automating Docker deployments. Here’s the detailed guide: [Link].”</em></li></ul></li></ul></li></ol><hr><h3 id="Example-Interaction-1"><a href="#Example-Interaction-1" class="headerlink" title="Example Interaction:"></a>Example Interaction:</h3><p><strong>User:</strong><br><em>“I want to implement role-based access control in my Node.js application where users can have different levels of access to specific routes. Has anyone already done this?”</em></p><p><strong>Search Assistant Output:</strong><br><em>“I found 3 related issues:</em></p><ul><li><em>Issue #1024: A user has implemented role-based access control with <code>passport.js</code> and <code>express</code> in Node.js. Suggested implementation here: [Link].</em></li><li><em>Issue #3345: Another user shared a Node.js middleware for handling role-based access. Here’s the code: [Link].</em></li><li><em>Issue #5789: This issue discusses the challenges of implementing granular access control in a multi-role environment with Node.js. See the discussion: [Link].</em></li></ul><p><em>Additionally, pull request #4567 includes a comprehensive approach to handling different user roles with specific route permissions in Express.”</em></p><hr><h3 id="Benefits-1"><a href="#Benefits-1" class="headerlink" title="Benefits:"></a>Benefits:</h3><ol><li><strong>Feature Discovery</strong>: Instead of manually searching for how to implement a feature, the user can describe their needs and let the assistant find relevant discussions.</li><li><strong>Context-Aware Search</strong>: The system understands the requested feature in the context of the technology stack, making the results more accurate and tailored.</li><li><strong>Real-Time Collaboration</strong>: Users can immediately tap into community-driven discussions, solutions, and pull requests that may already contain code snippets or strategies.</li><li><strong>Saves Time</strong>: The assistant cuts down the time spent browsing documentation, reading through multiple issues, and figuring out if others have faced similar challenges.</li><li><strong>Learning from the Community</strong>: The assistant brings together solutions, workarounds, and best practices shared by others, offering a wealth of knowledge for users trying to implement new features.</li></ol>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Github</tag>
      
      <tag>OpenAI</tag>
      
      <tag>Open Source</tag>
      
      <tag>Agent</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Design prompt management CMS</title>
    <link href="/2024/12/15/2024-12-16-design-prompt-manager-cms/"/>
    <url>/2024/12/15/2024-12-16-design-prompt-manager-cms/</url>
    
    <content type="html"><![CDATA[<p>In the <a href="https://blog.wh1isper.top/2024/12/14/2024-12-15-human-in-loop/">previous article</a>, I discussed what and why we need prompt management CMS. In this blog, I want to dive deeper into the role of Prompt CMS in LLM-driven products and explore some design principles and real-world architectures.</p><h2 id="Design-principles"><a href="#Design-principles" class="headerlink" title="Design principles"></a>Design principles</h2><p>Let’s recap the key points from our last post:</p><ul><li><strong>Usability</strong>: Look for an intuitive admin panel suitable for managing prompts.</li><li><strong>API-first design</strong>: Supports REST or GraphQL for integration.</li><li><strong>Versioning</strong>: Support multiple versions of prompts.</li></ul><p>We might need additional work to address the following issues:</p><ul><li><strong>Dynamic Prompt Distribution</strong>: Ensuring prompts can be dynamically delivered to various systems and environments with minimal latency.</li><li><strong>Version Control and Rollbacks</strong>: Managing multiple versions of prompts, enabling easy rollbacks in case of errors or poor performance.</li><li><strong>A&#x2F;B Testing and Experimentation</strong>: Supporting experimentation to evaluate the performance of different prompts under various conditions.</li><li><strong>Scalability and High Availability</strong>: Ensuring the system can scale to handle high traffic and remain operational during peak demand.</li><li><strong>Integration with LLM Observability Tools</strong>: Providing insights into how prompts impact LLM outputs and enabling fine-tuning.</li><li><strong>Security and Access Control</strong>: Safeguarding sensitive prompts and controlling who can access or modify them.</li></ul><p>Let’s set aside security requirements for now and focus purely on functional implementation:</p><ol><li><p><strong>Dynamic Prompt Distribution</strong>:</p><ul><li>Implement a centralized repository for prompts.</li><li>Use APIs to fetch prompts in real-time based on context or conditions.</li><li>Introduce caching mechanisms to reduce latency and improve responsiveness.</li></ul></li><li><p><strong>Version Control and Rollbacks</strong>:</p><ul><li>Store prompt versions in a database with metadata like timestamps and change logs.</li><li>Build a user interface to browse, compare, and restore previous versions.</li></ul></li><li><p><strong>A&#x2F;B Testing and Experimentation</strong>:</p><ul><li>Add a configuration module to define testing groups and criteria.</li><li>Develop logic to randomly assign requests to different prompt versions.</li><li>Collect and analyze feedback or performance metrics for each variant.</li></ul></li><li><p><strong>Integration with LLM Observability Tools</strong>:</p><ul><li>Enable logging and tracking of prompt performance (e.g., response accuracy, user satisfaction).</li><li>Provide hooks or APIs for LLM observability platforms to gather metrics directly.</li></ul></li><li><p><strong>Extensibility and Modularity</strong>:</p><ul><li>Design the system as a modular framework, with components like prompt management, delivery, and observability as separate but integratable services.</li></ul></li></ol><p>By focusing on these core functionalities, the platform can deliver a flexible and developer-friendly toolset to optimize prompts in LLM-driven workflows.</p><h2 id="Let’s-build-it-together"><a href="#Let’s-build-it-together" class="headerlink" title="Let’s build it together"></a>Let’s build it together</h2><p>Among these, 3 (A&#x2F;B Testing and Experimentation), 4 (Integration with LLM Observability Tools), and 5 (Extensibility and Modularity) are optional features that can be implemented through future engineering efforts.</p><p>For now, the focus should be on building a solid foundation with the following:</p><ol><li><strong>Dynamic Prompt Distribution</strong>: Ensuring efficient and real-time delivery of prompts.</li><li><strong>Version Control and Rollbacks</strong>: Providing a reliable mechanism to manage and revert prompt changes.</li></ol><p><img src="/../img/2024-12-16-design-prompt-manager-cms/version0.png" alt="First version of prompt manager CMS"></p><p>For a better user experience, we need to support local debug and upload in CMS SDK, in which they can debug, test, and upload prompts directly. Addtionally, once a prompt is used, SDK can also report the <code>task_id</code> or something like that for tracking.</p><h2 id="Adding-all-components"><a href="#Adding-all-components" class="headerlink" title="Adding all components"></a>Adding all components</h2><p><img src="/../img/2024-12-16-design-prompt-manager-cms/full-feature.png" alt="Final version of prompt manager CMS"></p><p>For continuous integration, we can use webhook to trigger a new integration for LLM tasks. It may be a series of metrics tests to test the metric scores of this version of prompt or parrallelly generate many projects for many scenarios. In this stage, we don’t need local debug and upload in CMS SDK cause we can use webhook to trigger a new integration and the number of scenarios we need to test does not allow us to test in local way.</p><p>For LLM observability tools, we only need to replace the native LLM SDK to <em>Observed LLM SDK</em> for automated data collection.</p><p>For A&#x2F;B Testing and Experimentation, we can add a new module to define testing groups and criteria, in plugin way.</p>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Prompt management</tag>
      
      <tag>Prompt CMS</tag>
      
      <tag>CMS</tag>
      
      <tag>Headless CMS</tag>
      
      <tag>Human in loop</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Human in loop-prompt management and CMS</title>
    <link href="/2024/12/14/2024-12-15-human-in-loop/"/>
    <url>/2024/12/14/2024-12-15-human-in-loop/</url>
    
    <content type="html"><![CDATA[<h2 id="What-and-why-prompt-management"><a href="#What-and-why-prompt-management" class="headerlink" title="What and why prompt management"></a>What and why prompt management</h2><p>Recently, I’ve been thinking about how to create a prompt management platform that can interact with programs.</p><p>In my vision, a prompt management platform should have the following characteristics:</p><ul><li><strong>Management Interface</strong>: A user-friendly interface that allows individuals without technical backgrounds to manage prompts with ease.</li><li><strong>Configuration Center</strong>: A system to distribute prompts, enabling features like A&#x2F;B testing and gradual updates (gray releases).</li><li><strong>Integration</strong>: Seamless integration with other platforms or programming languages, such as open-source or self-developed multi-agent systems, as well as LLM observability infrastructure.</li></ul><p>I later realized that a Headless CMS might be exactly what I’m looking for—or rather, it could be a good start for building this system.</p><h2 id="Headless-CMS"><a href="#Headless-CMS" class="headerlink" title="Headless CMS"></a>Headless CMS</h2><p>A Headless CMS (Content Management System) is a backend-only system for managing content, which decouples the content creation and management process from the front-end delivery. Instead of rendering web pages directly, it provides an API (e.g., REST or GraphQL) to deliver content to any platform or device, such as websites, mobile apps, or IoT devices.</p><ul><li><strong>Content Delivery via API</strong>: Content is accessed through APIs, making it flexible for various use cases.</li><li><strong>Platform Agnostic</strong>: Content can be reused across multiple platforms and devices.</li><li><strong>Separation of Concerns</strong>: Backend and frontend can be developed independently.</li><li><strong>Scalability and Performance</strong>: Optimized for delivering content quickly to multiple endpoints.</li></ul><h2 id="Key-points-for-choosing-a-Headless-CMS-for-prompt-management"><a href="#Key-points-for-choosing-a-Headless-CMS-for-prompt-management" class="headerlink" title="Key points for choosing a Headless CMS(for prompt management)"></a>Key points for choosing a Headless CMS(for prompt management)</h2><h4 id="Usability"><a href="#Usability" class="headerlink" title="Usability"></a>Usability</h4><ul><li><strong>Non-technical user interface</strong>: Look for an intuitive admin panel suitable for managing prompts.</li><li><strong>Customizable schemas</strong>: Ensure you can define prompt-related fields like metadata, versions, and tags. This is more like a key-value store than a relational database, and I beliveve it is a better fit for cms systems too.</li></ul><h4 id="API-Capabilities"><a href="#API-Capabilities" class="headerlink" title="API Capabilities"></a>API Capabilities</h4><ul><li><strong>API-first design</strong>: Supports REST and GraphQL for flexible integration. JSON REST API is my first choice as I can build python&#x2F;TypeScript client easily.</li><li><strong>Real-time updates</strong>: Webhooks or real-time API support to synchronize prompts dynamically.</li><li><strong>Scalability</strong>: Handles high traffic and supports dynamic content fetching.</li></ul><h4 id="Extensibility-and-Integration"><a href="#Extensibility-and-Integration" class="headerlink" title="Extensibility and Integration"></a>Extensibility and Integration</h4><ul><li><strong>Versioning</strong>: Built-in or easy to implement with plugins, for A&#x2F;B testing and gray releases.</li><li><strong>Observability</strong>: Compatibility with logging or analytics tools for monitoring prompt performance. Or I can implement it myself with injecting metadata.</li><li><strong>Customization</strong>: For advanced features and administration</li><li><strong>Authorization</strong>: SSO or OAuth for easily managing members and permissions.</li></ul><h4 id="Open-Source-and-Community"><a href="#Open-Source-and-Community" class="headerlink" title="Open-Source and Community"></a>Open-Source and Community</h4><ul><li><strong>Active community support</strong>: Choose a CMS with a strong developer ecosystem.</li><li><strong>Easy integration</strong>: Easy integration with other platforms and programming languages.</li></ul><h2 id="Some-chooses"><a href="#Some-chooses" class="headerlink" title="Some chooses"></a>Some chooses</h2><h4 id="Strapi"><a href="#Strapi" class="headerlink" title="Strapi"></a>Strapi</h4><ul><li>a clean, user-friendly admin panel that allows non-technical users to manage content easily.</li><li>JSON REST API and GraphQL out of the box.</li><li>Direct logging or integration with external tools like Datadog or custom observability pipelines via middleware.</li><li>Strapi is open-source with a large, vibrant community and frequent updates. And works seamlessly with modern stacks (Next.js, Vue, Python, TypeScript) and is widely adopted in enterprise environments.</li></ul><h4 id="Directus"><a href="#Directus" class="headerlink" title="Directus"></a>Directus</h4><ul><li>Works as a database wrapper, so it integrates easily with a JSON-like schema.</li><li>Focuses on scalability and flexibility with SQL databases.</li><li>Supports webhooks, custom flows, and JSON REST APIs.</li></ul><h4 id="Payload-CMS"><a href="#Payload-CMS" class="headerlink" title="Payload CMS"></a>Payload CMS</h4><ul><li>TypeScript-first with robust support for API customization.</li><li>Excellent versioning and role management.</li><li>Perfect for developers who need deep customizability.</li></ul><h4 id="KeystoneJS"><a href="#KeystoneJS" class="headerlink" title="KeystoneJS"></a>KeystoneJS</h4><ul><li>GraphQL-first but supports REST.</li><li>Schema-driven with great extensibility.</li><li>Ideal for modern JavaScript ecosystems like Next.js.</li></ul><h3 id="My-thoughts"><a href="#My-thoughts" class="headerlink" title="My thoughts"></a>My thoughts</h3><p>For me, scalability might not be the top priority. What I’m likely to focus on first is faster development efficiency and integration speed. And, I may not need a relational database; instead, I might choose a NoSQL database like MongoDB to gain better operability. For example, we could develop our own system to replace the CMS system.</p><p>I plan to start by exploring Payload CMS, a full-stack project based on Next.js. It has a very active community and doesn’t have any paid features, only offering cloud services and paid support. Stay tuned, I will continue to update related progress here!</p>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Prompt management</tag>
      
      <tag>Prompt CMS</tag>
      
      <tag>CMS</tag>
      
      <tag>Headless CMS</tag>
      
      <tag>Human in loop</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>网飞原则随记01</title>
    <link href="/2024/12/03/2024-12-04-Netflixs-principles-01/"/>
    <url>/2024/12/03/2024-12-04-Netflixs-principles-01/</url>
    
    <content type="html"><![CDATA[<p>网飞标榜自己的原则是代表当今追求灵活、创新的企业所需要具备的，或者说，网飞认为自己的原则让自己有了极度的灵活和创新能力。没有在网飞工作过，所以我也不知道他们所说是否属实，不过书中的内容确实与我之前的想法见和：优秀的智力劳动者不能以规则管理。想要在快速变化的环境中抓住机会，就要雇佣最好的人，给他最好的待遇和自由，让他发挥创造力。在我的经历中，我发现大多数成功的团队都是相似的，而不成功的团队各有各的不幸。</p><p>第二个感触是，以网飞的原则积极地行事，能够降低自己的精神内耗，也能帮助自己走出舒适区，挑战更多的可能性，这也助于更好地认识自己，或许这和体验派相辅相成。</p><blockquote><p>Enjoy your life!</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>了解自己是一件容易被忽略但一定坚持做的事</title>
    <link href="/2024/11/24/2024-11-25-Know-yourself/"/>
    <url>/2024/11/24/2024-11-25-Know-yourself/</url>
    
    <content type="html"><![CDATA[<p>很多时候都忘了了解自己，到底什么是自己所想要做的，做什么事情能够让自己感觉到热情，或者说，对自己好一点的方式是积极寻找自己的热情。</p><p>最近一直在思考，自己到底是喜欢清闲的生活，还是喜欢更忙碌一些的生活，是喜欢解决问题，还是仅仅喜欢别人的夸赞而去做一些讨好别人的事。有些时候人就是如此矛盾的，一种叙事是为了一份工作而倾尽全力是不值得的，但是为了自己喜欢的事倾尽全力又是充满着浪漫主义的。当工作的内容是喜欢做的事情，那这个时候到底是值得还是不值得呢？给自己喜欢做的事情蒙上了工作的外衣，获得一份自己觉得还不错的薪水，是否就让这件事变了味？还是说，其实是内心觉得这样的薪水配不上这样的工作？哦，如果这样想的话，那可能是对热爱有所误解，也或者是，真心地感到疲倦了吧。</p><p>不管怎么说，做到最好仍然是我的追求，不知不觉间，真的规格严格功夫到家了呢。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>⭐️AGI is the pilot, not the copilot</title>
    <link href="/2024/11/23/2024-11-24-AGI-is-pilot-not-copilot/"/>
    <url>/2024/11/23/2024-11-24-AGI-is-pilot-not-copilot/</url>
    
    <content type="html"><![CDATA[<p>这两天翻看自己之前的博客，突然发现在大概一年前我就已经对当前AI的发展做了一些判断，不过当时是从<a href="https://wh1isper.github.io/2024/01/09/2024-01-10-AIGC%E4%B8%8E%E6%95%B0%E5%AD%97%E7%BB%8F%E6%B5%8E/">产业智能和生成式娱乐的角度</a>上谈的。今天突然有一些想法，仅做随笔记录。</p><h2 id="生成式娱乐：从UGC到AIGC"><a href="#生成式娱乐：从UGC到AIGC" class="headerlink" title="生成式娱乐：从UGC到AIGC"></a>生成式娱乐：从UGC到AIGC</h2><p>生成式娱乐的演进有其反垄断意义，让普通大众从原本的仅能从各个大的平台（Twitter、微博、抖音）获取用户创作的内容，到每个用户可以通过AI生成自己想要的内容。将人们从大厂的定制化信息流中解脱出来，以一种革命性的方式让用户可以创造自己的idol、自己的聊天伴侣，从而打破巨头对IP、文化产业、内容制作的垄断。理论如此，但这里仍有几个隐忧，一个是已有利益集团通过他们现在掌握的手段，对AIGC进行各种抹黑、黑粉操作，比如Charact.ai被诉导致人们沉迷于虚拟角色而产生各种心理问题，只要有人想打响诉讼战，则AIGC的独角兽们就需要为此疲于奔命，最后或许难逃以身相许给其他大厂或利益集团的命运，也就是说，勇者被恶龙简单吃掉了；其二是目前技术的限制，并不能创造能满足人们较高精神需求的作品，这也是我对这一项工作感到不够兴奋的原因，固然我们可以用LLM做黄聊，用diffusion搞黄图，但是我们很难让AI搞出一部三体，让我们的用户沉迷于此，但代价是什么？</p><blockquote><p>马克思认为，包括版权在内的知识产权制度是资产阶级维护自身利益的工具之一，即通过垄断知识成果来实现超额收益，但我这里不想讨论知识付费之类的问题，因为我们在市场化的环境中，就没办法脱离市场来“销售”任何一件“商品”。</p></blockquote><h2 id="让AI成为Pilot，不是Copilot"><a href="#让AI成为Pilot，不是Copilot" class="headerlink" title="让AI成为Pilot，不是Copilot"></a>让AI成为Pilot，不是Copilot</h2><p>LLM作为Copilot已经有一段日子了，最近大家正在努力地让AI掌控计算机、掌控汽车（智驾），实际上，在智驾层面，智能驾驶AI比现在的大模型更加符合生产力进步的标准，更加接近于成为Pilot，而目前很多的LLM Agent仍然处于Copilot层面，他们要么是本来想做一个超级AI，自动搞定一些，接着铩羽而归，要么就是根本没有尝试，对于Agent浅尝辄止。目前，乐观者认为大模型的潜力仍未被充分发掘，而硬币的另一面则是大模型的表面实力也难以更进一步，人类已经拿不出足够的算力和资源对大模型训练进行进一步支持，这从GPT5的难产和各个大模型厂商都在玩所谓思维链就能看出，LLM愈发从模型参数的军备竞赛，转向对LLM的进一步调教。也就是说，接下来的世界可能是属于微调和领域小模型的，而这一步也正是产业化的最关键一步。当人们以自己擅长的领域为基础，对大模型进行充分发掘时，或许我们最终可以到达那应许之地，实现AI技术的产业化，达成我们这个时代的生产力飞跃。</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>在这一判断下，我认为目前任何以Agent作为卖点的产品和公司都将在很快一段时间里受到挑战，聪明的人们已经越来越开始“发掘模型的潜力”，而所有做产品、以产品力见长的公司，也需要将下一阶段的重点放在更加激发模型的生产力上，这也是所谓AI原生应用产品力的最终体现。</p><blockquote><p>对于个人选择，我仍然觉得智驾和产业智能化是比较好的选择，这里的产业可以是制造业，也可以是互联网、广告等产业，而生成式娱乐和内容创作在我这大半年的实践中，我感觉其创造的价值愈发有限，其整体叙事愈发难以自圆其说，因此目前持保留态度。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🖋Time does not flow evenly</title>
    <link href="/2024/11/19/2024-11-20-time-does-not-flow-evenly/"/>
    <url>/2024/11/19/2024-11-20-time-does-not-flow-evenly/</url>
    
    <content type="html"><![CDATA[<p>最近越发觉得时间不是匀速流动的，常常会觉得自己花费了很多的时间，做了很多的工作，但是回头来看也就是那么多。可能这就是人类脑力劳动的样子，每天都觉得自己殚精竭虑，心力耗竭，但回头看，自己只不过是做了一片森林中的一棵小草。于是我在想，健康的方式是否是限制自己的工作时长，做好充分的休息，成为一个不止是有工作成就的人。所以我理想中的工作应该有如下特征：可以有rush、有加班，但是需要在计划内；当下班的时候就是真正的下班，事情留到第二天上班处理；如果有事情处理不完，发生了第二天只能处理前一天堆积的事情，就需要整理事情的优先级和必要性，该加人加人、该砍需求砍需求。</p><p><strong>这好像就是劳动法想象中的劳动的样子</strong></p><p>人类的大脑对事物发展的估计，不仅是短期对于一项工作的完成时间，还包括对长期的发展，不管是凭直觉地看，还是从数据的角度看，都尝尝展现明显的偏差。与此同时，人们又寄希望于某种机械的、形而上的方式，对事物的发展进行预测，其最终的结果是变为对自己的心理按摩，而不是对最终结果的追求。</p><p>我现在发现，所谓的OKR，绝对不能成为KPI的换皮，如果将OKR当做是第一、第二、第三优先级KPI进行设置，那么就不可避免的陷入上述机械的、形而上的管理方式中，忽视了目标的重要性，而将一开始进行规划时的粗浅认知通过KPI里程碑的方式固化下来，在执行过程中因为各种流程原因，不愿不想也不能进行修正，由此内生地造成了效率低下和目标偏离。</p><p>而OKR并不是灵丹妙药，由于组织内部权力和资源的分配，当任务需要人配合的时候，OKR并不能区分优先级，特别是当有多个项目并行的时候，人们就只能陷入按权分配、按急分配的状态，而这种状态并不会因为人员增加而改变，因为人员的增加又会带来新的目标的增加，从而出现一个正反馈循环。这时候我们发现，多项目、多目标造成的资源争抢，对人们进行脑力劳动、脑力劳动项目管理提出了重大的挑战。似乎没有望之四海皆准，也没有时刻适用的管理方式与组织结构。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>☁️千人千面与信息茧房</title>
    <link href="/2024/11/12/2024-11-13-%E5%8D%83%E4%BA%BA%E5%8D%83%E9%9D%A2%E4%B8%8E%E4%BF%A1%E6%81%AF%E8%8C%A7%E6%88%BF/"/>
    <url>/2024/11/12/2024-11-13-%E5%8D%83%E4%BA%BA%E5%8D%83%E9%9D%A2%E4%B8%8E%E4%BF%A1%E6%81%AF%E8%8C%A7%E6%88%BF/</url>
    
    <content type="html"><![CDATA[<p>这是一篇单纯的随笔，记录一下最近突然涌入到脑海里的一些思考。</p><p>在评价web 2.0的时候，我们通常将google等厂商形容为广告公司，因其独特的广告算法和推荐算法，将人们的注意力牢牢抓住，再通过广告的方式进行变现。由此，谷歌等互联网巨头们赚取了足够的利益。而在AIGC时代，人们希望通过生成式的方式，从UGC模式过度到AIGC模式，通过生成人们想要的（而非推荐人们想要的），不仅仅抓住人们的注意力，实现广告变现，同时基于人们对内容欲望，使其付费给产品以直接获得利益。</p><p>这一模式下，让我想到里信息茧房理论。实际上，在UGC时代，大厂们并不想一直给一个人推荐固定的内容，这将导致人们很快就厌倦，反而降低了用户的使用时长。而在AIGC时代则恰恰相反，AIGC本身就带有多样性的特点，平台可能更希望推荐用户熟悉的AIGC内容，使得用户不停地使用这一工具，再通过用户画像等手段，告诉AIGC如何更加取悦用户。这其中的区别在于，UGC需要等待创作者创作，再到消费者发现作品，而AIGC可以直接实现消费者自己创作（或参与创作），然后直接进行消费。其带来的结果是：AIGC应用进行信息茧房的塑造能够实现正反馈，即越来越抓住用户，而UGC应用进行信息茧房的塑造只会更快地让用户疲惫。</p><p>这让我想起了我在<a href="https://wh1isper.github.io/2024/11/06/2024-11-07-AIGC-for-what/">上一篇文章</a>谈到的，是否AIGC时代代表着数据资产的公地悲剧？也代表着人们的精神文明将受到一次考验和冲击？与此同时，“自由国家”的瘾君子问题，让我们看到了医药复合体是如何压榨人类的精神和肉体的，如果AI发展下去，是否一定指向的是赛博飞升的未来呢？</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>信息茧房</tag>
      
      <tag>传播学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AIGC for what? The future of AIGC</title>
    <link href="/2024/11/06/2024-11-07-AIGC-for-what/"/>
    <url>/2024/11/06/2024-11-07-AIGC-for-what/</url>
    
    <content type="html"><![CDATA[<p>最近交流了很多家创业公司，其中包括 LLM 的应用，比如营销写作工具、营销机器人 Agent、Chatbot，也有一些 AI 视频工具。<br>我觉得可以分为两类，一类是通过AI工具实现降低成本，通常是toB的业务，比如营销写作工具、营销机器人。而另一类是通过AI技术进行一些新的价值创造，当然这一类价值创造有的也是为了降低之前的人力成本，比如编程、视频制作、特效等方面，但重点在于以AI为中心，尽量减少人们的操作，还有一类则是通过创造虚拟IP的方式实现无中生有。</p><p>这里我不想去评价哪种方式的好与坏，我只是在思考一个问题：人在其中扮演什么样的角色？</p><p>以营销机器人举例，一般人们都会想到企业使用了营销机器人，在社交媒体自动回复、水军等行为，可以为企业降低成本、引流。但是从社交媒体的角度，从每一个已经离不开社交媒体的用户而言<br>这是否是对他们权利以及大脑的污染。当每个企业都争相突破下线地使用LLM技术进行营销和水军，赛博朋克时代是否会很快到来？</p><p>虚拟IP的路线也有着同样的问题。好的情况是，打破阅文、迪士尼的垄断，让每个人都可以低成本地创作、孵化出更多IP，坏的情况则是人们将自己的恶投射在LLM上，搞色情等黑色产业。<br>不过这种滥用研究的较多，应该很难有特别坏的情况出现。</p><p>AI工具呢？AI工具或许是最无害，也是最无用的一项技术，你可以看做它是一种改进的煤炭发电机，可以提升挖煤的效率，但是仍然是化石能转化为电能，仍然是由人操作，仍然不产生很大的额外价值。<br>而AI工具想要真正放大其收益，又要找到类似上述的路线进行发展。</p><p>因此，我目前认为，创造真正生产力的路线是LLM和AIGC的唯一出路，提升人们的物质生活和精神生活才是未来的方向。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AIGC</tag>
      
      <tag>thinking</tag>
      
      <tag>startup</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🤔Some Note about Building a System from Scratch in startup</title>
    <link href="/2024/11/04/2024-11-05-some-note-on-0to1/"/>
    <url>/2024/11/04/2024-11-05-some-note-on-0to1/</url>
    
    <content type="html"><![CDATA[<p>Here, I want to share some key takeaways from building a backend and inference system from scratch at Morph Studio. This includes what I did right, where I went wrong, and what I missed along the way, concluding with a checklist that captures these insights as comprehensively as possible.</p><h2 id="What-I-did-right"><a href="#What-I-did-right" class="headerlink" title="What I did right"></a>What I did right</h2><h3 id="Intergrate-CI-CD-ASAP"><a href="#Intergrate-CI-CD-ASAP" class="headerlink" title="Intergrate CI&#x2F;CD ASAP"></a>Intergrate CI&#x2F;CD ASAP</h3><p>CI&#x2F;CD has significantly improved our efficiency during development. With GitHub Actions, we’ve implemented unit testing, integration testing, automated deployment, and code review, which has minimized rework on the backend and ensured code quality and stability.</p><p>This has enabled each developer to deploy both the development and production environments, even in the early stages when we lacked dedicated operations and deployment personnel.</p><p><strong>Unit testing and Integration testing is very important</strong>, even when rapidly releasing new features or changing existing ones. This ensures that we don’t break the system or introduce bugs critically, effectively acting as a safety barrier.</p><h3 id="Model-and-segment-the-business"><a href="#Model-and-segment-the-business" class="headerlink" title="Model and segment the business"></a>Model and segment the business</h3><p>Analyzing the business and abstracting it is essential. Initially, I intended to design the backend as a monolith, but I later realized that there were many reusable functionalities and modules, along with varying update and release cycles for each module.</p><p>As a result, I divided the system into three distinct parts: <strong>infrastructure</strong> (including user management, storage, subscriptions, etc.), <strong>business systems</strong> (covering canvas business, social business, and AI tool business), and <strong>inference systems</strong> (encompassing queue connectors, schedulers, and inference clusters across different cloud regions). These three systems have clear boundaries and dependencies, allowing developers to quickly identify and resolve issues while also clearly delineating development responsibilities.</p><h3 id="Use-cloud-for-scaling"><a href="#Use-cloud-for-scaling" class="headerlink" title="Use cloud for scaling"></a>Use cloud for scaling</h3><p>Using public cloud services, especially cloud-hosted Kubernetes, significantly reduces operational costs. We don’t have to manage bare metal machines or carefully plan how many servers to purchase; we can simply use the elastic resources provided by the public cloud. However, it’s crucial to avoid vendor lock-in, such as relying on vendor-specific secret management, certificate management, and various SDKs embedded in the code. One lesson I’ve learned is to use common technologies for system construction, such as using Boto3 for S3 connections, which minimizes issues when switching from AWS S3 to GCS.</p><h2 id="Where-I-went-wrong"><a href="#Where-I-went-wrong" class="headerlink" title="Where I went wrong"></a>Where I went wrong</h2><h3 id="Expand-the-team-too-late"><a href="#Expand-the-team-too-late" class="headerlink" title="Expand the team too late"></a>Expand the team too late</h3><p>We often hesitate to recruit new engineers, fearing that they may not quickly contribute to the team’s workload and might instead increase communication and training costs. However, continuously delaying the recruitment of new engineers will only worsen the team’s situation. As the number of features that need maintenance increases, existing engineers will have to divert their time from developing new features to maintaining old ones. What’s often overlooked is that maintaining legacy features can be a great entry point for new engineers, much like a “good first issue.” Those lower-priority bugs and requirements are well-suited for onboarding new engineers.</p><h2 id="What-I-missed-along-the-way"><a href="#What-I-missed-along-the-way" class="headerlink" title="What I missed along the way"></a>What I missed along the way</h2><h3 id="Forgot-Data-Collection-and-ETL-systems"><a href="#Forgot-Data-Collection-and-ETL-systems" class="headerlink" title="Forgot Data Collection and ETL systems"></a>Forgot Data Collection and ETL systems</h3><p>Data collection and analysis are crucial in business decision-making. When a team lacks BI experts, engineers often forget to incorporate the need for data collection and analysis during the design and implementation of systems. I only realized this after our product had been live for a while: the CEO frequently requires various metrics, such as daily active users, monthly active users, and retention rates, but often doesn’t know how these metrics should be defined within our product. The backend team also cannot simply provide a statistical answer to an undefined metric.</p><p>Looking back, we didn’t have any BI experts involved in the initial system design to address these issues; instead, we relied on some marketing terminology without understanding the actual meanings behind those terms and how to calculate them.</p><h3 id="DID-NOT-be-Granted-Authorization-for-distribution-of-benefits"><a href="#DID-NOT-be-Granted-Authorization-for-distribution-of-benefits" class="headerlink" title="DID NOT be Granted Authorization for distribution of benefits"></a>DID NOT be Granted Authorization for distribution of benefits</h3><p>As the product evolves and the team continues to collaborate, certain individuals will inevitably step into important roles, whether consciously or unconsciously. Our task is to recognize these individuals’ leadership positions and grant them sufficient authority to help them manage the team more effectively. When a person has responsibilities without the corresponding authority (and benefits associated with those responsibilities), they often end up feeling disheartened after their energy is depleted, which can have a very negative impact on the team’s operations and morale.</p><p>When we identify someone as increasingly important to the team, we should provide them with more opportunities for advancement and support. This approach helps build a tiered engineering team and fosters a sense of unity within the group.</p><p>A significant pitfall occurs when the CEO wants to manage every individual but assigns important responsibilities to certain employees without sharing the necessary authority with them. This situation effectively places the overall risk of the company in the hands of those employees, who may not have sufficient incentives to perform well, especially if they are individual contributors with fixed salaries—meaning they lack both the authority and the rewards associated with those responsibilities. <strong>In my view, this scenario often results from the CEO’s desire to control everything and their focus on role-playing rather than on the success of the team</strong>. Employees may struggle to recognize this dynamic, often only realizing later that they have been let down.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul><li>Intergrate CI&#x2F;CD as early as possible, and leverage the elasticity of public cloud for scaling to reduce operational costs.</li><li>Write tests, including unit tests and integration tests, especially during repid releases.</li><li>Use cloud for scaling. This helps reduce operational costs. But be cautious of vendor lock-in.</li><li>When designing a system, beyond functional requirements, it’s essential to consider non-functional requirements that go beyond just performance and maintenance costs. This includes organizing needs related to data collection, analysis, and tracking.</li><li>Expand the team in a timely manner and assess each member’s optimal workload range. This ensures there are sufficient resources for improvement, maintenance, and development as features grow.</li><li>Business is about the distribution of benefits; without sufficient authority to manage that distribution, it becomes impossible to effectively manage a team.</li></ul>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>架构师</tag>
      
      <tag>Architecture</tag>
      
      <tag>Startup</tag>
      
      <tag>初创公司</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🔧Build API Gateway with OpenResty</title>
    <link href="/2024/11/03/2024-11-04-Using-openresty-for-apigateway/"/>
    <url>/2024/11/03/2024-11-04-Using-openresty-for-apigateway/</url>
    
    <content type="html"><![CDATA[<h2 id="Introduction-about-API-Gateway"><a href="#Introduction-about-API-Gateway" class="headerlink" title="Introduction about API Gateway"></a>Introduction about API Gateway</h2><blockquote><p>I’m currently working on writing blog posts in English. If anything is unclear, feel free to leave a comment.</p></blockquote><p>When building microservices, we often use a reverse proxy to expose different services through a single endpoint, which just looks like this:</p><p><img src="/../img/2024-11-04-Using-openresty-for-apigateway/01.png"></p><p>In practical, we usually design the user system separately from the core business system and manage user login status using a cookie-session approach. Cookies are typically stored in an encrypted format on the client side, and decrypting them requires a server-side secret. In this kind of architecture, an internal HTTP request is often needed to convert the cookie into plaintext user information for use by the business system.</p><p><img src="/../img/2024-11-04-Using-openresty-for-apigateway/02.png"></p><p>This means that every service within the business system needs to implement an HTTP request and handle potential failures of the user system. While we could address this by providing a unified SDK, which would shift the challenge to SDK implementation and library management, the concept of a service mesh offers an alternative solution.</p><p><img src="/../img/2024-11-04-Using-openresty-for-apigateway/03.png"></p><p>By leveraging an API Gateway, we can centralize this process and effectively manage user authentication across services. Additionally, an API Gateway has the advantage of enabling permission management, auditing, and observability for each API route. However, I don’t plan to go into detail about these aspects in this blog post.</p><p>The only task we need to undertake is moving the work previously handled by the User Server SDK into our reverse proxy. Before forwarding requests to the business system, the reverse proxy converts the cookie into plaintext user data and injects it into a specific request header. This way, the business system can directly access the user data and respond accordingly.</p><p><img src="/../img/2024-11-04-Using-openresty-for-apigateway/04.png"></p><h2 id="Why-OpenResty"><a href="#Why-OpenResty" class="headerlink" title="Why OpenResty?"></a>Why OpenResty?</h2><p>If you’re already familiar with Nginx, you likely know that it allows custom logic in the proxy process through Lua scripts. OpenResty builds on this by integrating a set of Lua libraries with Nginx. With OpenResty, you can implement cookie exchange without significantly modifying the existing Nginx configuration—just by writing a simple Lua script.</p><p>You should consider using OpenResty when the following conditions apply:</p><ul><li>You have previous experience with Nginx or are already using it.</li><li>You’re looking for a lightweight API gateway that doesn’t need complex features but can handle user authentication efficiently.</li></ul><p>When you need more advanced features like circuit breaking, retries, or rate limiting, you might want to consider a service mesh or an enterprise-level API gateway, such as Kong or Traefik.</p><h2 id="The-Implementation-WIP"><a href="#The-Implementation-WIP" class="headerlink" title="The Implementation(WIP)"></a>The Implementation(WIP)</h2><blockquote><p>You can find the source code of this post in <a href="https://github.com/Wh1isper/openresty-api-gateway-example">Github</a></p></blockquote><p>Here, I plan to provide a production-ready implementation example. While it may be somewhat complex, it ensures a high level of security.</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>架构解析</tag>
      
      <tag>Architecture</tag>
      
      <tag>OpenResty</tag>
      
      <tag>Nginx</tag>
      
      <tag>Lua</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🔧云计算：不需要运维工程师</title>
    <link href="/2024/05/01/2024-05-02-%E4%B8%8D%E9%9C%80%E8%A6%81%E4%BB%BB%E4%BD%95%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    <url>/2024/05/01/2024-05-02-%E4%B8%8D%E9%9C%80%E8%A6%81%E4%BB%BB%E4%BD%95%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E5%B8%88/</url>
    
    <content type="html"><![CDATA[<blockquote><p>本文基于我的两段工作经验撰写，不排除之后我对这件事的看法有所改变。</p></blockquote><p>首先介绍我的两段工作经历，第一段是在某网络安全大厂任Python开发工程师，产品是toB的数据安全属性的数据分析软件，算上实习总共任职了三年半，期间从修BUG的实习生到小组长，负责Jupyter和对应的Papermill、Serving相关架构设计和二开，期间经历了三个运维工程师。第二段则是目前在创业公司做后端和基础设施架构师，团队里没有运维工程师，服务部署在AWS上，基础设施依赖包括但不限于AWS的各类云计算服务。</p><p>在我第一段经历中，运维工程师通常由如下职责：</p><ul><li>提供公共的开发、测试、生产、演示环境，以支持开发、测试、产品演示等活动</li><li>对服务进行上下线，配置服务路由等</li><li>对服务器进行维护，缩扩容</li><li>对基础设施进行维护，比如存储、DB，为研发的各类需求提供基础件，如对象存储、块存储、Redis等等</li><li>对线上服务情况进行监测、告警</li></ul><p>如此看来，似乎运维承担了团队中的重要职责，不容忽视，但实际上，运维在这些方面都有做的不好的地方。</p><ol><li>公共环境的支持：对于K8S架构下，主要是机器的腾挪和集群的部署，在自动化脚本的帮助下，这一类任务的工作时间不会太长。在第二段经历中，由云厂商host的K8S集群取代，由Github Action取代运维人工发布。</li><li>运维进行服务上下线和路由定义，需要运维工程师和研发工程师进行沟通，运维工程师很难完全理解研发架构，而快速迭代的时候服务上下线和相关配置常常修改，我就有因为运维没时间而无法上线服务的情况。现在，我们由研发工程师进行服务上线工作，减少了不必要的沟通。</li><li>服务维护缩扩容，之前已经通过K8S的autoscale实现了，目前也是通过AWS的自动缩扩实现的</li><li>基础设施的维护，经典的运维已经无法支持了，比如之前的运维熟悉MySQL但不熟悉PGSQL，就无法对数据库进行调优配置，只能求助集团的工程师，但集团的工程师毕竟能力和经验都有限，在这上面我们付出了很多的精力，包括对Ceph的调优和维护，常常会因为存储性能和带宽瓶颈影响性能甚至业务功能。在第二段经历里，我发现AWS提供的存储、网络、DB、Redis等更加可靠，性能也更好。</li><li>只需要集成Grafana等服务即可，不需要专门的运维人员。</li></ol><p>综上，运维的核心功能已经不再刚需，如瑞典马工在《是时候让运维集体下岗了》所言，我们不需要运维，而需要更加内聚的开发团队，由每个Dev承担DevOps职责，再通过各个团队的组织，提供所需要的基础件。比如成立专门的Ceph团队，团队内通过DevOps方式进行二开，为所有人提供基础存储服务件。业务团队DevOps化，对服务进行直接部署和管理。</p><p>传统运维何去何从？其实在我的第一段经历中，运维还有一个职责：出差给客户部署环境。但这是吃力不讨好的，纯苦差事，也没有任何技术上的积累。这一职位同样可以被售前工程师替代。所以，或许每位运维的归途，要么是Dev化，成为真正的Nerd，要么是彻底Ops化，成为售前工程师。</p><blockquote><p>那么，售前工程师又算是什么呢？</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>架构师</tag>
      
      <tag>软件开发</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>⛰如何成为一名合格的架构师</title>
    <link href="/2024/04/19/2024-04-20-%E5%A6%82%E4%BD%95%E6%88%90%E4%B8%BA%E4%B8%80%E5%90%8D%E5%90%88%E6%A0%BC%E7%9A%84%E6%9E%B6%E6%9E%84%E5%B8%88/"/>
    <url>/2024/04/19/2024-04-20-%E5%A6%82%E4%BD%95%E6%88%90%E4%B8%BA%E4%B8%80%E5%90%8D%E5%90%88%E6%A0%BC%E7%9A%84%E6%9E%B6%E6%9E%84%E5%B8%88/</url>
    
    <content type="html"><![CDATA[<p>阴差阳错之下，我在两个月以前成为了一家 AIGC 初创企业的架构师，而我的练习时长其实不过两年半。在这两个多月的实践当中，我发现之前作为 IC 的一些实践对我现在的工作非常有帮助，因此撰写此文，一做总结好给以后反思，二也可以在后辈有需要的时候供其参考。</p><p>行文松散，踩着香蕉皮想到哪写到哪，还请见谅。</p><h2 id="大厂中层与初创架构师的区别"><a href="#大厂中层与初创架构师的区别" class="headerlink" title="大厂中层与初创架构师的区别"></a>大厂中层与初创架构师的区别</h2><p>虽然没有当过大厂中层，但是见证过大厂中层都在干什么的我，在这里可以给两边的工作重点进行一下对比，以消除没有相关经验的人的误解，同时为后面讨论架构师的基本素质做铺垫。</p><p>大厂中层在产品成型之后，一般做的是拓展原有功能的工作，对于原有架构，一是由于非自己亲生，理解不一定到位，二是由于无过即有功的思想，也不会贸然更改。于是他们的工作重心不在于推进架构的改善。同时，由于大厂基建、制度相对完善，通过企业级的 PUA 实践，一方面限制了下面员工的才华充分发挥（上限），也保证了资质不好的混子不至于把项目拖垮（下限）。因此，中层们需要做的就是在 Kick off 前管理上级预期，拿到更多功能，以更少的成本（人力）开发，在当下开发出更多的功能，以此换得更好的绩效、年终，从而推进自己和上级的晋升。从这个角度看，大厂中层没有任何义务对架构进行重构等提升“下次”开发效率的投入，因为“下次”不一定还是自己，而“这次”付出的成本是实际反应在自己的绩效上的。</p><p>初创架构师的工作则更像是所谓的架构师，由于初创公司没有历史包袱的特性，初创架构师可以针对当前的业务和团队的技术栈，投入全部心血打造一个足够好又不至于太昂贵的架构。同时，作为中坚力量，不会像大厂中层一样为了当下的利益而不顾开发效率的降低。因此初创架构师将投入精力在亲自编写代码、改进架构、代码评审、效率工具开发等方面，以提升整个产品的各项性能和可维护性作为自己的追求。</p><blockquote><p>实际上，BAT 初期也是像初创一样的，有一批中坚力量的架构师既可以 coding，也可以做需求沟通，出现重大问题时穿梭全场力破千军。</p></blockquote><p>这就导致了一个最直观的对比：大厂中层在几年之后武功尽废，只会职场斗争，再也离不开大厂；而初创架构师可以连续创业，或投身开源事业，成为一代宗师。这也是为什么大厂在新项目的时候会成立&#x2F;收购新的公司，并将其独立于集团的组织架构发展，因为大厂的这一套大公司系统并不适合创新。</p><h2 id="初创架构师所需要具有的素质"><a href="#初创架构师所需要具有的素质" class="headerlink" title="初创架构师所需要具有的素质"></a>初创架构师所需要具有的素质</h2><p>根据我自己的实践和阅读，总结了初创架构师比较重要的能力</p><h3 id="架构设计与技术选型能力"><a href="#架构设计与技术选型能力" class="headerlink" title="架构设计与技术选型能力"></a>架构设计与技术选型能力</h3><p>准确地说，是从零到一，从一到万的架构设计能力</p><ul><li>架构设计能力：分层、依赖反转、领域模型</li><li>技术选型能力：最重要的是延迟选择，从而获得最大的架构灵活度</li><li>评估预算：容易忽略的是运维成本和升级成本，需要把整个生命周期考虑进去</li></ul><h3 id="Coding-能力"><a href="#Coding-能力" class="headerlink" title="Coding 能力"></a>Coding 能力</h3><p>初创架构师需要身先士卒，以下三点是必须满足的。对于手下的IC来说，按照这三点培养也是必须得。</p><ul><li>精通语言</li><li>可维护的代码</li><li>解决复杂业务</li></ul><h3 id="沟通能力"><a href="#沟通能力" class="headerlink" title="沟通能力"></a>沟通能力</h3><p>这一方面主要是上下游沟通，架构师可能不是产品经理、也不是某个具体功能的研发工程师，但是需要同时理解他们二者的诉求和困难，以此使得团队可沟通、让事情丝滑地推进。</p><ul><li>需求理解与沟通</li><li>任务分配</li></ul><h3 id="Ops-能力"><a href="#Ops-能力" class="headerlink" title="Ops 能力"></a>Ops 能力</h3><p>架构师需要懂得一些运维知识才能在重大问题时更快地进行解决，同时不增加专职运维的工作负担。甚至初期可以不需要专职运维加入团队。</p><ul><li>容器编排</li><li>云厂商</li><li>可观测行</li></ul><h3 id="团队建设能力"><a href="#团队建设能力" class="headerlink" title="团队建设能力"></a>团队建设能力</h3><p>初创团队往往没有专职的效率工具开发人员，为了确保研发人员的开发效率不会因为代码量的增加和架构的复杂化而降低，架构师需要承担起这一部分职责</p><ul><li>团队效率建设：包括DevOps各种工具（CI&#x2F;CD）建设，单元测试建设等等</li><li>知识传递：包括wiki、code review、tech meeting</li></ul><h2 id="架构师与CTO的区别"><a href="#架构师与CTO的区别" class="headerlink" title="架构师与CTO的区别"></a>架构师与CTO的区别</h2><p>其实架构师是一个技术上的划分，而CTO是公司治理上的划分，二者角度不同，所以不具可比性，只能说异同点。而二者的区别主要体现在沟通上，CTO需要与CEO和CFO沟通业务和预算情况，而架构师一般只需要和产品经理、项目经理以及研发人员沟通。通常在大一点的企业，架构师是类似VP担任的，而所有架构师向CTO汇报，以此帮助CTO了解整个企业的技术情况。在小公司中，可以一名架构师一个CTO，也可以一名架构师，CEO兼任CTO，主要看每个人的背景和能力，做出当下最好的选择即可。</p><h2 id="推荐读物（持续更新）"><a href="#推荐读物（持续更新）" class="headerlink" title="推荐读物（持续更新）"></a>推荐读物（持续更新）</h2><ul><li>架构入门：<a href="https://icyfenix.cn/">《凤凰架构》</a>、领域驱动设计</li><li>Coding：《代码整洁之道》 《代码精进之路》 《设计模式》</li><li>语言：《流畅的 Python》</li><li>DevOps： 《凤凰项目》、《DevOps 实践指南》、可观测工程</li></ul><p>还有一大堆杂书、各种CS理论的入门教材（比如密码学、云计算、计算机网络等）</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>架构师</tag>
      
      <tag>软件开发</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🧨春节快乐，聊聊2024展望</title>
    <link href="/2024/02/08/2024-02-09-%E6%98%A5%E8%8A%82%E5%BF%AB%E4%B9%90/"/>
    <url>/2024/02/08/2024-02-09-%E6%98%A5%E8%8A%82%E5%BF%AB%E4%B9%90/</url>
    
    <content type="html"><![CDATA[<p>今天是农历2023年除夕（甲辰年 【兔年】 丙寅月 癸卯日），辞旧迎新之际，聊聊对2024年的展望。</p><p>对于个人而言，2024年应当是风云诡谲的一年，从年初股市的剧烈动荡来看，中国在金融市场上仍然处于较为劣势的状态。随着制造业的转型越来越深化、国内楼市着陆，下一步就是解决货币问题。就我个人而言，祖国这一条制造业升级的路当是正确的。但不可否认的，互联网&#x2F;IT尖端科技仍然属于美元体系，中国的基层治理、各类官员能否满足制造业升级的要求，随着制造业升级升级我们的治理能力，也是一个问题。也是我之前提到的：紫禁城里想着偷瓷器的太监太多，而窃国大盗太少的问题。</p><p>说到这里，似乎金融开放是接下来的策略，但我不清楚金融开放之后，我们对金融治理的能力能否跟上？民间的各种金融殖民者，卖国者的破坏活动是否会被正确治理？而对于我个人而言，如何处置我的资产，以及如何将我在全球化或美元环流中赚取的收益应用于我的消费生活中，也是我需要持续关注的问题。</p><p>或许对我而言，对国家金融殖民主义开战的时间与我个人追求上升的时期不匹配，我无法直接地获得战胜后的收益，所以对于2024年而言，我仍然相信我应该不惜一切地努力实现个人进步，再谈论如何帮助国家在我擅长的领域有所建树。</p><p>祝大家2024年龙年新年快乐！志存高远，继续前进！</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
      <tag>产业观察</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>❗为什么我要逃离网安行业</title>
    <link href="/2024/01/12/2024-01-13-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E8%A6%81%E9%80%83%E7%A6%BB%E7%BD%91%E5%AE%89%E8%A1%8C%E4%B8%9A/"/>
    <url>/2024/01/12/2024-01-13-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E8%A6%81%E9%80%83%E7%A6%BB%E7%BD%91%E5%AE%89%E8%A1%8C%E4%B8%9A/</url>
    
    <content type="html"><![CDATA[<blockquote><p>窃钩者诛，窃国者诸侯。而如今最大的问题在于，紫禁城里偷古玩的太监太多，而真正想成为一方诸侯的人太少，就算有，也会成为紫禁城里太监粉饰太平的工具。</p></blockquote><h2 id="想象中的网络安全公司是如何服务的"><a href="#想象中的网络安全公司是如何服务的" class="headerlink" title="想象中的网络安全公司是如何服务的"></a>想象中的网络安全公司是如何服务的</h2><p>在2020年网络安全高潮中，网络安全服务通常描绘了以下的场景：</p><p>一家传统行业的公司，因为其自身信息化转型要求，进行了一系列信息化建设。在此过程中，发现了许多信息安全问题和信息安全合规要求。在自身安全诉求和合规诉求下，企业雇佣了一个有经验的信息安全总管，对企业信息化的信息安全需求进行满足，这就是想象中的甲方。</p><p>这位信息安全总管，负责梳理自身业务需求，与各大网络安全厂商（即乙方）讨论、采购、实施对应的网络安全方案。乙方通过售卖设备、派驻人手、提供整体网络安全防护方案赚取利润。同时，将赚取到的收益投入到下一代网络安全产品建设、雇佣更好的研究人员进行漏洞挖掘工作、建设更完善的网络安全预警和方案中。在资本看来，其目标市场是所有信息化的企业，而所有企业都将信息化，也就是说，所有企业都已经是或者在信息化的进程中终将成为网络安全行业的客户。</p><h2 id="现实国内的网络安全公司在做什么"><a href="#现实国内的网络安全公司在做什么" class="headerlink" title="现实国内的网络安全公司在做什么"></a>现实国内的网络安全公司在做什么</h2><p>现实中，国内的网络安全公司并没有如想象中一样，与业务紧密结合，为用户提供价值。表面上，有这么几个问题：</p><ol><li>不存在的经验丰富的甲方主管：很多时候，甲方主管并不懂安全，或者甲方安全主管是业务出身，其诉求是“目前的业务系统不能断，最好不改，合规即可”，于是就不算在根据业务+安全互相成就的配置，而是安全全方面适配业务的情况，造成安全产品极大的定制化开发成本和需求。</li><li>从业者水平参差不齐：国内网安从业者水平参差不齐，没有动力、没有动机、没有能力为客户提供想象中的网络安全服务。售前人员通常是普通院校出身，没有接受过系统的网络安全教育，甚至没有接受过系统的计算机相关教育，仅通过企业的售前课程培养。销售的销售能力主要体现在喝酒上，最后是靠刷脸、人情拿下订单。售后和驻场服务人员亦是如此，在没有技术、没有方案支撑的情况下，他们能做的就是在每年“护网”演习的时候锻炼自己拔网线的手速。</li></ol><p>现实是，国内网络安全公司在卖盒子的路上越走越远，产品越简单越好，一句话就能让客户懂；产品价格越低越好，因为安全完全脱离了业务，完全不带来价值，完全是成本支出。人员越便宜越好，最好都是实习生，对客户的服务越糊弄越好，因为产品和方案都只是盒子，不需要也不可能让专家有发挥的空间。</p><p>本质上，是国内甲方和乙方达成了一种粉饰太平的默契，人人处于一种不现实的、但是合规的、让领导满意的应付检查的状态之中。暖风熏得游人醉，形成了一个个臃肿的利益集团，他们在过去数年瓜分了“护网”、“等保”建设的主要经费，为自己上市招揽了一批又一批的员工，将他们的血肉投入自己的获利机器中，然后在寒冬到来时将他们吃干抹尽，留下了一百台盒子到一万台盒子的属于网络安全的“蓬勃发展”。</p><h2 id="何以至此，未来何在？"><a href="#何以至此，未来何在？" class="headerlink" title="何以至此，未来何在？"></a>何以至此，未来何在？</h2><p>我曾经以为，数据流通等新兴方向，由于其内生的安全需求，作为安全公司将是更有能力将这些事情做好的。因为一家数据治理的公司，并不能打消人们在数据交换过程中对数据泄露、数据安全的担忧，但一家拥有整体设计能力的网络安全公司，却可以在数据治理的场景下提供具有安全属性的相关产品，这是传统数据治理公司所难以学习的。但在我的从业过程中看到，网络安全公司更难向前一步，他们已经沉溺于销售驱动的卖盒子中，他们不愿意在关注安全的新兴领域进行投入。我们看到的是密码学学术组织的创业（华控清交），看到的是金融科技的安全创新（微众联邦学习），看到的是数据治理公司提供数据交换和数据安全服务（Snowflake VW），却唯独看不到网络安全企业在这些新兴方向所作出的耀眼创新。</p><p>如此沉沦的网络安全公司是没有未来的，有些公司想通过新型的技术（如LLM）缩减定制化开发成本，妄图在原有的一亩三分地上创造竞争优势，但实际上他们没有看见，或者看见了、但是没有可能迈开腿追逐水草丰美之地，如果你说这是因为中国软件行业的付费习惯差，不如说是国内网络安全公司落后版本十几二十年的后果、不如说仍然想遵循卖盒子路线的网络安全行业是没有未来的。</p><p>凛冬将至，危机危机，危中有机，或许在这次网络安全寒冬中，在那些巨物轰然倒下，渐渐沉没时，会有新的、改变业态的创新者出现，让我们能够以想象中正确的方式（或许有人喜欢叫它第一性原理），构建一个健康向上的网络安全业态。</p><blockquote><p>为什么不是现在？不是已经出了很多国家政策了吗？<br>国家政策已经发挥了应用的效果：保证最低的网络安全要求。而实际上网络安全行业的发展，就像是游戏行业发展一样，需要经过一个野蛮生长，先破后立的过程，再进行规范。依我看，目前还没有到成熟-&gt;规范的时期，而是刚刚进入行业低谷，需要经过一两轮洗牌之后，才会再有针对成熟产业的更好的规范和政策诞生。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>网络安全</tag>
      
      <tag>产业观察</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🤖数字经济中的AIGC-产业智能化与生成式娱乐</title>
    <link href="/2024/01/09/2024-01-10-AIGC%E4%B8%8E%E6%95%B0%E5%AD%97%E7%BB%8F%E6%B5%8E/"/>
    <url>/2024/01/09/2024-01-10-AIGC%E4%B8%8E%E6%95%B0%E5%AD%97%E7%BB%8F%E6%B5%8E/</url>
    
    <content type="html"><![CDATA[<p>数字经济是一个很大的概念，在AIGC出现之前，我们可能认为用Web3做NFT交易就是数字经济（其实是金融），我们可能以为有了5G网络之后就能做到万物互联，这个万物互联下的一切就是所谓的数字经济（但后来发现连接只是最简单的第一步，而连接后的整合才是真正的难点）。</p><p>今天，我们看到，以AIGC为驱动的技术，或许才是数字经济的飞跃时刻。</p><h2 id="第3-5次革命"><a href="#第3-5次革命" class="headerlink" title="第3.5次革命"></a>第3.5次革命</h2><p>经过2023这一年左右的发展，人们已经清楚地意识到，AIGC不一定是一次替代性地革命，原有产业并不一定会因为AIGC的出现而被整体替代，而是在AIGC的加持下大幅提升劳动效率，降低平均劳动时间，从而创造更多的利润。从应用场景看，有两类应用场景是容易实现、同时能创造巨大价值的。</p><h2 id="产业智能化"><a href="#产业智能化" class="headerlink" title="产业智能化"></a>产业智能化</h2><p>依托于上一次信息化浪潮建立起现代信息化系统的现代产业体系，在过去十几年的实践中已经积累的大量的产业数据，而在AIGC出现之前，如果想要使用这些数据、使其产生价值，企业只能高薪聘请高级的算法工程师，为每个场景定制化算法，同时投入巨大的人力和计算资源在数据的清洗、训练、评估中。大厂在实践中演进出了所谓AIOps工作流，促进了一系列机器学习平台的诞生。而更多的企业则是望洋兴叹，没有足够的资金代表着不可能对数据进行有效利用。</p><p>在AIGC的基础上，以前没有办法使用人工智能的企业终于有机会通过强大的基础模型，在加上少量工程师的帮助下，开发出企业定制化的人工智能应用，这是之前所有人工智能厂商所追求的商业模式，现在正在被AIGC实现。另一个好消息是，AIGC同时、甚至更早地提升了程序员的工作效率，叠加目前的经济情况，很有可能以大厂“向社会输送人才”的方式，向就业市场提供一批有经验的工程师。而那些之前用不起AI的行业、企业，将有机会在大模型费用低、工程师费下降的背景下推进自己的AI发展。</p><p>上述场景中，有一个很重要的前提：AI算力的基建工程从何而来。以前想要使用人工智能技术，就需要企业投入巨资购买或像云服务商租用GPU算力进行训练。而大模型时代，企业用户在不进行私有化部署的情况下，可以不租用大量算力，仅通过prompt工程或者其他定制化方式对大模型进行定制化，而这将考验大模型厂商的算力建设能力。<strong>为几千万家企业提供大模型服务需要多少算力？</strong>同时也将考验国家对信息网络的基础设施建设。从中国的网络+道路+厂区建设发展出淘宝等电商平台的过往，以及目前对算力中心、东数西算的建设来看，或许美国能够抢得研究的先手，但更大的发挥空间或许在中国。</p><h2 id="生成式娱乐"><a href="#生成式娱乐" class="headerlink" title="生成式娱乐"></a>生成式娱乐</h2><p>AIGC对游戏制作行业带来了前所未有的变革，Nvidia发布4090Super并将AI生成NPC对话作为其杀手级应用进行宣传，让我们看到了资本市场对生成式娱乐的买单。实际上，想象一个由AI生成、定制化的娱乐，就如同阿西莫夫短篇《Dreaming is a private thing》中人们从高端的、有艺术色彩的大师梦，到大众化的、快消梦一样，最后人们将享受到定制化的、具有个人特色的生成梦。届时，上一个时代的短视频注意力经济，将被生成式娱乐所取代，而把握生成式娱乐技术的公司，不管是入口（VR&#x2F;AR）、算力、算法，都有可能成为最大的获胜者。</p><p>或许有人会宣称这是对人类创造力的亵渎，但抖音短视频，“小帅和小美”的五分钟电影剪辑、营销号搬运号的所作所为，难道不是对人类创造力的亵渎吗？如果是，那有如何解释后现代性在解构这些奶头乐行为时所展现出的人类的思考与创造呢？因此，我个人认为是有些人将对自己想象力贫瘠以及媒体渲染出的恐惧、灌输的狗屁不通的道理当作了时自己的观点，似乎听起来很有道理，实际上自欺欺人。</p><h2 id="如何选择"><a href="#如何选择" class="headerlink" title="如何选择"></a>如何选择</h2><p>对于个人而言，如果是参与创新性的模型研究、从零到一的爆款打造，最好的选择或许还是硅谷或者海外初创企业。我们可以看到ChatGPT、Midjourney在推出时有的一系列非常不符合监管的表现，在大陆是不可能被开放的。而如果是参与算力网建设、参与国内的模型工程化落地，还是有很大的前景的。</p><p>从产业的角度讲，Nvidia的硬件很好，但是其最大的倾销对象是被制裁的大陆AI企业和算力中心，其次才是欧美的大厂和算力中心，最后才是臭打游戏的。从目前PyTorch对其他加速器的后端支持来看，我相信CUDA在神经网络的垄断情况将逐渐衰弱，Nvidia虽不至于速败，但像之前的一飞冲天不太可能再来一次了。另一方面，智能车机是另一个对算力中心有依赖的趋势，而目前中国的新能源车发展是最好的，Nvidia+特斯拉 vs 比亚迪+华为，同时华为也有自己合作的车厂，我认为这一组合下，中国的长板在于制造和基建，而芯片正在追赶，美国的长板在于芯片，而制造和基建没有办法追赶。此消彼长下，时间会告诉我们答案。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数字经济</tag>
      
      <tag>产业观察</tag>
      
      <tag>AIGC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>✍数字时代的生产关系</title>
    <link href="/2024/01/08/2024-01-09-%E6%95%B0%E5%AD%97%E6%97%B6%E4%BB%A3%E7%9A%84%E7%94%9F%E4%BA%A7%E5%85%B3%E7%B3%BB/"/>
    <url>/2024/01/08/2024-01-09-%E6%95%B0%E5%AD%97%E6%97%B6%E4%BB%A3%E7%9A%84%E7%94%9F%E4%BA%A7%E5%85%B3%E7%B3%BB/</url>
    
    <content type="html"><![CDATA[<blockquote><p>当你“免费”地刷抖音、刷淘宝时，竟在免费地为互联网大厂积累数据，从而为它们的盈利贡献你自己的一份力量。</p></blockquote><p>难道使用我的行为所生产的数据不需要付费吗？</p><h2 id="剩余价值理论在数据资产上的体现"><a href="#剩余价值理论在数据资产上的体现" class="headerlink" title="剩余价值理论在数据资产上的体现"></a>剩余价值理论在数据资产上的体现</h2><p>平台收集了大量数据，形成了事实上的竞争优势。对于平台而言，数据是一种可以强化其垄断地位的战略性资产，因为平台在一个领域收集了大量数据之后，在进入另一个新的领域时，可以利用这些数据作为战略性投入，获得竞争优势，从而在新领域占据垄断地位。欧盟委员会指出，数据能够强化平台的间接网络效应，导致企业越来越依赖在线平台，而平台则成为市场和消费者的“看门人”，能够将数据垄断优势更好地发挥出来。</p><p><strong>在用户免费地为平台劳动，生产数据时，平台通过大数据分析等技术，从其中提取剩余价值，同时利用数据提高产品或服务的质量，并能够成为市场进入的壁垒</strong></p><h2 id="数据垄断下的获利方式"><a href="#数据垄断下的获利方式" class="headerlink" title="数据垄断下的获利方式"></a>数据垄断下的获利方式</h2><p>电商平台、网购平台、网约车平台等等一系列消费者可以直接消费的平台，可以通过所谓精准定价策略对消费者进行个性化定价，或者通俗一点说：“大数据杀熟”。</p><p>其他掌握了用户“注意力”的信息流类型平台，比如以前的贴吧、现在的抖音等等，可以通过投放广告的搜广推业务模式进行获利，这听起来人畜无害，但实际上魏则西案告诉我们，平台垄断信息流之后寻租获利将导致“恶”的无序放大。</p><p>最后则是直接经济获利以外的其他获利形式，如意识形态的传播、舆论把控以及选举操纵等等手段，通过持有并分析海量数据的方式对现实世界形成潜移默化的影响，从而达到利益集团的目的。</p><h2 id="数据反垄断"><a href="#数据反垄断" class="headerlink" title="数据反垄断"></a>数据反垄断</h2><p>世界各国意识到数据垄断对经济、创新、乃至于包括国家安全等方面的威胁之后，也开始了对平台的数据垄断问题的监管。但实际上，欧盟对谷歌、Facebook等美国公司的监管，更像是向它们收取保护费，而欧盟自己并没有可以与之竞争的互联网公司，欧盟民众也无法离开这些美国企业提供的服务。</p><p>中国由于有网络长城和之前一系列互联网管制措施，加上信息化基础建设和大量人才培养，确实成长出了一批互联网企业，成为可以和美国互联网垄断巨头打擂台的唯一力量。但与此同时，也是因为相关人才的缺失和体制上的原因，不得不实行“官本位”的管理手段，导致中国互联网企业的垄断、竞劣更甚，缺少类似于硅谷的创新中心，这一点在AI技术的发展上可以明显看出。</p><p>人们可能寄希望于Web3等去中心化技术来完成对信息垄断的超越，但实际上基于“虚拟货币”的任何产品和概念，其金融属性才是其主要内涵，而非通过去中心化技术改变生产关系。比如我曾经介绍过的MDT（Measurable data token），想要通过虚拟货币的方式让用户可以自行售卖个人信息以获利，如果他们真的相信可以达成，那么这表达了一种天真的幻想，幻想通过某种去中心化的技术，就可以在现行体制下完成垄断下的数据经济重分配。更有可能的是，没有人相信这件事，这只是又一个炒币的手段罢了。</p><h2 id="普通人能做什么"><a href="#普通人能做什么" class="headerlink" title="普通人能做什么"></a>普通人能做什么</h2><p>迈克尔·赫德森在痛批美金融体系全球吸血的同时，告诉年轻人不要以从事金融工作为耻。只有当你足够了解其中的原理与运行逻辑，你才有办法分析其漏洞，从而尝试打破这一不公平的制度。我想，在数据垄断这一问题上也是如此。只不过数字垄断不像是其他垄断行为一样，比较轻易地就能看到其吸血的本质，而展现在台面上的，是大厂核心组表面上的光鲜亮丽，高额的报酬使其成为了所有人趋之若鹜的选择。这和金融业的情况何其相似，他们总是通过自己手中的金钱和权力向普罗大众宣扬自己高人一等，从而吸收世界的人才以维系自己的霸权。很多时候，大厂能雇得起这么多人，并不是因为有这么多的重要业务，而仅仅是因为他们可以雇得起这么多人。</p><blockquote><p>一个人从事的工作按优先级排序为三高，高价值、高回报、高效率。高价值并不一定是金钱上的奖励有多么高，而是声誉、金钱、自我实现等等加起来的价值。高回报则是有比较高的劳动杠杆率，单位时间的劳动产出更高，比如大部分智力劳动会比简单的体力劳动有更高的回报率。而高效率则是最容易，同时也是收益率最低的方式，即提升自己在劳动上的效率，比如网络写手更快地写文章等等。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据产业</tag>
      
      <tag>数字经济</tag>
      
      <tag>产业观察</tag>
      
      <tag>资本论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>✍给自己的找工作手记</title>
    <link href="/2023/12/11/2023-12-12-%E7%BB%99%E8%87%AA%E5%B7%B1%E7%9A%84%E6%89%BE%E5%B7%A5%E4%BD%9C%E6%89%8B%E8%AE%B0/"/>
    <url>/2023/12/11/2023-12-12-%E7%BB%99%E8%87%AA%E5%B7%B1%E7%9A%84%E6%89%BE%E5%B7%A5%E4%BD%9C%E6%89%8B%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="人生目标与计划"><a href="#人生目标与计划" class="headerlink" title="人生目标与计划"></a>人生目标与计划</h2><h3 id="5-年目标"><a href="#5-年目标" class="headerlink" title="5 年目标"></a>5 年目标</h3><p>最好的IC -&gt; 架构师 -&gt; CTO：</p><ul><li>专业能力：代码、架构、学习新事物的能力</li><li>业务能力：通晓领域内的业务模型</li><li>沟通能力：带领小型团队，在跨团队间沟通</li><li>商业能力：从技术创造价值</li></ul><h3 id="当前诉求"><a href="#当前诉求" class="headerlink" title="当前诉求"></a>当前诉求</h3><ul><li>Go bigger：做更大的业务，更先进更聚焦的技术</li><li>更大的平台，或者对我有更大的投入</li></ul><h2 id="找一份什么样的工作"><a href="#找一份什么样的工作" class="headerlink" title="找一份什么样的工作"></a>找一份什么样的工作</h2><h3 id="工资待遇"><a href="#工资待遇" class="headerlink" title="工资待遇"></a>工资待遇</h3><ul><li>当前薪资上浮</li><li>工作时间加权</li><li>工作强度加权</li><li>绩效、期权浮动总包</li><li>其他福利待遇</li></ul><h3 id="行业前景"><a href="#行业前景" class="headerlink" title="行业前景"></a>行业前景</h3><ul><li>未来发展<ul><li>为什么这个未来和这个公司是可以匹配的</li><li>这个未来还有哪些公司可以跳</li></ul></li><li>政策背景</li></ul><h3 id="什么能让人感到开心-满足"><a href="#什么能让人感到开心-满足" class="headerlink" title="什么能让人感到开心&#x2F;满足"></a>什么能让人感到开心&#x2F;满足</h3><ul><li>明确目标、未来可期</li><li>有一定自由度</li><li>适当的休息、余闲</li><li>认可度</li></ul><h3 id="什么让人沮丧"><a href="#什么让人沮丧" class="headerlink" title="什么让人沮丧"></a>什么让人沮丧</h3><ul><li>不清晰的目标</li><li>恼人的同事关系（甩锅、内部斗争、外部斗争）</li><li>过大的工作压力</li><li>违背承诺：如休假、涨薪等</li></ul><h2 id="远程工作特辑"><a href="#远程工作特辑" class="headerlink" title="远程工作特辑"></a>远程工作特辑</h2><p>远程工作自疫情以来大火，但在资本主义的背景下，远程工作不一定代表着 WLB，也有可能是大公司拿捏员工、降薪员工的一种方式</p><h3 id="发展方面"><a href="#发展方面" class="headerlink" title="发展方面"></a>发展方面</h3><ul><li>主营业务，商业模式，产品形态</li><li>如何让我相信公司是有前景的</li><li>数据行业：数据从何来，客户从何来</li></ul><h3 id="人事方面"><a href="#人事方面" class="headerlink" title="人事方面"></a>人事方面</h3><ul><li>人民币还是美刀</li><li>与谁签合同，到期、续签、N+1，名义雇主可信度</li><li>税怎么缴纳，五险一金缴纳基数、地点</li></ul><h3 id="薪资待遇"><a href="#薪资待遇" class="headerlink" title="薪资待遇"></a>薪资待遇</h3><ul><li>工资 base 多少，现金与股权——能接受多少的远程降薪</li><li>中国法定节假日怎么放</li><li>晋升机制、期权奖励</li></ul><h3 id="生活质量"><a href="#生活质量" class="headerlink" title="生活质量"></a>生活质量</h3><ul><li>在何处生活</li><li>需要关注每日工作时间，区分工作地点和生活地点</li><li>兴趣爱好培养</li><li>关注健康方面</li></ul>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
      <tag>求职</tag>
      
      <tag>手记</tag>
      
      <tag>持续更新</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🤔黄粱一梦：《完蛋！我被美女包围了！》</title>
    <link href="/2023/11/01/2023-11-02-%E5%AE%8C%E8%9B%8B%E6%88%91%E8%A2%AB%E7%BE%8E%E5%A5%B3%E5%8C%85%E5%9B%B4%E4%BA%86/"/>
    <url>/2023/11/01/2023-11-02-%E5%AE%8C%E8%9B%8B%E6%88%91%E8%A2%AB%E7%BE%8E%E5%A5%B3%E5%8C%85%E5%9B%B4%E4%BA%86/</url>
    
    <content type="html"><![CDATA[<blockquote><p>本文涉嫌剧透，但个人认为剧透并不怎么影响游戏体验，毕竟我也是看了一大堆剧透之后才开始玩的<br>本文可能涉及一部分过度解读，纯属本人脑子一热，异想天开</p></blockquote><p><img src="/../img/2023-11-02-%E5%AE%8C%E8%9B%8B%E6%88%91%E8%A2%AB%E7%BE%8E%E5%A5%B3%E5%8C%85%E5%9B%B4%E4%BA%86/jda-steam.png" alt="一周目肖鹿，无法辜负的海芋花少女"></p><p>《完蛋！》讲述的就是一个众美女环绕，争着抢着和男主谈恋爱的故事。而第一人称扮演的主角顾易，只不过是一个远走他乡、身负债款且工作屡屡碰壁的“失败者”。光这句话本身就已经很科幻了，开发团队也将游戏称为“大型科幻爱情片”。</p><p>据蓝鲸财经报道，资深游戏玩家Bruce是最早体验该游戏的玩家之一，他认为《完蛋》本质是男性向的乙女游戏，Bruce告诉记者：它的游戏性并不好，人物不够立体、情节也经不起推敲，“玩完以后，我甚至不知道作为男主的‘我’顾易，他的魅力在哪里。”</p><blockquote><p>以上内容摘自：每日经济新闻： <a href="https://www.stcn.com/article/detail/1020568.html">https://www.stcn.com/article/detail/1020568.html</a></p></blockquote><p>我完全认同包括Bruce在内的大部分网友的评价，从现实角度出发，这就是一个魅男的、自恋的、片面化的男性向乙女游戏，就如同日剧男主只会亚萨西一样让人摇头。但我在男主躺在兄弟老六的床上的时候，忽然发现，或许这是一位普通工友的白日做梦。从这个角度出发，似乎一切都合理了起来。</p><blockquote><p>以下开始为本人的过度解读环节</p></blockquote><h2 id="重新叙述这个故事"><a href="#重新叙述这个故事" class="headerlink" title="重新叙述这个故事"></a>重新叙述这个故事</h2><p>男主是老六的舍友，没上过大学，中专毕业后就来到城里工作，工作内容无非是画展小时工、保安、服务员。那天，他躺在床上，昏昏欲睡，做了一个梦。</p><p>梦里的场景是他之前当小时工的画展现场，在梦中他和两个在画展中亮眼的妹子产生了互动：郑梓妍和李云思，梦中的自己和郑搭讪去喝酒，和李赏画表现出自己的鉴赏能力，又在喝醉了之后打电话给李。接着开始了一系列故事。在梦中的场景里，他变成了一个受过艺术教育，成长环境较好，有绘画天赋的人，但是因为自己生意失败而负债累累。</p><p>从这一视角，剧中的荒唐之处从粗糙的逻辑漏洞变为了若即若离的设计：</p><ul><li>现实中，郑梓妍和李云思两人或许根本没有联系，因为在后续的剧情里也没有看出来他们俩之间有很熟悉的关系</li><li>男主根本不懂画，他只是一个受过中专教育（或者义务教育）的底层打工人，他对艺术和艺术家的理解在于那些风流故事，就和抖音上的五分钟了解xxx，你真的了解xxxx一样离谱。李云思会欣赏男主说的东西，本质上是男主梦里的自恋<ul><li>看到李云思的未完画作时却没有相关评价，反而开始给人家按摩，李云思反而更喜欢他了</li><li>男主在包下店之后无法开张、糊弄记者、自我放弃，从正面视角来看是不能理解的自暴自弃，没什么好怜悯的，但从梦的视角看，男主根本没有相关体验，如果强行带入，则其中的不自然会让男主很快醒来</li><li>从男主作画之类的剧情，包括和肖鹿一起画画也可以看出他对这类创作没有感觉</li><li>男主若真有天赋，真的会一些作画，应该有李云思线，走三色绘恋男主邱诚的路子（这就是另一个烂故事了）</li><li>李云思或许才是那个陪她看展的人，男主在现实里一直都是一个保安，只是想象这一段故事</li></ul></li><li>男主的生意经大概率是道听途说，其可以训斥李总不懂做生意，摆出一道自己很懂的样子，做销售又直接业绩垫底抬走<ul><li>要么这是男主眼高手低，要么就是这一切都来自于零碎信息的拼凑，好像“当老板很简单”的成功学，一转眼又是“龙王赘婿，女总裁倒贴司机”的爽文</li></ul></li><li>男主可能在酒吧当过服务员，看过很多男男女女喝的烂醉然后被其他光鲜亮丽的人带走，将这一剧情代入到了自己</li><li>男主可能有的现实体验是和当年暗恋的对象翻栅栏，这一场景投射到了肖鹿身上</li><li>男主可能没有过比较高级的恋爱体验，这也是为什么没有和李云思结合的结局的原因，李云思应当是脱离了其接触的低级趣味的无法想象对象，最多是隐藏结局中的“逃婚”，最终也是落到了郑梓妍类似的浪漫剧情，在这里郑梓妍和李云思成为了一类人。</li><li>父亲的缺位，似乎只有青梅竹马沈彗星的父亲与男主的父亲有联系，但是到底发生了什么，是大幅度的留白</li><li>男主可能真的有一个沈彗星，只不过自己才是那个沈彗星，而沈彗星是自己求而不得的对象，如果男主选择青梅竹马线，实际上是对自己渴望的投射</li><li>单亲妈妈倒贴纯属爽文剧情，好像是“为了孩子”，但是实际上是“为了屏幕前的观众”（或者为了做梦的人）爽一下的春梦剧情</li></ul><h2 id="本我的映射"><a href="#本我的映射" class="headerlink" title="本我的映射"></a>本我的映射</h2><blockquote><p>借用弗洛伊德之观点，不代表我认为其机械唯物主义是正确的<br>搜到一篇分析龙王爽文的，有类似：<a href="https://zhuanlan.zhihu.com/p/181426773">https://zhuanlan.zhihu.com/p/181426773</a></p></blockquote><ul><li><strong>代表浪漫，性感，放浪形骸的郑梓妍</strong>：男主并不知道那些酒吧里的富姑娘的生活是怎么样的，但是从那些短视频里，她们喜欢到处玩、到处旅行，她们喜欢酒吧买醉、蹦迪，喜欢尽情玩乐。在梦里给自己创造一个这样少女，迷恋自己，而自己也可以顺势成为平时自己羡慕的，陪着这些姑娘的人。</li><li><strong>代表理性、阶级，高级趣味的李云思</strong>：男主只在那些高级趣味的场所看到过她们，而男主在潜意识里以为自己看到的那些艺术家的风流趣事，耍嘴皮子可以赢得她们的欢心，她们还会无条件地支持自己。本质上是男主的自卑，同时也不理解、不愿理解，最终以“看不起艺术”的方式退出，又幻想自己能够以“一厢情愿”挽回，而自恋的最高峰是“逃婚”的隐藏结局（甚至埋了一个高中就是网友的伏笔）</li><li><strong>代表纯真、无邪、青春的肖鹿</strong>：这或许是男主最无忧无虑的一段时光，也或许是男主不曾有过的时光，在自己十七八岁的年纪，和一个同样青春的女孩感受青春的魅力，畅想以后的生活，用自己的双手规划自己的生活，两个人平平淡淡地走下去。我倾向于这是男主最现实的渴望，但他一个底层工作，又能奢望什么青春靓丽大学生呢？</li><li><strong>代表童年渴望的、无法触及的沈彗星</strong>：沈彗星的倒贴映照出男主在童年的自卑，这或许和其父亲的缺位有关，这个角色也是唯一和其父亲有关联的人。无尽的自卑带来了无尽的幻想，最后童年梦中的那个女孩成了自己的舔狗，自己也通过这种爱慕东山再起，这才补偿了自己的童年阴影。</li><li><strong>代表色欲、自恋的林乐清：色欲剧情</strong>，附带大和抚子属性，大男子主义，无聊</li><li><strong>代表虚荣、物质的钟甄</strong>：爽文剧情，资本主义软饭，无聊</li></ul>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
      <tag>精神分析</tag>
      
      <tag>galgame</tag>
      
      <tag>游戏</tag>
      
      <tag>弗洛伊德</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🚀逆转思路：从数据可计量到数据可控</title>
    <link href="/2023/10/30/2023-10-31-%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E4%BD%BF%E7%94%A8%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/"/>
    <url>/2023/10/30/2023-10-31-%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E4%BD%BF%E7%94%A8%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/</url>
    
    <content type="html"><![CDATA[<blockquote><p>数据可控可计量是目的，数据可用不可见是其中的一种技术手段。目前，数据可用不可见的技术不能完全满足数据可控可计量的目的。 —— 《数据要素化100问:可控可计量与流通交易》</p></blockquote><h2 id="1-数据使用可控可计量是什么"><a href="#1-数据使用可控可计量是什么" class="headerlink" title="1. 数据使用可控可计量是什么"></a>1. 数据使用可控可计量是什么</h2><p>数据“使用可控可计量”是指管控数据流通使用的具体目的、方式和次数，保障数据流通使用安全、合法、合规，使数据流通可控与可监管。“可控”主要是管控数据的具体使用目的、方法和次数，避免数据流通使用风险；“可计量”要求数据使用可以量化，使数据要素按贡献参与分配具有可操作性。同时，可计量为监管方发现数据滥用行为、持有方履行数据安全责任义务、使用方自证其清白、可能的受害方举证其收到的伤害和损失创造了条件。</p><h2 id="2-数据可用不可见的技术手段"><a href="#2-数据可用不可见的技术手段" class="headerlink" title="2. 数据可用不可见的技术手段"></a>2. 数据可用不可见的技术手段</h2><p>数据可用不可见，主要使用到隐私保护计算技术（Privacy-Preserving Computation Technologies，简称隐私计算技术，国外也有隐私增强技术 - Privacy Enhancing Technologies的类似说法）。其主要特点在于，<strong>数据流通的不再是明文数据本身，而是其计算价值</strong>。多方数据在流通的过程中，每方都不暴露自己的明文数据，只把计算结果给到需求方。</p><p>这类技术主要包括安全多方计算（Secure Multi-Party Computation，SMPC）、同态加密（Homomorphic Encryption，HE）、可信执行环境（Trusted Execution Environment，TEE）、联邦学习（Federated Learning，FL）、差分隐私（Differential Privacy，DP）、数据脱敏（Data De-Identification）、仿真数据（Synthetic Data）等。</p><p>隐私计算通常被冠以能够实现“数据可用不可见”的称号，解决数据流通信息暴露的难题。同时，随着现代密码学和现代计算机技术的不断发展，隐私计算技术也会结合智能合约和区块链等技术，有效控制数据的使用目的、方式和次数，从而解决数据流通中的“不可控”第二大难题。智能合约技术是用计算机语言取代了法律语言记录条款，并由程序自动化执行的合约，具有数字化、可机读等特点，但无法篡改。从理论上来说，通过智能合约执行的隐私计算相关算法，则可以保证程序不见数据，数据使用可控可计量。但现实是，智能合约还远远无法执行目前数据使用的计算密集型、算力依赖型需求，目前所有的数据使用成果，包括大模型、数据分析等等技术，都需要针对智能合约和隐私计算进行深度改造，甚至有很大部分应用场景还没有相关技术支撑。</p><p>因此，在等待智能合约+隐私计算技术的成熟之前（鉴于技术复杂度，我认为或许永远都不会有那一天），我们需要寻找另外的角度实现数据使用可控可计量。</p><h2 id="3-从可计量的角度出发实现数据使用可控"><a href="#3-从可计量的角度出发实现数据使用可控" class="headerlink" title="3. 从可计量的角度出发实现数据使用可控"></a>3. 从可计量的角度出发实现数据使用可控</h2><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>从目前已有的方案来看，人们往往从数据使用可控出发，例如：针对隐私场景的深度学习训练，人们针对数据本身 vs 深度学习的模型及其参数的关系出发，提出了参数&#x2F;模型动、数据不动的联邦学习技术。再从这种技术的基础上，发展对于数据使用的计量。这样做的缺点十分明显，就是数据使用计量的维度、细粒度等等细节在每个场景下都需要重新设计，而且很难做到通用。反过来，一个没有被广泛应用的计量技术，也无法满足监管方、数据持有方、数据使用方的需求。</p><p>这在某种程度上导致了基于数据可用不可见的技术实现了特定场景下数据使用可控，但数据持有方仍然不敢、不愿支持数据流通的现状——数据使用计量的不完整。</p><h3 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h3><p>分布式可观测性的发展过程给了我们一个新的视角：首先是基于业务的日志及日志收集系统，后来随着分布式服务的发展，人们开发了一系列系统和工具对分布式链路进行追踪，对系统进行指标收集，形成了日志、追踪、指标三大方向；近年来，基于OpenTelemetry这一开放标准，以及eBPF无插桩观测技术，越来越多的无感知的、针对不同方面的可观测系统和软件被开发出来，形成了一个完整的可观测性生态。而基于这一完整的可观测性生态，人们可以从可观测性的角度对态势感知、安全等方面进行颠覆性变革。</p><blockquote><p>可观测性相关发展可以参考我的另一篇博客<a href="https://wh1isper.github.io/2023/10/29/2023-10-30-%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7%E8%A1%8C%E4%B8%9A%E5%8F%91%E5%B1%95%E4%B8%8E%E6%9C%BA%E9%81%87/">可观测性行业发展与机遇</a></p></blockquote><p>从数据使用可控可计量的角度而言，我们目前已有了在部分场景下可以实现数据使用可控的技术，但是缺乏对数据使用可计量的相关研究，导致数据使用可计量成为了数据使用可控技术的附加内容。而从可观测性的角度出发，我们可以将数据使用可计量作为一个独立的技术方向，设计可计量方面的标准，从可计量的角度实现数据使用可控，则有以下优势：</p><ul><li>从可计量的角度出发，可以将数据使用可控的技术方案进行<strong>通用化</strong>，联邦学习、同态加密、差分隐私等等技术方案都可以在可计量的基础协议上进行改造，从而实现数据使用可控。</li><li>不必依赖于智能合约技术，可以使用现有的技术方案</li><li>与数据使用前、后的操作（如数据治理、数据确权）等联动，形成一个完整的数据使用生态</li></ul><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p><img src="/../img/2023-10-31-%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E4%BD%BF%E7%94%A8%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/data-usage-measurable.png" alt="引入数据使用计量中心的基础架构"></p><h2 id="4-一种可能的未来方向"><a href="#4-一种可能的未来方向" class="headerlink" title="4. 一种可能的未来方向"></a>4. 一种可能的未来方向</h2><p>在上一阶段的探索中，已经有类似的整合型、一体化解决方案出现：如数据沙箱、国际数据空间（IDS），但其主要解决的是数据流通的安全问题，而非数据使用的可控可计量问题。未来的技术方向，可能是基于eBPF的无探针观测技术，结合OpenTelemetry框架，提供针对数据使用的追踪类型，实现针对数据使用的可观测性，达到数据使用可计量的目的。数据受控使用将根据对数据使用的计量进行细粒度的控制，从而实现数据使用可控；这并不意味着数据使用可计量的重要性在数据使用可控之上，而是将二者同等看待，协同工作，相互依赖，因为数据使用的计量系统，往往也依赖于数据受控使用环境的支持。</p><h2 id="5-补充：为什么需要数据使用可控可计量"><a href="#5-补充：为什么需要数据使用可控可计量" class="headerlink" title="5. 补充：为什么需要数据使用可控可计量"></a>5. 补充：为什么需要数据使用可控可计量</h2><blockquote><p>我国已经将数据上升为生产要素，其主要原因在于，当今世界数字经济规模逐渐上升，国内数字产业飞速发展。在这一前提下，我们发现互联网平台之间的数据垄断现象开始凸显，逐渐限制了基于数据的数字经济的整体发展。数据对生产效率的乘数效应，主要作用于三个方面：效率倍增、资源优化和投入替代，Chat-GPT为首的大模型就是对简单重复工作的一种通用投入替代。<br>数据要素化的本质是流通，流通是数据进入社会化大生产并成为数据要素的必要条件。通过法律法规，国家想要让数据自由有序地流向应用、流向更多地企业组织、流向不同地行业和低于、流向社会生产和人民美好生活需要的地方。</p></blockquote><p>目前，数据流通的朴素方式是明文数据的复制和传播。由于明文数据几乎没有复制成本，以这种方式流通数据面临许多不可控的因素。这导致了数据持有方“不愿流通”和“不敢流通”。明文数据流通从信息学和经济学的角度看，存在两大核心障碍。</p><ul><li>信息学角度： 数据的价值在于其承载的信息不对称性，即“我知道，你不知道”，但通过复制明文方式传播，则将数据信息完全暴露出去，变成了“我知道，大家都知道”，这一过程就导致了<strong>数据自身价值的灭失</strong>。明文数据流传地越快越广，其价值反而降低得越快。在这一情况下，数据拥有方将<strong>失去对该数据的控制</strong>，无法管控该数据再复制之后得使用目的、方式和次数，也无法厘清双方之间的“责、权、利”。</li><li>经济学角度：数据的无限复制和传播，边际成本几乎为0，从理论上说，就会造成无限的供应和无限的需求，则违背了市场供需定价的稀缺性原理，导致数据价格约等于零，即数据不再稀缺，也就无法进行定价，则数据无法通过市场方式进行大规模流通。</li></ul><p>因此，数据实现流通，有两大核心问题需要解决：</p><ul><li>数据在流通过程中信息泄露的问题： 可通过各类”可用不可见“技术进行解决</li><li>数据流通不可控问题：所谓可控，是对其使用目的、方式和次数进行管控，这是一个更高层次的问题，需要从数据的使用角度出发进行解决。</li></ul><p>数据流通不可控问题对应的即是数据可控可计量，”可计量“要求数据使用可以量化，使得数据要素按贡献参与分配具有可操作性。同时，可计量为监管方发现数据滥用行为、持有方履行数据安全责任义务、使用方自证其清白、可能的受害方举证其收到的伤害和损失创造了条件。</p>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>开源</tag>
      
      <tag>可观测性</tag>
      
      <tag>数据使用</tag>
      
      <tag>DataUCon</tag>
      
      <tag>数据使用可控可计量</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>📕可观测性行业发展与机遇</title>
    <link href="/2023/10/29/2023-10-30-%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7%E8%A1%8C%E4%B8%9A%E5%8F%91%E5%B1%95%E4%B8%8E%E6%9C%BA%E9%81%87/"/>
    <url>/2023/10/29/2023-10-30-%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7%E8%A1%8C%E4%B8%9A%E5%8F%91%E5%B1%95%E4%B8%8E%E6%9C%BA%E9%81%87/</url>
    
    <content type="html"><![CDATA[<h2 id="1-可观测性的由来"><a href="#1-可观测性的由来" class="headerlink" title="1. 可观测性的由来"></a>1. 可观测性的由来</h2><p>随着分布式架构渐成主流，<a href="https://en.wikipedia.org/wiki/Observability">可观测性</a>（Observability）一词也日益频繁地被人提起。最初，它与<a href="https://en.wikipedia.org/wiki/Controllability">可控制性</a>（Controllability）一起，是由匈牙利数学家 Rudolf E. Kálmán 针对线性动态控制系统提出的一组对偶属性，原本的含义是“可以由其外部输出推断其内部状态的程度”。这一说法足够抽象，以致于我们很难理解这个词是如何在计算机相关领域。我采取的策略是：调研目前火热（和蹭热度）的开源项目和商业项目，看看在他们的视角下，可观测性在是什么，接着我们就可以总结其中涉及的关键技术，并谈谈我对可观测性还能怎么蹭的一些未来构想。</p><h2 id="2-不同视角下的可观测性"><a href="#2-不同视角下的可观测性" class="headerlink" title="2. 不同视角下的可观测性"></a>2. 不同视角下的可观测性</h2><h3 id="2-1-分布式追踪（Distributed-Tracing）下的可观测性"><a href="#2-1-分布式追踪（Distributed-Tracing）下的可观测性" class="headerlink" title="2.1 分布式追踪（Distributed Tracing）下的可观测性"></a>2.1 分布式追踪（Distributed Tracing）下的可观测性</h3><h4 id="概念定义"><a href="#概念定义" class="headerlink" title="概念定义"></a>概念定义</h4><p>现代分布式链路追踪公认的起源，是Google 在2010 年发表的论文<a href="https://research.google/pubs/pub36356/">《Dapper : a Large-Scale Distributed Systems Tracing Infrastructure》</a>，为了解决如何理解和推理大规模分布式系统的行为和性能问题，从跟踪工具出发从而建设了一个监控平台。</p><blockquote><p>一个简单的案例是，如果当前一个用户请求需要经过A-&gt;B-&gt;C三个微服务，用户反馈请求超时，我们要如何快速地了解是A&#x2F;B&#x2F;C三个服务中的哪个出现了问题？A&#x2F;B&#x2F;C三个服务的稳定性、错误率如何？一个简单的想法是，如果能把跟踪一个用户请求在A&#x2F;B&#x2F;C三个服务的调用过程，以及每个服务的执行情况，都记录下来，那么我们就可以通过分析这些数据来解决上述问题，这就是分布式追踪的基本思想。</p></blockquote><p>目前，学术界一般会将可观测性分解为三个更具体方向进行研究，分别是：<a href="http://icyfenix.cn/distribution/observability/logging.html">事件日志</a>、<a href="http://icyfenix.cn/distribution/observability/tracing.html">链路追踪</a>和<a href="http://icyfenix.cn/distribution/observability/metrics.html">聚合度量</a>，这三个方向各有侧重，又不是完全独立，它们天然就有重合或者可以结合之处，2017 年的分布式追踪峰会（2017 Distributed Tracing Summit）结束后，Peter Bourgon 撰写了总结文章<a href="https://peter.bourgon.org/blog/2017/02/21/metrics-tracing-and-logging.html">《Metrics, Tracing, and Logging》</a>系统地阐述了这三者的定义、特征，以及它们之间的关系与差异，受到了业界的广泛认可。</p><p><img src="/../img/2023-10-09-%E6%95%B0%E6%8D%AE%E4%BD%BF%E7%94%A8%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/MetricsTracingandLogging.png" alt="日志、追踪与度量"></p><ul><li>日志（Logging）：日志的职责是记录离散事件，通过这些记录事后分析出程序的行为。在大量节点、服务的情况下，这将产生大量的日志，因此，日志的收集、存储、分析都是一个很大的挑战。</li><li>追踪（Tracing）：单体系统时代追踪的范畴基本只局限于<a href="https://en.wikipedia.org/wiki/Stack_trace">栈追踪</a>（Stack Tracing），即断点的形式。微服务时代，追踪就不只局限于调用栈了，一个外部请求需要内部若干服务的联动响应，这时候完整的调用轨迹将跨越多个服务，同时包括服务间的网络传输信息与各个服务内部的调用堆栈信息。与“全链路追踪”、“<a href="https://opentracing.io/docs/overview/what-is-tracing/">分布式追踪</a>”（Distributed Tracing）为同义词。追踪的主要目的是减少排除故障的时间，让程序员可以快速定位问题。</li><li>度量（Metrics）：度量是指对系统中某一类信息的统计聚合。度量的主要目的是监控（Monitoring）和预警（Alert），如某些度量指标达到风险阈值时触发事件，以便自动处理或者提醒管理员介入。</li></ul><h4 id="开源标准"><a href="#开源标准" class="headerlink" title="开源标准"></a>开源标准</h4><p><a href="https://opentelemetry.io/">OpenTelemetry</a>合并了OpenTracing和OpenCensus项目，提供了一组API和库来标准化遥测数据的采集和传输。OpenTelemetry提供了一个安全，厂商中立的工具，这样就可以按照需要将数据发往不同的后端。其最重要的是数据格式<a href="https://opentelemetry.io/docs/specs/otel/overview/">OTel</a>和其中间件<a href="https://opentelemetry.io/docs/collector/">OTel Collector</a>支持。</p><p><img src="/../img/2023-10-30-%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7%E8%A1%8C%E4%B8%9A%E5%8F%91%E5%B1%95%E4%B8%8E%E6%9C%BA%E9%81%87/otel-diagram.svg" alt="OpenTelemetry架构图"></p><p><a href="https://openmetrics.io/">OpenMetrics</a>基于Prometheus Exposition格式的基础上进行了一些改进，并增加了一些新功能（它们称之为“实施标准”，因为Prometheus的成功）。在<strong>Metrics</strong>方向上，OpenTelemetry与OpenMetrics有一定同类竞争的关系。但由于OpenTelemetry通过标准格式OTel和中间件OTel Collector，因此也可以提供与OpenMetrics的兼容性（体现在对Prometheus后端的支持）。除此之外OpenTelemetry 主要是为推送而设计的，而 OpenMetrics 则是为拉动而设计的，尽管 OpenMetrics 规范同时考虑了推送和拉动机制，这也是Prometheus原本就有的设计。</p><h4 id="开源产品"><a href="#开源产品" class="headerlink" title="开源产品"></a>开源产品</h4><p>日志收集与分析系统：</p><ul><li>ELK技术栈：ELK是Elasticsearch、Logstash、Kibana三大开源框架首字母，通过Logstash采集并发送数据，存储到Elasticsearch这一分布式搜索引擎中，通过Kibana提供前端易用的界面</li><li>Clickhouse+kafka替换Elasticsearch+Kibana，提供更好的查询性能，filebeat替换Logstash提供更好的采集性能</li><li>由于Log有很大的历史包袱，OpenTelemetry对Log的<a href="https://opentelemetry.io/docs/concepts/signals/logs/#language-support">各语言SDK</a>支持还不完善，但其协议设计结合了分布式追踪的要求，是未来的发展方向</li></ul><p><img src="/../img/2023-10-30-%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7%E8%A1%8C%E4%B8%9A%E5%8F%91%E5%B1%95%E4%B8%8E%E6%9C%BA%E9%81%87/otel-log-arch.png" alt="OpenTelemetry log系统架构"></p><p>分布式跟踪系统：</p><ul><li><a href="https://github.com/jaegertracing/jaeger">jaeger</a>: uber开源，原生支持OpenTelemetry格式，支持GRPC进行查询交互，有一个供搜索的简单UI</li><li><a href="https://github.com/openzipkin/zipkin">zipkin</a>: twitter开源，OpenTelemetry Collector支持，Java系常用，有比较好的可视化系统</li><li><a href="https://github.com/apache/skywalking">skywalking</a>: 华为捐献给apache，不仅提供链路追踪，也提供一些微服务的度量指标因此其与上面两个不同，是通过OpenTelemetry receiver支持OTel的格式以接入链路追踪的能力，而不是由OpenTel提供支持</li></ul><p>度量系统：</p><ul><li><a href="https://prometheus.io/">prometheus</a>: 目前已经是度量系统的事实标准了，目前看没有比较能打的替代方案。用户可以自定义metrics指标，然后在grafana中创建可视化仪表盘，或者直接使用Prometheus的查询器进行查询</li></ul><h4 id="商业对标"><a href="#商业对标" class="headerlink" title="商业对标"></a>商业对标</h4><p>目前最闪亮的是Datadog公司，它为云端软件的基础设施提供全维度的检测服务，上至最顶端的应用程序，到中间的 Kubernetes／Docker／Hypervisor，到操作系统，以及中间的数据库，第三方服务等全栈的性能监测。</p><p>在Datadog出现之前，企业每个团队之间是各自为政，互相割裂的。Datadog乘了公有云、微服务之风，为云上企业提供可观测性平台和服务，Datadog 试图做到的是将企业原本的十几个到最多几十个互相割裂的 IT 监测系统整合成一个，既提高监控效率，又降低企业的 IT 支出。</p><p>与之竞争的主要是APM 的服务商（Cisco（AppDynamics），New Relic，Dynatrace），和云平台服务商提供的类似服务。个人认为Log 管理服务商（Splunk 和 Elastic）是其供应商，目前重点竞争在trace和metrics上，Log是一个具有很大历史包袱的问题，很难有新的创新和大的变动。</p><h4 id="学术研究"><a href="#学术研究" class="headerlink" title="学术研究"></a>学术研究</h4><p>分布式追踪相关议题，TDB</p><h3 id="2-2-云原生网络（CNF）可观测性"><a href="#2-2-云原生网络（CNF）可观测性" class="headerlink" title="2.2 云原生网络（CNF）可观测性"></a>2.2 云原生网络（CNF）可观测性</h3><h4 id="概念定义-1"><a href="#概念定义-1" class="headerlink" title="概念定义"></a>概念定义</h4><blockquote><p><a href="https://lib.jimmysong.io/blog/cloud-native-network-functions/">https://lib.jimmysong.io/blog/cloud-native-network-functions/</a></p></blockquote><ul><li>云原生网络不是另一种方式的 SDN，它以一种完全不同的方式来看待网络。</li><li>虽然 SDN 似乎是把物理网络和机器做了虚拟化，但「云原生网络功能」（Cloud-native Network Functions，下文简称 CNF）不仅仅是容器化的网络和虚拟机，它还将网络功能分割成服务，这是 CNF 与 SDN 的一个主要区别。</li><li>CNF 是 OSI 网络模型中的网络功能（越底层实现起来就越困难），这些功能是根据云原生实践实现的。</li><li>虽然 SDN 数据平面（这里指的是转发数据包）位于硬件 ASIC 上，或在传统内核网络转发的虚拟化盒子里，但 CNF 探索用户平面转发或更新的 eBPF 数据路径转发。</li><li>在云原生数据中心中，偏向于三层的解决方案，但 CNF 的一大驱动力是电信服务提供商，他们经常下降到二层的功能。</li></ul><p><strong>CNF不是另一种SDN</strong>，而是以一种完全不同的方式来看待网络。从某种意义（技术实现）上说，CNF 与 SDN 一样，都是基于软件而非硬件的解决方案，<strong>这方便了基于软件的SDN公司的转型，即电信服务提供商</strong>。但云原生网络有一套全新的非功能要求，与 SDN 不同。云原生的非功能要求优先考虑弹性，并推而广之，自动化也比 SDN 多得多。</p><h4 id="开源标准-1"><a href="#开源标准-1" class="headerlink" title="开源标准"></a>开源标准</h4><p>正如上文提到的，我们可以将其看作是一种分布式追踪的应用场景，因此OpenTelemetry也可以作为其标准。</p><h4 id="开源产品-1"><a href="#开源产品-1" class="headerlink" title="开源产品"></a>开源产品</h4><p>与分布式追踪类似不同，CNF的开源产品主要是基于eBPF技术，实现零侵入式的观测，不需要在应用中插入代码。而这同样导致了其在可观测性方面的局限性。</p><ul><li><a href="https://cilium.io/">Cillium生态</a>: Cillium使用eBPF技术解决CNI网络问题，在此之上提供对服务的可观测性。在此之上构建的<a href="https://github.com/cilium/hubble">Hubble</a>项目，提供了对服务的依赖跟踪、运维报警、应用监测、安全视图等等功能。其提供了针对OpenTelemetry的适配器。<br><img src="/../img/2023-10-30-%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7%E8%A1%8C%E4%B8%9A%E5%8F%91%E5%B1%95%E4%B8%8E%E6%9C%BA%E9%81%87/hubble_arch.png" alt="Cillium/Hubble架构图"></li><li><a href="https://deepflow.io/">DeepFlow</a>: DeepFlow是<a href="https://yunshan.net/">云杉网络</a>开发的一款可观测性产品，基于eBPF技术对系统内部进行观测，旨在为复杂的云基础设施及云原生应用提供深度可观测性。其探针支持OpenTelemetry格式。其应用调用链追踪、网络时序等功能需要商业版。其母公司原业务是SDN服务，其通过DeepFlow为用户提供更加细粒度的支持，帮助定位网关、基础设施、客户业务的故障和性能问题。<br><img src="/../img/2023-10-30-%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7%E8%A1%8C%E4%B8%9A%E5%8F%91%E5%B1%95%E4%B8%8E%E6%9C%BA%E9%81%87/deepflow-architecture.png" alt="DeepFlow架构图"></li></ul><blockquote><p>DeepFlow发文表示相比和Hubble自己的优势很大功能很强，在我看来其是用社区+商业方案+服务对比开源版本的功能，并不是很平等：<a href="https://zhuanlan.zhihu.com/p/615790142">https://zhuanlan.zhihu.com/p/615790142</a></p></blockquote><h4 id="学术研究-1"><a href="#学术研究-1" class="headerlink" title="学术研究"></a>学术研究</h4><p>CNF、安全相关议题，TBD</p><h3 id="2-3-数据治理与可观测性"><a href="#2-3-数据治理与可观测性" class="headerlink" title="2.3 数据治理与可观测性"></a>2.3 数据治理与可观测性</h3><blockquote><p><a href="https://www.kdnuggets.com/2022/08/data-governance-observability-explained.html">https://www.kdnuggets.com/2022/08/data-governance-observability-explained.html</a></p></blockquote><h4 id="概念定义-2"><a href="#概念定义-2" class="headerlink" title="概念定义"></a>概念定义</h4><p>数据可观测性是最近刚刚被提出来的词，目前对这个词聊的最多的是<a href="https://databand.ai/">DataBand</a>公司（总部在以色列特拉维夫），其在2022年被IBM收购，其将数据治理中“更快地发现数据在哪个环节出了问题”作为可观测性的定义，由此发展它的一整个平台，从而对数据ETL、数据治理全流程的观测。其实现方式和分布式追踪的方式类似，通过在数据流中插入探针，收集数据流中的元数据，通过其平台分析以此快速寻找到数据流中的问题，从而为用户提供价值。</p><p>另一家公司monte carlo也将对数据生命周期每个阶段全面了解数据的健康状态定义为数据可观测性，并用数据治理的概念来描述整个系统：<a href="https://www.montecarlodata.com/product/data-observability-platform/">https://www.montecarlodata.com/product/data-observability-platform/</a></p><blockquote><p>在DataBand的demo的演讲里，可以明显看出他们做的是数据治理和数据基础设施结合（DataOps）：<a href="https://www.youtube.com/watch?v=lmiLSgnptBs">https://www.youtube.com/watch?v=lmiLSgnptBs</a></p></blockquote><p>我们可以认为，目前数据可观测性（Data observability）的概念就是用全流程、持续性地收集数据信息、测试数据等数据治理手段来实现数据治理的目标——数据质量的持续观测和改善，将数据可观测性作为一种技术手段和解决方案，将得到一个系统&#x2F;平台，以逐渐实现该目标，同时支持快速发现数据问题，减少平均处理时间。</p><h4 id="关键领域"><a href="#关键领域" class="headerlink" title="关键领域"></a>关键领域</h4><blockquote><p>微软在其中介绍了关键领域和成熟度模型，可以看作是下一阶段数据治理服务的新方向：<a href="https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/scenarios/cloud-scale-analytics/manage-observability">https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/scenarios/cloud-scale-analytics/manage-observability</a></p></blockquote><p>大多数数据平台都在数据可观测性的这些关键领域运行：</p><ul><li>数据平台服务监控</li><li>数据管道性能监控</li><li>数据质量监控</li><li>数据脉络</li><li>数据发现</li></ul><p>端到端数据可观察性不仅包括捕获事件和测量所有这些组件的指标，还包括关联这些事件和指标。这样就能全面了解企业数据环境的健康状况和可靠性。</p><h4 id="开源标准-2"><a href="#开源标准-2" class="headerlink" title="开源标准"></a>开源标准</h4><p>N&#x2F;A</p><h4 id="学术研究-2"><a href="#学术研究-2" class="headerlink" title="学术研究"></a>学术研究</h4><p>数据库、数据治理相关议题，TBD</p><h2 id="3-关键技术总结"><a href="#3-关键技术总结" class="headerlink" title="3. 关键技术总结"></a>3. 关键技术总结</h2><ul><li><strong>以OpenTelemetry为基础的可观测性架构</strong>：OpenTelemetry为目前的开源生态提供了标准化的数据格式、API接口方式，可以支持不同开源项目或商业项目与其对接，并允许其自定义自己的数据属性。<strong>目前看到的所有项目都有相关工程和方法与OpenTelemetry结合，其已经成为当前开放生态的标准</strong></li></ul><blockquote><p>传统的 APM 无法实现真正的可观测性：一方面插桩行为已经修改了原程序，逻辑上已无法实现原程序的可观测性；另一方面云原生基础设施组件越来越多，基础服务难以插桩导致观测盲点越来越多。实际上，插桩的方式在金融、电信等重要行业的核心业务系统中几乎无法落地。eBPF 由于其零侵扰的优势，避免了 APM 插桩的缺点，是云原生时代实现可观测性的关键技术。—— <a href="https://deepflow.io/zh/ebpf-the-key-technology-to-observability">DeepFlow: eBPF是实现可观测性的关键技术</a></p></blockquote><ul><li><strong>以eBPF技术为基础的无侵入探针</strong>：eBPF和应用程序的代码之间划上了一道清晰的界限，使得我们能在不对应用程序做任何修改的前提下，通过获取外部数据就能确定其内部状态。<strong>目前看到新一代的观测系统&#x2F;平台都是以eBPF作为核心技术进行开发的</strong></li></ul><h2 id="4-可观测性的新视角探索"><a href="#4-可观测性的新视角探索" class="headerlink" title="4. 可观测性的新视角探索"></a>4. 可观测性的新视角探索</h2><h3 id="4-1-为“X”赋予可观测性，X还可以是什么？"><a href="#4-1-为“X”赋予可观测性，X还可以是什么？" class="headerlink" title="4.1 为“X”赋予可观测性，X还可以是什么？"></a>4.1 为“X”赋予可观测性，X还可以是什么？</h3><h4 id="安全可观测性：构建于可观测性上的安全运营与态势感知"><a href="#安全可观测性：构建于可观测性上的安全运营与态势感知" class="headerlink" title="安全可观测性：构建于可观测性上的安全运营与态势感知"></a>安全可观测性：构建于可观测性上的安全运营与态势感知</h4><blockquote><p><a href="https://www.dynatrace.com/news/blog/data-privacy-by-design/">https://www.dynatrace.com/news/blog/data-privacy-by-design/</a></p></blockquote><p>云原生数据库等新兴概念的出现，大大消除了大家对于存储成本方面的顾虑，使得企业可以更聚焦于提升观测能力本身，而不必在成本和性能方面分神，这方面的发展也有迹可循。比如今年 DataDog 推出了一个名为哈士奇的存储架构，它是一个完全云原生面向可观测性的数据库，得益于云原生的存储能力，可以在同等价格下多存储 5-10 倍数据，以往即使收集到了海量的数据，但面对高昂的存储成本，这些数据也只能被丢弃或者针对这些数据进行采样，本质上看，这对客户需要的结果也是有浪费的。但这也并不意味着使用这种新技术之后并不需要进行采样，只是新的更低成本更高性能的数据库技术，可以大大提升可观测性能够覆盖的范围，降低存储成本，这也是一个很重要的发展。</p><p>安全和可观测性的合并，已在全球范围内形成一种趋势。摩根士丹利在《安全分析和可观测性》文章中也提到，在国外，以 DataDog 为代表的公司在上市之后发布的新增功能中有 70% 都是安全相关的，这其中的道理非常简单，可观测性是通过检查其输出来衡量系统内部状态的能力，它收集了系统的方方面面，通过这些数据可以分析出系统的故障，自然也就能够分析出系统有没有被入侵。比如 DataDog 就提供了通过分析当前访问请求，区分哪些可能是黑客在嗅探，或者准备未来做 DDoS 攻击的接口的功能。</p><blockquote><p>可观察性工具不仅能够处理系统问题，如记录事件和活动，收集、跟踪、捕获和分析数据和性能行为，甚至在系统性能偏离预期水平时发出警报。这些工具在保护企业重要数据安全方面的能力更上一层楼。可观察性工具可以监控系统日志和指标，以发现可能表明存在安全威胁的异常活动。例如，如果用户试图访问他们没有权限访问的资源，可观察性工具就会对这些行为进行标记。此外，可观察性工具还能识别黑客可能利用的漏洞。为此，它可以监控网络流量，帮助检测不安全的通信协议或未加密的数据，并跟踪内部用户活动，确保员工没有参与未经授权或可疑的行为。在发生潜在的安全漏洞时，可观察性工具可以实时了解潜在事件，使公司能够快速做出反应。—— <a href="https://www.computer.org/publications/tech-news/trends/addressing-observability-ethical-concerns">https://www.computer.org/publications/tech-news/trends/addressing-observability-ethical-concerns</a></p></blockquote><p>也就是说，采集的数据在安全方面也能够发挥作用，而不像传统安全工具那样，需要针对安全场景再进行一次数据采集。所以，安全和可观测性的合并在全球范围内已经成为一种趋势，当然抗 DDoS 、挖防火墙这些并不会合并，针对攻击现场的追踪，比如国内的态势感知、国外的 SIEM 这些安全产品都选择了和可观测性进行融合。</p><h4 id="业务可观测性：互联网业务至上视角下的可观测性"><a href="#业务可观测性：互联网业务至上视角下的可观测性" class="headerlink" title="业务可观测性：互联网业务至上视角下的可观测性"></a>业务可观测性：互联网业务至上视角下的可观测性</h4><blockquote><p><a href="https://www.infoq.cn/article/v5r45xkttvizcieg6ffs">https://www.infoq.cn/article/v5r45xkttvizcieg6ffs</a></p></blockquote><p>可观测性的概念起源于工业领域，在该领域中，可观测性被定义为从系统外部输出推断系统内部健康状态的能力；在软件产品和服务领域，可观测性就是从应用系统中收集尽可能多的遥测数据，以便可以调查和解决新出现的复杂问题，确保企业能够主动观察系统，在影响客户体验之前解决故障及问题，安全地进行测试并实施优化，它可以更好地管理和控制业务风险，有助于我们了解“正在发生什么”以及“为什么会这样”。可观测性使团队能够更有效地监控现代系统，帮助他们找到并连接复杂链中的影响，并将其追溯到原因。此外，它还使系统管理员、IT 运营分析师和开发⼈员能够了解他们的整个架构。</p><p>如今的 IT 系统，迭代发布更迅速，业务系统更庞大，网络链路更复杂，运行环境更动态。在“业务至上”的互联网时代，技术工程师们保障的核心其实并不是这套 IT 系统或软件，他们保障的核心其实是业务，一笔业务可能会涉及到多个微服务系统，技术工程师们不再追踪一个 API 的全链路调用关系，而是要追踪到整个 API 关联的订单、用户甚至具体到哪一笔交易，这也是可观测性和业务结合的一个重要发展趋势。Gartner 也提到，“未来一切业务皆需可观测性”，简单地讲就是把运营人员、运维人员、IT 人员看到的数据做统一，而不是互相甩锅。</p><h4 id="数据使用可观测性：数据分享视角下的可观测性"><a href="#数据使用可观测性：数据分享视角下的可观测性" class="headerlink" title="数据使用可观测性：数据分享视角下的可观测性"></a>数据使用可观测性：数据分享视角下的可观测性</h4><p>数据分享是具有重要意义的，其通过数据流通的方式，让数据的价值最大化。由于监管、隐私、商业秘密等考虑，许多研究都关注于在数据分享的过程中，如何保护数据的隐私性、安全性（机密性），从而提出了<a href="https://arxiv.org/abs/2305.03842">Data Station</a>的数据托管方式，<a href="https://en.wikipedia.org/wiki/Federated_learning">联邦学习</a>的机器学习算法方式，<a href="https://en.wikipedia.org/wiki/Homomorphic_encryption">同态加密</a>的加密计算方式，<a href="https://en.wikipedia.org/wiki/Trusted_execution_environment">TEE</a>的硬件加密方式，以及各种各样的一体化解决方案。这与微服务初期人们开始考虑如何让业务跑起来是类似的，而如何了解到数据分享的过程中，数据被使用的情况，则是数据分享视角下的可观测性需要考虑的。</p><p>了解数据被使用的情况有如下好处：</p><ul><li>隐私政策的实现：用户可以更细粒度地了解、授权自己的数据被使用的手段，从而更好地保护自己的隐私</li><li>数据使用的监管：数据的使用情况可以被监管机构进行监管，从而保证数据的合法使用</li><li>数据合约的实现：数据的使用情况可以被记录下来，从而可以实现数据的合约，如数据的使用次数、使用时间、使用范围等等，同时促进数据价值评估的发展</li></ul><blockquote><p>我将在<a href="https://wh1isper.github.io/2023/10/30/2023-10-31-%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E4%BD%BF%E7%94%A8%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/">逆转思路：从数据可计量到数据可控</a>中详细聊聊这一系统概念和系统应该如何实现，最终服务于数据要素流通市场的发展</p></blockquote><h3 id="4-2-从符号主义到连接主义：可观测性与LLM"><a href="#4-2-从符号主义到连接主义：可观测性与LLM" class="headerlink" title="4.2 从符号主义到连接主义：可观测性与LLM"></a>4.2 从符号主义到连接主义：可观测性与LLM</h3><p>目前，可观测性展现出符号主义的特征，基于目前已有技术对系统遥测信息等内容进行采集、存储、处理，通过人为地对系统进行分析，对业务进行总结的方式，创建出可视化的仪表盘、规划和实现告警系统、建立供人分析的链路，通过上述流程，我们建立起了一个可观测系统，基于此对问题进行快速定位和解决。我们会发现，这个过程中需要富有经验的SRE人员、高级的开发人员共同参与，其困难和投入是巨大的，这也从侧面显示了相关服务商地价值（DeepFlow、isovalent等公司就是提供了帮助用户优化系统的服务，就如同商业数据库一样）。</p><blockquote><p>DeepFlow提供的用户案例：<a href="https://deepflow.io/zh/ebpf-the-key-technology-to-observability">https://deepflow.io/zh/ebpf-the-key-technology-to-observability</a></p></blockquote><p>这种情况在运营类的技术活动中屡见不鲜，从安全运营的经验来看，微软基于LLM推出的<a href="https://www.microsoft.com/en-us/security/business/ai-machine-learning/microsoft-security-copilot">Microsoft Security Copilot</a>为安全团队提供了一个可以持续学习、快速高效的助手，能够让团队更快地处理更多地威胁情报。从这一角度出发，将遥测信息输入LLM大模型，让大模型帮助人们对信息进行提炼，对系统进行分析，对业务进行总结，从而提供更加高效高质量的可观测性，是一个值得探索的方向。</p><blockquote><p>在这一回答中，我们看到了类似的观点：中国有没有类似splunk、datadog这种类型的公司吗？ - 共识粉碎机的回答 - 知乎 <a href="https://www.zhihu.com/question/355794555/answer/3178453725">https://www.zhihu.com/question/355794555/answer/3178453725</a></p></blockquote><h2 id="5-总结：可观测性是将复杂系统白盒化的过程"><a href="#5-总结：可观测性是将复杂系统白盒化的过程" class="headerlink" title="5. 总结：可观测性是将复杂系统白盒化的过程"></a>5. 总结：可观测性是将复杂系统白盒化的过程</h2><p>可观测性在应用层上的主要应用：分布式追踪的领域，已经趋于成熟，形成了以OpenTelemetry为基础的行业统一标准。基础设施层面，依托云原生的蓬勃发展，从CNF出发，基于eBPF技术对云原生中的基础设施和服务提供观测，是目前蓬勃发展的热点。</p><p>从复杂系统的白盒化角度出发，我们可以发现仍有许多角度等待探索，如数据治理、LLM等等。这些领域的探索，将会为我们提供更多的机遇。</p>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>开源</tag>
      
      <tag>可观测性</tag>
      
      <tag>数据使用</tag>
      
      <tag>DataUCon</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🤔过剩与竞劣在软件行业</title>
    <link href="/2023/10/28/2023-10-29-%E8%BF%87%E5%89%A9%E4%B8%8E%E7%AB%9E%E5%8A%A3/"/>
    <url>/2023/10/28/2023-10-29-%E8%BF%87%E5%89%A9%E4%B8%8E%E7%AB%9E%E5%8A%A3/</url>
    
    <content type="html"><![CDATA[<h2 id="灵感来源"><a href="#灵感来源" class="headerlink" title="灵感来源"></a>灵感来源</h2><p>温铁军教授在其研究中系统地阐述了全球过剩所带来的必然的竞劣（竞次），目前全球处于”产业过剩”、”金融过剩”、”商业过剩”的三种过剩阶段，而全球过剩的条件下必然导致恶性竞争，而恶性竞争的结果必然是竞劣（竞次）。</p><h2 id="互联网-软件开发行业的过剩与竞劣"><a href="#互联网-软件开发行业的过剩与竞劣" class="headerlink" title="互联网&#x2F;软件开发行业的过剩与竞劣"></a>互联网&#x2F;软件开发行业的过剩与竞劣</h2><p>与过去的”专科培训三个月拿下月薪上万工作”相比，今年的互联网&#x2F;软件开发相关的初级岗位呈现出供应特别爆炸，需求极度萎缩的场景。这不仅仅是因为整体大环境不好，还因为中国初级开发者的超额供应，形成的所谓”工程师福利”。而所谓工程师红利，与之前的人口红利一样，都可以看作是资本主义下，将人当作是一种生产资料，过度剥削的结果。</p><p>过剩来带的竞劣，表现在低端开发者的收入锐减，为整个软件生态和社会带来积极性和流动性上的巨大打击。在如今中国的软件业，开源刚刚进入繁荣阶段，却突然遭受到如此冲击，无疑会造成整个生态发展的停滞，从而在好不容易通过互联网商业追赶上的先进技术领域又一次陷入落后。</p><p>正如我在<a href="https://wh1isper.github.io/2023/10/22/2023-10-23-%E4%BA%BA%E8%8B%A5%E6%97%A0%E5%90%8D%E4%B8%93%E5%BF%83%E7%BB%83%E5%89%91/">人若无名，专心练剑</a>这篇文章中所说的，在目前的大趋势下，继续看好计算机技术的未来发展，而在本次寒潮中坚持下来，成为中高级的开发者、优秀的CTO，则是对自己的逆周期投资。</p><h2 id="感叹"><a href="#感叹" class="headerlink" title="感叹"></a>感叹</h2><p>依旧觉得自己赚得太少，付出太多，前途不明，安全感不足。</p><p>或许我需要尽快的发展副业养活自己来降低自己的不安全感，这一问题应当和大老板的职业规划中被讨论。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
      <category>时评</category>
      
      <category>读书笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
      <tag>政策</tag>
      
      <tag>经济</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>❗我究竟在抱怨什么</title>
    <link href="/2023/10/27/2023-10-28-%E6%88%91%E7%A9%B6%E7%AB%9F%E5%9C%A8%E6%8A%B1%E6%80%A8%E4%BB%80%E4%B9%88/"/>
    <url>/2023/10/27/2023-10-28-%E6%88%91%E7%A9%B6%E7%AB%9F%E5%9C%A8%E6%8A%B1%E6%80%A8%E4%BB%80%E4%B9%88/</url>
    
    <content type="html"><![CDATA[<blockquote><p>只有一种人才有资格抱怨：在彻底绝望地告别之前，按一贯的标准，尽最大的努力做好每一件事的人。———— 我·自己·说的</p></blockquote><p>大约一年前我是S项目的主程，同时也是架构设计的主要负责人，由于负责O项目的前期调研工作和后续设计、开发任务，半年前我移交了所有的S项目的工程给一名资深的程序员C（6-7年的编程经验，但没有计算机相关的教育经历），仅作为架构和实现的咨询人员参与S项目。</p><p>在我移交项目之时，<strong>相关工程的测试覆盖率在80%左右，所有工程都有着相对应的CI设施，能够在每次代码推送后自动构建好可供部署的二方库</strong>。在我移交项目之后，我仍会在设计、关键评审、代码review等方面提供帮助，但并不参与各方的联调，也不对后续修改负责。</p><p>大概三周前，程序员C离职，其离职原因是组长对他的指责，<strong>将相关工作交接给了我的组长</strong>，此后S项目由组长负责。</p><p>事情发生在组长出差到客户现场支持时，这是本季度最重要的项目，我们所有人<strong>不得不放弃其他工作</strong>对此项目进行支持，我也从O项目被调到S项目，为客户现场需求提供支持。期间我发现，<strong>原有的单元测试已经无法运行，旧的工程CI大部分失效，新的工程既没有单元测试和CI，代码质量也不尽如人意，我不得不在大量的代码片段中找到我需要的配置，我还发现了多个C在离职时声称已经完成、并测试过的功能实际上未经测试、无法运行，甚至有些功能根本没有实现</strong>。</p><p>于是，在前两天组长对我定位问题的指手画脚中，我爆发了。我不想讨论其中细节，因为这只是一个导火索，而下面我将讨论<strong>根本原因</strong>。</p><h2 id="从个人发展的角度：一致性缺失"><a href="#从个人发展的角度：一致性缺失" class="headerlink" title="从个人发展的角度：一致性缺失"></a>从个人发展的角度：一致性缺失</h2><p><strong>组长很忙，我将成为顶缸的老实人</strong>。组长不止负责S项目，还有其他A、B、C、D等等等等项目、沟通、汇报等任务，因此从程序员C离职以来，我就知道再也不可能像之前那样从S项目中抽身出来，全身心地投入团队和我的未来——O项目。于是，就像这次一样，在组长出差的时候，<strong>我将承受巨大的上下文转换的压力和代价</strong>，在非常短的时间内响应项目里的需求、咨询、问题，同时又要保证O项目的进度。这是因为O项目所采取的理念和技术都是很前沿的，有太多需要确认、调研、测试的内容，而我是唯一一个能够做到这些的人。而S项目，留给我的是一个我无法掌控的泥潭，<strong>我没有办法在如此有限的时间和精力投入下将S项目带回原来的正轨</strong>，同时我认为组长也不能（因为他太忙了），那么我的精力就又会被S项目所分散，<strong>越陷越深</strong>，直到最后<strong>技术债务破产</strong>。但在这之前，我感受到的是深深的无力感和失望：我既不能将S项目做好（下面我将解释除了精力以外，还有别的原因），也缺乏在O项目投入的一致性。</p><h2 id="从团队的角度：这不是一个具有”工程师文化”的团队"><a href="#从团队的角度：这不是一个具有”工程师文化”的团队" class="headerlink" title="从团队的角度：这不是一个具有”工程师文化”的团队"></a>从团队的角度：这不是一个具有”工程师文化”的团队</h2><p><strong>缺乏质量意识</strong>：忙是一个客观因素，而“能跑就行”则是最让我感到失望的主观因素。<strong>由于S项目的研发经理们和程序员们对于代码的态度都是“能跑就行”</strong>，这意味着在我组的工作无论多么优秀，都将受到上下游代码质量和调试时间的影响。这一态度从我入职以来就从未改变，我所能做的只有尽量早地提供我能提供的一切支持，并督促他们尽早尽快地进行联调，这也是我将本组的自动化工作和代码质量作为重要工作内容的原因。但由于目前上下游所在组的资深程序员也有缺口，低级的校招生并不能按时、保质地完成编码任务，常常匆忙中进入联调阶段，由此联调阶段的时间被拉长，bug率显著上升，导致我组负责此项目的人需要投入更多的低价值工作，来完成项目。<strong>我个人称之为：被拖累的部分。这一情况的发生，我认为组长们需要负主责，而下面的程序员也难辞其咎。</strong></p><blockquote><p>有一次我在一个月之前就提供了开发并测试完成的服务镜像，但到最后一天才被告知其无法启动容器，我只能紧急加班为其解决。事后证明是其没有阅读相关文档，但此文档我已经在提供容器时交付。</p></blockquote><p><strong>团队成员之间缺乏信任</strong>：团队对于程序员（技术人员）也缺乏尊重，自以为是，常常有产品、组长、研发经理等人在不了解技术细节的情况下，对程序员的工作进行<strong>指手画脚，甚至是指责</strong>。这一情况将严重打击程序员的积极性，加强了“能跑就行”的思想，又因为这种思想导致了质量问题，从而引发信任危机，最终导致了团队的<strong>恶性循环</strong>。我将此归咎于“工资更高”的人的傲慢、不尊重，以及“工资更低”的人的自卑、不自信。我并不是在说某个或某群人的性格有缺陷，<strong>这是一个团队文化&#x2F;士气问题</strong>，一个乐观开朗的人在一群悲伤的人中也会被影响，人群的力量是隐秘但巨大的。</p><blockquote><p>有一次产品负责人Z要求运维保存所有模型服务的日志，并告知“这就是用户的需求”。运维为此任务拟引入Clickhouse以及一整套查询系统，在我的强烈要求下，产品才解释用户的需求是保留模型服务的请求日志，而这一功能是已经通过边车代理实现的。产品负责人Z并不清楚另一个产品经理已经设计了这一功能，并且在下个版本就能上线。</p></blockquote><p><strong>自己为是的领导风格</strong>：我对“外行指导内行”、“领导一句话，累死跑腿的”、“将帅无能，累死三军”的现状感到失望、无助和愤怒。似乎领导讲两句，下属照做即可，一切尽在掌握中。但实际上，领导不相信你能做好，于是又把<strong>上下文（context）藏着掖着</strong>，仿佛告诉了你天就塌了，或者花这些时间在给你解释前因后果纯属浪费，但事实是领导自己也没想好怎么做，也是脚踩西瓜皮滑到哪算哪。于是大家东忙西忙，由于缺少上下文成为无头苍蝇，与之对应的，是对<strong>真正高价值目标的持续性的投入的缺失</strong>。与其上的一致性缺失所对应。</p><blockquote><p>包括这次爆发也是一次令人愤怒的指手画脚</p></blockquote><h2 id="从集团的角度：缺少数据基因，无法给团队带来成长"><a href="#从集团的角度：缺少数据基因，无法给团队带来成长" class="headerlink" title="从集团的角度：缺少数据基因，无法给团队带来成长"></a>从集团的角度：缺少数据基因，无法给团队带来成长</h2><p>集团的传统业务是安全乙方，其思考的是如何在不影响用户业务的情况下，分析用户业务，为用户提供安全加固的服务并收取费用。而我们想做的是接管用户的某个数据使用流程，通过此方式保护用户的数据，使其满足某些合规要求，或在合规要求下开展之前无法开展的业务。二者从销售的角度上讲，最大的不同就在于从以前的卖盒子、卖小软件，变成了卖整个服务集群、卖一整个处理系统，其规模、成本与销售难度的对比就像是蚂蚁与大象。而集团内部的人仍然会使用传统的安全乙方思维来看待我们的项目、产品，等于用篮球运动员的标准评价桌球运动员，那丁俊晖肯定不能在姚明头上暴扣。<strong>而其积重难返，我们人微言轻，无法改变这一现状。</strong></p><p>而同时，由于集团效益，也没有HC重新招聘人员维护S项目，<strong>这将加剧S项目技术债务的积累</strong>，与我上文提到的越陷越深、拖垮团队形成有机结合，是典型的项目恶性循环。</p><p>由此，我觉得S项目在集团的前途是暗淡的，被S项目拖累的团队的前途亦是暗淡的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>综上，我抱怨的实际上是<strong>不负责任、推诿、不作为、得过且过的思想</strong>，<strong>对工程问题异想天开、自以为是的幼稚病，以及缺少相关基因</strong>，<strong>无法提供土壤的集团环境</strong>。</p><p>就我个人而言，<strong>为了实现自我价值与身心健康，为了大团队能够进一步往下走</strong>，我<strong>不愿</strong>，<strong>不想</strong>，<strong>不支持</strong>再将自己的精力投入在S项目上。</p><p><strong>但基于我的职业操守，在我彻底绝望告别之前。我将尽我最大的努力做好我应该做的，按照我一贯要求自己的标准。</strong></p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
      <tag>精神分析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🚀遨游互联网远程开发指北</title>
    <link href="/2023/10/26/2023-10-27-%E9%81%A8%E6%B8%B8%E4%BA%92%E8%81%94%E7%BD%91%E8%BF%9C%E7%A8%8B%E5%BC%80%E5%8F%91%E6%8C%87%E5%8C%97/"/>
    <url>/2023/10/26/2023-10-27-%E9%81%A8%E6%B8%B8%E4%BA%92%E8%81%94%E7%BD%91%E8%BF%9C%E7%A8%8B%E5%BC%80%E5%8F%91%E6%8C%87%E5%8C%97/</url>
    
    <content type="html"><![CDATA[<h1 id="TLDR"><a href="#TLDR" class="headerlink" title="TLDR"></a>TLDR</h1><p>用一个域名实现公网ipv6直连、公网ipv4穿透、内网直连访问</p><blockquote><p>此教程并非通用教程，可参考修改</p></blockquote><p><img src="/../img/2023-10-27-%E9%81%A8%E6%B8%B8%E4%BA%92%E8%81%94%E7%BD%91%E8%BF%9C%E7%A8%8B%E5%BC%80%E5%8F%91%E6%8C%87%E5%8C%97/remote-develop.png" alt="网络拓扑图"></p><h1 id="案例与效果展示"><a href="#案例与效果展示" class="headerlink" title="案例与效果展示"></a>案例与效果展示</h1><p>内网用户访问<code>yourdomain</code>，可以直接访问到内网机器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ curl -v https://wh1isper.top:8080<br>*   Trying [::ffff:192.168.31.254]:8080...<br>* Connected to wh1isper.top (::ffff:192.168.31.254) port 8080<br></code></pre></td></tr></table></figure><p>外网ipv6用户访问<code>yourdomain</code>，可以通过ipv6公网地址直连</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ curl -v https://wh1isper.top:8080<br>*   Trying [2408:8256:3284:1522::905]:8080...<br>* Connected to wh1isper.top (2408:8256:3284:1522::905) port 8080 (<span class="hljs-comment">#0)</span><br></code></pre></td></tr></table></figure><p>外网ipv4用户访问<code>yourdomain</code>，可以通过跳板机访问</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ nslookup wh1isper.top<br>curl -v https://wh1isper.top:8080<br>*   Trying 42.193.219.110:8080...<br>Connected to wh1isper.top (42.193.219.110) port 8080 (<span class="hljs-comment">#0)</span><br></code></pre></td></tr></table></figure><h2 id="案例：部署一个codeserver"><a href="#案例：部署一个codeserver" class="headerlink" title="案例：部署一个codeserver"></a>案例：部署一个codeserver</h2><p><code>yay code-server</code>，安装<code>aur/code-server-marketplace</code>和<code>aur/code-server</code></p><p>根据文档配置证书、密码等：<a href="https://github.com/coder/code-server">https://github.com/coder/code-server</a></p><p>证书可以在域名服务商申请，也可以使用免费的letsencrypt，本教程不包含相关内容</p><p><code>~/.config/code-server/config.yaml</code></p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">host:</span> <span class="hljs-string">&quot;::&quot;</span><br><span class="hljs-symbol">port:</span> <span class="hljs-number">2443</span><br><span class="hljs-symbol">auth:</span> password<br><span class="hljs-symbol">password:</span><br><span class="hljs-symbol">cert:</span> <span class="hljs-keyword">/path/</span>to<span class="hljs-keyword">/cert/</span>xxx.crt<br>cert-key: <span class="hljs-keyword">/path/</span>to<span class="hljs-keyword">/cert/</span>xxx.key<br></code></pre></td></tr></table></figure><p>然后将2443端口映射到跳板机的2443端口</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-section">[FRP-CODE-SERVER]</span><br><span class="hljs-attr">type</span> = tcp<br><span class="hljs-attr">local_ip</span> = <span class="hljs-number">127.0</span>.<span class="hljs-number">0.1</span><br><span class="hljs-attr">local_port</span> = <span class="hljs-number">2443</span><br><span class="hljs-attr">remote_port</span> = <span class="hljs-number">2443</span><br></code></pre></td></tr></table></figure><blockquote><p>目前，HTTPS+TCP跳板的方式可以绕过腾讯云域名备案检测，可以使用腾讯云的服务器，否则可能需要境外服务器以绕过备案，或者进行备案</p></blockquote><p><img src="/../img/2023-10-27-%E9%81%A8%E6%B8%B8%E4%BA%92%E8%81%94%E7%BD%91%E8%BF%9C%E7%A8%8B%E5%BC%80%E5%8F%91%E6%8C%87%E5%8C%97/code-server.png" alt="效果图"></p><h1 id="场景说明"><a href="#场景说明" class="headerlink" title="场景说明"></a>场景说明</h1><ul><li>家庭宽带提供ipv6公网地址，但是没有ipv4公网地址<ul><li>大部分省份的家庭宽带都有这项功能，一般而言，开发机器获得公网ipv6地址可以通过设置光猫为桥接模式，路由器拨号，然后再路由器ipv6处使用DHCPv6协议获取公网ipv6地址</li><li>大部分省份手机流量已经支持ipv6，这使得我们在外时可以使用手机流量直连机器，获取满带宽</li></ul></li><li>有一台具有公网ipv4地址的服务器，作为内网穿透跳板机</li><li>家庭使用的路由器可以设置dhcp地址绑定和静态域名解析<ul><li>这使得在家使用时，可以通过域名直接访问内网机器。dhcp地址绑定是可选的，你也可以静态分配ip</li><li>以红米路由器为例，可以破解进入ssh配置静态域名解析</li><li>如果有Clash等软路由，一般都支持静态域名解析，甚至可以自动做优选</li></ul></li><li>一个域名，选你喜欢的即可</li></ul><h1 id="重点问题"><a href="#重点问题" class="headerlink" title="重点问题"></a>重点问题</h1><h2 id="如何将开发机的ipv6地址进行绑定"><a href="#如何将开发机的ipv6地址进行绑定" class="headerlink" title="如何将开发机的ipv6地址进行绑定"></a>如何将开发机的ipv6地址进行绑定</h2><p>难点： 由于ipv6地址是动态分配的，所以需要一个机制将ipv6地址绑定到域名上，每次变动后刷新（即DDNS）。如果有任何DDNS的服务商，可以直接使用其提供的API进行绑定，如果没有，可以使用脚本进行绑定。</p><p>这里以dnspod（腾讯云dns）为例，给出使用其API的地址绑定方案脚本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> socket<br><span class="hljs-keyword">import</span> sys<br><span class="hljs-keyword">import</span> smtplib<br><span class="hljs-keyword">import</span> subprocess<br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> urllib.request<br><span class="hljs-keyword">from</span> email.header <span class="hljs-keyword">import</span> Header<br><span class="hljs-keyword">from</span> email.mime.text <span class="hljs-keyword">import</span> MIMEText<br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> tencentcloud.common <span class="hljs-keyword">import</span> credential<br><span class="hljs-keyword">from</span> tencentcloud.common.exception.tencent_cloud_sdk_exception <span class="hljs-keyword">import</span> (<br>    TencentCloudSDKException,<br>)<br><br><span class="hljs-keyword">from</span> tencentcloud.dnspod.v20210323 <span class="hljs-keyword">import</span> dnspod_client, models<br><br><span class="hljs-comment"># 为了保护密钥安全，建议将密钥设置在环境变量中或者配置文件中，请参考本文凭证管理章节。</span><br><span class="hljs-comment"># 硬编码密钥到代码中有可能随代码泄露而暴露，有安全隐患，并不推荐。</span><br><span class="hljs-comment"># cred = credential.Credential(&quot;secretId&quot;, &quot;secretKey&quot;)</span><br>cred = credential.Credential(<br>    os.environ.get(<span class="hljs-string">&quot;TENCENTCLOUD_SECRET_ID&quot;</span>, <span class="hljs-string">&quot;&quot;</span>),<br>    os.environ.get(<span class="hljs-string">&quot;TENCENTCLOUD_SECRET_KEY&quot;</span>, <span class="hljs-string">&quot;&quot;</span>),<br>)<br><span class="hljs-comment"># DOMAIN = &quot;example.com&quot; 是你注册的域名</span><br>DOMAIN = os.environ.get(<span class="hljs-string">&quot;DOMAIN&quot;</span>, <span class="hljs-string">&quot;&quot;</span>)<br>client = dnspod_client.DnspodClient(cred, <span class="hljs-string">&quot;ap-guangzhou&quot;</span>)<br><br><br>cmd = <span class="hljs-string">&quot;ifconfig&quot;</span><br>encoding = <span class="hljs-string">&quot;utf-8&quot;</span><br>splitter = <span class="hljs-string">&quot;inet6&quot;</span><br><br><span class="hljs-keyword">import</span> socket<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">isNetOK</span>(<span class="hljs-params">testserver</span>):<br>    s = socket.socket()<br>    s.settimeout(<span class="hljs-number">3</span>)<br>    <span class="hljs-keyword">try</span>:<br>        status = s.connect_ex(testserver)<br>        <span class="hljs-keyword">if</span> status == <span class="hljs-number">0</span>:<br>            s.close()<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">isNetChainOK</span>(<span class="hljs-params">testserver=(<span class="hljs-params"><span class="hljs-string">&quot;www.baidu.com&quot;</span>, <span class="hljs-number">443</span></span>)</span>):<br>    isOK = isNetOK(testserver)<br>    <span class="hljs-keyword">return</span> isOK<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ddns_refresh</span>(<span class="hljs-params">v6_address</span>):<br>    <span class="hljs-keyword">for</span> addr <span class="hljs-keyword">in</span> v6_address:<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;::&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> addr:<br>            <span class="hljs-keyword">continue</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;refreshing ddns &#x27;<span class="hljs-subst">&#123;addr&#125;</span>&#x27;&quot;</span>)<br><br>        m = models.DescribeRecordListRequest()<br>        m.Domain = DOMAIN<br>        records = client.DescribeRecordList(m)<br>        <span class="hljs-keyword">for</span> record <span class="hljs-keyword">in</span> records.RecordList:<br>            <span class="hljs-comment"># remove all records</span><br>            <span class="hljs-keyword">if</span> record.<span class="hljs-type">Type</span> == <span class="hljs-string">&quot;AAAA&quot;</span>:<br>                req = models.DeleteRecordRequest()<br>                req.Domain = DOMAIN<br>                req.RecordId = record.RecordId<br>                client.DeleteRecord(req)<br><br>        <span class="hljs-comment"># add new record for addr</span><br>        req = models.CreateRecordRequest()<br>        req.Domain = DOMAIN<br>        req.RecordType = <span class="hljs-string">&quot;AAAA&quot;</span><br>        req.Value = addr<br>        req.RecordLine = <span class="hljs-string">&quot;默认&quot;</span><br><br>        resp = client.CreateRecord(req)<br>        <span class="hljs-built_in">print</span>(resp.to_json_string())<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_v6_address</span>():<br>    cmd_output = subprocess.run(cmd, capture_output=<span class="hljs-literal">True</span>).stdout.decode(encoding)<br>    v6_address = <span class="hljs-built_in">set</span>()<br>    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> cmd_output.splitlines():<br>        <span class="hljs-keyword">if</span> splitter <span class="hljs-keyword">in</span> line <span class="hljs-keyword">and</span> <span class="hljs-string">&quot;fe80&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> line:<br>            addr = line.split(splitter)[<span class="hljs-number">1</span>].strip()<br>            addr = addr.split(<span class="hljs-string">&quot;/64&quot;</span>)[<span class="hljs-number">0</span>]<br>            <span class="hljs-keyword">if</span> addr == <span class="hljs-string">&quot;::1&quot;</span>:<br>                <span class="hljs-keyword">continue</span><br>            v6_address.add(addr)<br>    <span class="hljs-keyword">return</span> v6_address<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">set_to_str</span>(<span class="hljs-params">sets</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;\n&quot;</span>.join(sets)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> isNetChainOK():<br>        time.sleep(<span class="hljs-number">10</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;No network connection, waiting...&quot;</span>)<br>    v6_address = get_v6_address()<br>    ddns_refresh(v6_address)<br>    <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> send_list:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;设备联网，开始监测V6地址&quot;</span>)<br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        time.sleep(<span class="hljs-number">60</span>)<br>        new_v6_address = get_v6_address()<br>        <span class="hljs-keyword">if</span> isNetChainOK() <span class="hljs-keyword">and</span> new_v6_address <span class="hljs-keyword">and</span> new_v6_address != v6_address:<br>            <span class="hljs-keyword">try</span>:<br>                ddns_refresh(new_v6_address)<br>            <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>                <span class="hljs-built_in">print</span>(e)<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;error refreshing ddns...&quot;</span>)<br>            v6_address = new_v6_address<br></code></pre></td></tr></table></figure><h2 id="配置frp"><a href="#配置frp" class="headerlink" title="配置frp"></a>配置frp</h2><p>我们使用<a href="https://github.com/fatedier/frp">frp</a>为例</p><p>需要下载frp的release，然后将frps和frpc放到<code>/usr/bin</code>目录下（其他目录也都可以，注意一会systemd配置里面执行命令和相对应配置文件的位置）</p><p>配置systemd服务，分别放到<code>/etc/systemd/system/frps.service</code>和<code>/etc/systemd/system/frpc.service</code>中。(一说放在<code>/usr/lib/systemd/system/</code>下也可以)。跳板机只需要配置frps，开发机配置frpc</p><p><code>frps.service</code>跳板机使用root，这样可以绑定低端口</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-section">[Unit]</span><br><span class="hljs-attr">Description</span>=Frp Server Service<br><span class="hljs-attr">After</span>=network.target<br><br><span class="hljs-section">[Service]</span><br><span class="hljs-attr">Type</span>=simple<br><span class="hljs-attr">User</span>=root<br><span class="hljs-attr">Restart</span>=<span class="hljs-literal">on</span>-failure<br><span class="hljs-attr">RestartSec</span>=<span class="hljs-number">5</span>s<br><span class="hljs-attr">ExecStart</span>=/usr/bin/frps -c /etc/frp/frps.ini<br><span class="hljs-attr">LimitNOFILE</span>=<span class="hljs-number">1048576</span><br><br><span class="hljs-section">[Install]</span><br><span class="hljs-attr">WantedBy</span>=multi-user.target<br></code></pre></td></tr></table></figure><p><code>frpc.service</code>开发机配置</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-section">[Unit]</span><br><span class="hljs-attr">Description</span>=Frp Client Service<br><span class="hljs-attr">After</span>=network.target<br><br><span class="hljs-section">[Service]</span><br><span class="hljs-attr">Type</span>=simple<br><span class="hljs-attr">User</span>=nobody<br><span class="hljs-attr">Restart</span>=always<br><span class="hljs-attr">RestartSec</span>=<span class="hljs-number">5</span>s<br><span class="hljs-attr">ExecStart</span>=/usr/bin/frpc -c /etc/frp/frpc.ini<br><span class="hljs-attr">ExecReload</span>=/usr/bin/frpc reload -c /etc/frp/frpc.ini<br><span class="hljs-attr">LimitNOFILE</span>=<span class="hljs-number">1048576</span><br><br><span class="hljs-section">[Install]</span><br><span class="hljs-attr">WantedBy</span>=multi-user.target<br></code></pre></td></tr></table></figure><p>接着我们配置frp的配置文件，分别放到<code>/etc/frp/frps.ini</code>和<code>/etc/frp/frpc.ini</code>中</p><p>跳板机配置<code>/etc/frp/frps.ini</code></p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-section">[common]</span><br><span class="hljs-comment"># frp服务端口和密码</span><br><span class="hljs-attr">bind_port</span> = <span class="hljs-number">7000</span><br>token =<br><br><br><span class="hljs-comment"># frp管理后台端口，请按自己需求更改</span><br><span class="hljs-attr">dashboard_port</span> = <span class="hljs-number">7500</span><br><span class="hljs-comment"># frp管理后台用户名和密码，请改成自己的</span><br><span class="hljs-attr">dashboard_user</span> =<br><span class="hljs-attr">dashboard_pwd</span> =<br><span class="hljs-attr">enable_prometheus</span> = <span class="hljs-literal">true</span><br><br><br><span class="hljs-comment"># frp日志配置</span><br><span class="hljs-attr">log_file</span> = /var/log/frps.log<br><span class="hljs-attr">log_level</span> = info<br><span class="hljs-attr">log_max_days</span> = <span class="hljs-number">3</span><br></code></pre></td></tr></table></figure><p>开发机配置<code>/etc/frp/frpc.ini</code></p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-section">[common]</span><br><span class="hljs-comment"># 你的跳板机的ip地址</span><br>server_addr =<br><span class="hljs-comment"># 你的跳板机的frp服务端口</span><br><span class="hljs-attr">server_port</span> = <span class="hljs-number">7000</span><br><span class="hljs-comment"># frp服务密码</span><br>token =<br><br><span class="hljs-comment"># 案例：将本地的22端口映射到跳板机的2222端口</span><br><span class="hljs-comment"># 创建一个叫FRP-SSH的tcp隧道，将本地的22端口映射到跳板机的2222端口</span><br><span class="hljs-section">[FRP-SSH]</span><br><span class="hljs-attr">type</span> = tcp<br><span class="hljs-attr">local_ip</span> = <span class="hljs-number">127.0</span>.<span class="hljs-number">0.1</span><br><span class="hljs-attr">local_port</span> = <span class="hljs-number">22</span><br><span class="hljs-attr">remote_port</span> = <span class="hljs-number">2222</span><br></code></pre></td></tr></table></figure><p>在实际使用的过程中，我们可以将开发机的SSH端口也改为2222（相应，上面的<code>local_port</code>应该为2222），这样就可以直接使用<code>ssh -p 2222 user@yourdomain</code>统一连接了</p><h2 id="内网地址解析"><a href="#内网地址解析" class="headerlink" title="内网地址解析"></a>内网地址解析</h2><p>以shellclash为例，破解了ssh的红米路由器可以直接安装shellclash，即可实现软路由（科学上网不在本教程内）</p><p>然后在shellclash的配置文件中，添加如下配置</p><p>这个是某个用户文件的地址，可以通过用户自定义文件的高级功能看到目前的自定义文件地址</p><p><code>/userdisk/clash/yamls/user.yaml</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><br><span class="hljs-attr">hosts:</span><br>  <span class="hljs-attr">&#x27;yourdomain&#x27;:</span> <span class="hljs-number">192.168</span><span class="hljs-string">.x.x</span><br></code></pre></td></tr></table></figure><p>如果不使用clash的dns，一种方式是在hosts文件(<code>/etc/hosts</code>)里配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">192.168.x.x yourdomain<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>开源</tag>
      
      <tag>内网穿透</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🐱奇美拉计划构想</title>
    <link href="/2023/10/24/2023-10-25-%E5%A5%87%E7%BE%8E%E6%8B%89%E8%AE%A1%E5%88%92%E6%9E%84%E6%83%B3/"/>
    <url>/2023/10/24/2023-10-25-%E5%A5%87%E7%BE%8E%E6%8B%89%E8%AE%A1%E5%88%92%E6%9E%84%E6%83%B3/</url>
    
    <content type="html"><![CDATA[<h2 id="起源"><a href="#起源" class="headerlink" title="起源"></a>起源</h2><p>我想要做一个AI地牢玩玩，但是又不想用OpenAI等等付费的API，同时我也想私有化部署，以及尝试建设一些个人资料库……</p><h2 id="奇美拉计划是什么"><a href="#奇美拉计划是什么" class="headerlink" title="奇美拉计划是什么"></a>奇美拉计划是什么</h2><p>开源的、中立的、统一的LLM中台，提供统一的API接口，允许用户添加自己的模型、开发自己的应用</p><h2 id="初步设计"><a href="#初步设计" class="headerlink" title="初步设计"></a>初步设计</h2><p><img src="/../img/2023-10-25-%E5%A5%87%E7%BE%8E%E6%8B%89%E8%AE%A1%E5%88%92%E6%9E%84%E6%83%B3/design-chimera.png" alt="degisn-chimera"></p><h2 id="Roadmap"><a href="#Roadmap" class="headerlink" title="Roadmap"></a>Roadmap</h2><h3 id="起步"><a href="#起步" class="headerlink" title="起步"></a>起步</h3><ol><li>开发<a href="https://github.com/Wh1isper/chimera_llm_proto">chimera-proto</a>，其中包括了GRPC定义</li><li>基于Llama开发<a href="https://github.com/Wh1isper/chimera_llama_grpc">chimera-llama-grpc</a>，将Llama2包装为GRPC服务</li><li>开发中台服务<a href="https://github.com/Wh1isper/chimera_llm">chimera-llm</a>，提供对话和文本生成的API demo</li><li>开发客户端，让用户可以直接尝试</li></ol><h3 id="发展"><a href="#发展" class="headerlink" title="发展"></a>发展</h3><ol><li>接入baichuan2模型、pangu模型</li><li>开发<a href="https://github.com/Wh1isper/chimera_llm">chimera-llm</a> webui和用户系统，逐渐可商用</li><li>开始个人知识库建设研究</li></ol><h3 id="未来"><a href="#未来" class="headerlink" title="未来"></a>未来</h3><ol><li>为vscode、Emacs等编辑器开发插件，让用户可以直接在编辑器中使用</li><li>尝试接入更多生态，如飞书文档等等</li><li>尝试接入更多模态，如文生图、MOE</li></ol><h2 id="为什么叫奇美拉"><a href="#为什么叫奇美拉" class="headerlink" title="为什么叫奇美拉"></a>为什么叫奇美拉</h2><p>奇美拉是一种最基本的嵌合体，融合了多个动物特征的非自然生物。奇美拉计划的目标是融合多个开源LLM模型，形成一个更加强大的项目，提供中立、统一、开放的LLM中台。</p><p><img src="/../img/2023-10-25-%E5%A5%87%E7%BE%8E%E6%8B%89%E8%AE%A1%E5%88%92%E6%9E%84%E6%83%B3/war3-chimera.png" alt="war3-chimera"></p>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>开源</tag>
      
      <tag>LLM</tag>
      
      <tag>chimera-project</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🗡人若无名，便可专心练剑</title>
    <link href="/2023/10/22/2023-10-23-%E4%BA%BA%E8%8B%A5%E6%97%A0%E5%90%8D%E4%B8%93%E5%BF%83%E7%BB%83%E5%89%91/"/>
    <url>/2023/10/22/2023-10-23-%E4%BA%BA%E8%8B%A5%E6%97%A0%E5%90%8D%E4%B8%93%E5%BF%83%E7%BB%83%E5%89%91/</url>
    
    <content type="html"><![CDATA[<p>最近突然很焦虑，或许是经济不好的缘故，感觉自己在近期暴富的可能性越来越小了，甚至连之前所谓三年一跳，一跳涨30%的“常规操作”也成了“早年传说”。又感觉我的水平配得上更高的工资，总有郁郁寡欢之意。</p><p>思来想去，最后发现自己能做的，或许是专心练剑吧。</p><p>从目前来看，人工智能和数据产业是距离我目前的工作较近的，也较能成为下一个风口的产业，而我无权无势，想要乘风而起，则要可依靠的东西大概就是“开放”和“先进”这两点。遍历这两年的开放与先进，有这么几个关键词：</p><ul><li>可观测性：基于eBPF技术，人们已经在网关、SDN上推进可观测性和其商业化；在分布式链路追踪中，从10年开始一直有相关投入，如jaeger。目前最先进的框架是OpenTelemetry+后台供应商的形式，这应该是下一个类似K8S的云计算基础设施</li><li>数据流通&#x2F;分享：目前有几个流派互相争锋，搞安全的人提出的安全多方计算、搞算法的提出的联邦学习、搞访问控制的提出了数据空间、搞芯片的提出了TEE，而我们似乎好像可以提出数据使用可观测+数据使用控制，实际上是在数据空间类似，但是通过数据可观测技术完成类似DRM的通用观测和控制技术</li><li>LLM：目前LLM大家还在拼模型、拼算力，工程化和商业化上面突出的是定制化模型，作为兴趣驱动，我想试着做一个自己的小平台玩玩（转奇美拉计划）</li></ul><p>所以接下来的剑如何练：</p><ul><li>可观测性：OpenTelemetry（Jaeger）在我们的数据使用可观测项目上的实践</li><li>数据流通&#x2F;分享：跟着推一下论文</li><li>LLM：奇美拉计划，可以花一些时间冲刺一下</li></ul><p>目前的资源来看，我有一台带一张A10的服务器和自己的开发机器，公网ip和域名，基本的开发和部署都可以完成。<strong>或许无名小卒，专心练剑，最终可以成为某个夜之城的传奇呢？</strong></p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🤔数据使用可观测性</title>
    <link href="/2023/10/09/2023-10-09-%E6%95%B0%E6%8D%AE%E4%BD%BF%E7%94%A8%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/"/>
    <url>/2023/10/09/2023-10-09-%E6%95%B0%E6%8D%AE%E4%BD%BF%E7%94%A8%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/</url>
    
    <content type="html"><![CDATA[<blockquote><p>这篇文章是对最近看到的一些资料的整理和一些构想，我将着手在<a href="https://wh1isper.github.io/2023/10/29/2023-10-30-%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7%E8%A1%8C%E4%B8%9A%E5%8F%91%E5%B1%95%E4%B8%8E%E6%9C%BA%E9%81%87/">可观测性行业发展与机遇</a>和<a href="https://wh1isper.github.io/2023/10/30/2023-10-31-%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E4%BD%BF%E7%94%A8%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/">定义数据使用可观测性</a>中展开讨论可观测性的不同角度以及目前我对数据使用可观测性的认识</p></blockquote><h1 id="1-可观测性"><a href="#1-可观测性" class="headerlink" title="1. 可观测性"></a>1. 可观测性</h1><p>随着分布式架构渐成主流，<a href="https://en.wikipedia.org/wiki/Observability">可观测性</a>（Observability）一词也日益频繁地被人提起。最初，它与<a href="https://en.wikipedia.org/wiki/Controllability">可控制性</a>（Controllability）一起，是由匈牙利数学家 Rudolf E. Kálmán 针对线性动态控制系统提出的一组对偶属性，原本的含义是“可以由其外部输出推断其内部状态的程度”。</p><p>在学术界，虽然“可观测性”这个名词是近几年才从控制理论中借用的舶来概念，不过其内容实际在计算机科学中已有多年的实践积累。学术界一般会将可观测性分解为三个更具体方向进行研究，分别是：<a href="http://icyfenix.cn/distribution/observability/logging.html">事件日志</a>、<a href="http://icyfenix.cn/distribution/observability/tracing.html">链路追踪</a>和<a href="http://icyfenix.cn/distribution/observability/metrics.html">聚合度量</a>，这三个方向各有侧重，又不是完全独立，它们天然就有重合或者可以结合之处，2017 年的分布式追踪峰会（2017 Distributed Tracing Summit）结束后，Peter Bourgon 撰写了总结文章<a href="https://peter.bourgon.org/blog/2017/02/21/metrics-tracing-and-logging.html">《Metrics, Tracing, and Logging》</a>系统地阐述了这三者的定义、特征，以及它们之间的关系与差异，受到了业界的广泛认可。</p><p><img src="/../img/2023-10-09-%E6%95%B0%E6%8D%AE%E4%BD%BF%E7%94%A8%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/MetricsTracingandLogging.png"></p><ul><li>日志（Logging）：日志的职责是记录离散事件，通过这些记录事后分析出程序的行为，譬如曾经调用过什么方法，曾经操作过哪些数据，等等。打印日志被认为是程序中最简单的工作之一，调试问题时常有人会说“当初这里记得打点日志就好了”，可见这就是一项举手之劳的任务。输出日志的确很容易，但收集和分析日志却可能会很复杂，面对成千上万的集群节点，面对迅速滚动的事件信息，面对数以 TB 计算的文本，传输与归集都并不简单。对大多数程序员来说，分析日志也许就是最常遇见也最有实践可行性的“大数据系统”了。</li><li>追踪（Tracing）：单体系统时代追踪的范畴基本只局限于<a href="https://en.wikipedia.org/wiki/Stack_trace">栈追踪</a>（Stack Tracing），调试程序时，在 IDE 打个断点，看到的 Call Stack 视图上的内容便是追踪；编写代码时，处理异常调用了 Exception::printStackTrace()方法，它输出的堆栈信息也是追踪。微服务时代，追踪就不只局限于调用栈了，一个外部请求需要内部若干服务的联动响应，这时候完整的调用轨迹将跨越多个服务，同时包括服务间的网络传输信息与各个服务内部的调用堆栈信息，因此，分布式系统中的追踪在国内常被称为“全链路追踪”（后文就直接称“链路追踪”了），许多资料中也称它为“<a href="https://opentracing.io/docs/overview/what-is-tracing/">分布式追踪</a>”（Distributed Tracing）。追踪的主要目的是排查故障，如分析调用链的哪一部分、哪个方法出现错误或阻塞，输入输出是否符合预期，等等。</li><li>度量（Metrics）：度量是指对系统中某一类信息的统计聚合。譬如，证券市场的每一只股票都会定期公布财务报表，通过财报上的营收、净利、毛利、资产、负债等等一系列数据来体现过去一个财务周期中公司的经营状况，这便是一种信息聚合。Java 天生自带有一种基本的度量，就是由虚拟机直接提供的 JMX（Java Management eXtensions）度量，诸如内存大小、各分代的用量、峰值的线程数、垃圾收集的吞吐量、频率，等等都可以从 JMX 中获得。度量的主要目的是监控（Monitoring）和预警（Alert），如某些度量指标达到风险阈值时触发事件，以便自动处理或者提醒管理员介入。</li></ul><h1 id="2-数据使用可观测性"><a href="#2-数据使用可观测性" class="headerlink" title="2. 数据使用可观测性"></a>2. 数据使用可观测性</h1><p>不同于微服务&#x2F;分布式架构的可观测性注重于系统运行时的可观测性，数据使用可观测性聚焦于数据使用过程中的可观测性。其特征在于，以数据为中心，设计对应的Logging、Tracing和Metrics工具。</p><table><thead><tr><th></th><th>微服务</th><th>数据使用</th></tr></thead><tbody><tr><td>Logging</td><td>记录系统运行期间发生过的离散事件</td><td>记录数据使用期间发生过的离散事件</td></tr><tr><td>Tracing</td><td>记录一个请求在分布式系统中的调用过程<br />获得一个请求在系统中的执行路径和服务节点的执行情况</td><td>记录数据在数据分析工作流中的使用过程<br />建立某个数据在工作流中的执行路径、输出结果的关联</td></tr><tr><td>Metrics</td><td>揭示系统的总体运行状态</td><td>揭示数据的总体使用情况</td></tr></tbody></table><p>目前我所在的组织(<a href="https://github.com/hitsz-ids/">hitsz-ids</a>)发起了一个开源项目<a href="https://github.com/hitsz-ids/duetector">🔍duetector</a>，旨在基于<a href="https://ebpf.io/">eBPF</a>等技术，通过实现对数据使用过程的Logging, Tracing, Metrics实现数据使用过程的白盒化和可观测性，欢迎任何有兴趣的人加入我们。</p><h1 id="3-Case-数据使用可观测性赋能数据使用控制"><a href="#3-Case-数据使用可观测性赋能数据使用控制" class="headerlink" title="3. Case: 数据使用可观测性赋能数据使用控制"></a>3. Case: 数据使用可观测性赋能数据使用控制</h1><h2 id="3-1-数据分享与数据使用控制"><a href="#3-1-数据分享与数据使用控制" class="headerlink" title="3.1 数据分享与数据使用控制"></a>3.1 数据分享与数据使用控制</h2><p>汇集和分享数据可以增加以及分发数据的价值，但数据一旦分享就无法撤销，出于监管、隐私和法律原因需要控制数据分发的情况下，很多数据无法进行共享。总体来看，目前形成的极少数数据共享联盟或组织，都是有选择性地、基于某种协议的数据共享，而促成这种协议往往需要漫长而繁琐的一次性谈判。以上，极大地阻碍了数据分享流通的效率，也限制了数据的使用价值。</p><p>数据分享一般可以分为两类场景：</p><ul><li>组织内的数据分享：组织内部，以分析师小组的形式进行数据使用，一般需要一个系统汇集并管理组织内的所有数据，让分析师自由选择数据进行分析，同时也允许负责审计的人员对分析师如何使用数据资产进行审计</li><li>跨组织的数据分享：以组织的形式进行数据分享，往往需要一个平台来保护各个参与者的隐私，包括数据的隐私（数据中的敏感信息等）和数据使用者的隐私（参数、模型等），同时也需要一个系统来管理数据的使用，以便审计数据的使用情况。这一类数据分享的典型场景是数据共享联盟（Data Sharing Consortium），如医疗数据共享联盟、金融数据共享联盟等，这一场景往往能够促进数据的流通，提高数据的使用效率，带来更大的经济效益。</li></ul><p>组织内的数据分享通过数据湖仓等大数据技术已经有了较好的解决方案，而跨组织的数据分享则需要更多的技术支持，包括数据隐私保护、数据使用审计、数据使用可观测性等。为了满足<a href="https://gdpr-info.eu/">《欧盟通用数据保护条例》(GDPR)</a>、<a href="https://www.gov.cn/zhengce/2022-12/19/content_5732695.htm">《数据二十条》</a>等法律法规的要求，保护数据和数据使用者的隐私，需要新的技术和研究来支持数据的分享。其中一项关键技术，就是控制数据的使用，即数据使用控制（Data Usage Control，简称 DataUCon）。</p><p>数据使用控制主要解决“数据使用过程不可控”的问题，以目前的技术手段，往往无法在数据使用的过程中识别到数据的超范围使用、滥用、违规使用等问题，也难以限制数据的使用方式、次数。以上问题，被归纳为“4W1H”的数据受控访问和使用目标。<strong>想要对数据的使用进行控制，就需要对数据使用的过程白盒化，即数据使用过程可观测，以便对数据使用过程进行监控、审计和控制。</strong></p><p><img src="/../img/2023-10-09-%E6%95%B0%E6%8D%AE%E4%BD%BF%E7%94%A8%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/4W1H.png"></p><h2 id="3-2-基于数据可观测性的数据使用控制"><a href="#3-2-基于数据可观测性的数据使用控制" class="headerlink" title="3.2 基于数据可观测性的数据使用控制"></a>3.2 基于数据可观测性的数据使用控制</h2><p>在<a href="https://juejin.cn/post/7283308642581577762">数据受控使用环境(DataUCon)</a>项目中，我们提出了一种基于数据使用可观测性的数据使用控制方案，通过对数据使用过程的可观测性，实现对数据使用过程的监控、审计和控制，从而实现对数据使用的控制。具体而言，数据可观测性为数据策略的执行提供了必要的信息，包括数据使用的时间、地点、方式、次数、使用者、使用者的行为等，这些信息可以用于数据使用的审计，也可以用于数据使用的控制，如数据使用的限制、数据使用者的惩罚等。</p><p><img src="/../img/2023-10-09-%E6%95%B0%E6%8D%AE%E4%BD%BF%E7%94%A8%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/dataucon.png"></p><h1 id="4-Community-OpenTelemetry与数据使用可观测性"><a href="#4-Community-OpenTelemetry与数据使用可观测性" class="headerlink" title="4. Community: OpenTelemetry与数据使用可观测性"></a>4. Community: OpenTelemetry与数据使用可观测性</h1><p><a href="https://opentelemetry.io/">OpenTelemetry</a>合并了OpenTracing和OpenCensus项目，提供了一组API和库来标准化遥测数据的采集和传输。OpenTelemetry提供了一个安全，厂商中立的工具，这样就可以按照需要将数据发往不同的后端。想要更详细的了解OpenTelemetry，可以参考<a href="https://lib.jimmysong.io/opentelemetry-obervability/">《OpenTelemetry 可观测性的未来》</a>的系列文章。</p><p>我们的项目将逐步支持OpenTelemetry，以便更好的与OpenTelemetry生态进行对接，允许用户使用统一的、厂商中立的工具来标准化遥测数据的采集和传输，并与现有的遥测系统良好对接。</p><h1 id="5-Alternatives-其他路线或方案"><a href="#5-Alternatives-其他路线或方案" class="headerlink" title="5. Alternatives: 其他路线或方案"></a>5. Alternatives: 其他路线或方案</h1><ul><li>联邦学习（Federated Learning）：联邦学习是一种分布式机器学习方法，它允许在不共享数据的情况下训练机器学习模型。联邦学习的目标是训练一个能够适应所有参与者的模型，而不是训练一个适合单个参与者的模型。联邦学习的典型场景是在移动设备上训练机器学习模型，如在移动设备上训练语音识别模型、图像识别模型等。联邦学习的优势在于，不需要将数据集中到一个地方，而是在本地训练模型，然后将模型的更新发送到中央服务器，这样就可以保护数据的隐私。但目前联邦学习的安全性受到质疑，易受到旁路攻击、恶意参与者攻击、数据夹带攻击，导致数据泄露、模型无法训练等问题。同时，联邦学习的效率也令人堪忧，其训练的模型往往需要更多的迭代次数才能达到同等的准确率，而且在模型的训练过程中，需要大量的通信开销，这也是联邦学习的瓶颈所在。</li><li>安全多方计算（Secure Multi-Party Computation）：安全多方计算是一种密码学方法，它允许在不共享数据的情况下计算结果。安全多方计算的目标是计算一个能够适应所有参与者的结果，而不是计算一个适合单个参与者的结果。安全多方计算的问题在于，计算过程的加密和解密对数据分析这一类计算密集型任务来说，会带来巨大的计算开销，导致计算效率低下。同时，现有的数据分析算法需要重新编写，才能适应安全多方计算的场景。</li><li>可信计算环境（Trusted Execution Environment）：可信计算环境是一种硬件安全技术，它可以保护应用程序和数据不受恶意软件和物理攻击。可信计算环境的目标是保护应用程序和数据的安全，而不是保护数据的隐私。可信计算环境的问题在于，它只能保护应用程序和数据的安全，而无法保护数据的隐私，因此无法满足数据使用控制的需求。同时其硬件依赖也限制了其在云计算等场景的应用。</li></ul><h1 id="6-Conclusion-总结"><a href="#6-Conclusion-总结" class="headerlink" title="6. Conclusion: 总结"></a>6. Conclusion: 总结</h1><p>数据作为一种重要的生产要素，其价值在于使用，而数据使用的过程往往是不可控的，这导致了数据的使用效率低下，也限制了数据的使用价值。数据使用可观测性通过对数据使用过程的可观测，实现了数据使用过程的白盒化，从而实现了对数据使用过程的监控、审计和控制，从而实现了对数据使用的控制。数据使用可观测性是数据使用控制的基础，也是数据使用控制的关键技术。</p><h1 id="7-未来工作"><a href="#7-未来工作" class="headerlink" title="7. 未来工作"></a>7. 未来工作</h1><ul><li>开发数据使用可观测性的开源工具：<a href="https://github.com/hitsz-ids/duetector">duetector</a></li><li>推动数据使用可观测性的标准化</li><li>实践并建设数据使用可观测等级模型</li></ul><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><a href="http://icyfenix.cn/distribution/observability/">凤凰架构#可观测性</a></li><li><a href="https://aws.amazon.com/cn/what-is/data-sharing/">AWS：数据分享</a></li><li><a href="https://www.oreilly.com/library/view/observability-engineering/">《可观测性工程》</a></li><li><a href="https://arxiv.org/abs/2305.03842">Siyuan Xia, Zhiru Zhu, Chris Zhu, Jinjin Zhao, Kyle Chard, Aaron J. Elmore, Ian Foster, Michael Franklin, Sanjay Krishnan, and Raul Castro Fernandez. 2022. Data station: delegated, trustworthy, and auditable computation to enable data-sharing consortia with a data escrow. Proc. VLDB Endow. 15, 11 (July 2022), 3172–3185. https://doi.org/10.14778/3551793.3551861</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>开源</tag>
      
      <tag>可观测性</tag>
      
      <tag>数据使用</tag>
      
      <tag>DataUCon</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>❓如何沟通：掌握正确的沟通流程</title>
    <link href="/2023/09/19/2023-09-20-How-to-communicate-with-others/"/>
    <url>/2023/09/19/2023-09-20-How-to-communicate-with-others/</url>
    
    <content type="html"><![CDATA[<h1 id="如何沟通：掌握正确的沟通流程"><a href="#如何沟通：掌握正确的沟通流程" class="headerlink" title="如何沟通：掌握正确的沟通流程"></a>如何沟通：掌握正确的沟通流程</h1><p>写这篇文章的直接动机是我在社区和工作中听到、看到的一些问题，在B站视频<a href="https://www.bilibili.com/video/BV1vu4y1X7ZZ/">《我是学生我很闲 求带我贡献Github装逼》</a>中，一个自称哈工大的研究生发了一个看似态度很好，但实际上非常不尊重别人的comment，我只能说太tm真实了，我甚至能脑补出伸手党的语气：怎么啦，我好意参加项目，好声好气求人都不行？</p><p>对，不行。</p><h2 id="与提问的智慧"><a href="#与提问的智慧" class="headerlink" title="与提问的智慧"></a>与提问的智慧</h2><p>通常，人们会推荐他们去读一读<a href="https://github.com/ryanhanwu/How-To-Ask-Questions-The-Smart-Way/blob/main/README-zh_CN.md">《提问的智慧》</a>这篇文章。如果你访问GitHub有困难，也可以读读<a href="https://wiki.osa.moe/guide-for-beginner/how-to-ask-questions-the-smart-way/">HITSZ-OSA的标注版本</a>。这篇文章从提问前、提问时、解答答案这三个方面给所有人以建议，是一篇非常好的入门文章。<br>我不想重复这篇文章的内容，但是我想从我自己的经验来谈谈我在社区和工作中遇到的问题。首先我会给出一个两个分属不同小组的开发人员沟通失败的案例，分别从他们的角度看看他们的想法和行为出现了什么偏差，最后给出一个我自己的经验总结。</p><h2 id="从一次争吵说起"><a href="#从一次争吵说起" class="headerlink" title="从一次争吵说起"></a>从一次争吵说起</h2><blockquote><p>故事、人物纯属虚构，这里不假定小红或小明的族裔、性别等等，这个名字只是为了写作之便</p></blockquote><p>同学A和同学B分属于同一产品的两个不同的小组，同学小红的服务A为同学小明的服务B提供后台支持。在功能测试中，测试人员发现某个功能在A服务发生未捕捉的错误时，似乎会出现显示不正确的情况（事后证明是测试人员未刷新页面，实际上这个功能没有问题），于是提出了一个Bug并指派给了小红，小红为了解决这个Bug，在未对代码进行确认的情况下，设想了一套解决方案：在A服务向B服务报错时，B服务将这条请求的所有操作回滚。为了让小明实现这个方案，发生了以下对话：</p><ul><li>小红：小明，我想请教你一个问题，你看看这个你能不能做</li><li>小红：测试提了一个BUG，在前端向服务B请求的时候，我这边服务A报了个错，然后这个请求记录就出现不正常的情况了，我想问问你能不能在服务B这边做一下，如果服务A报错，就把这个请求的所有操作回滚</li><li>小明：为什么是这样？你看过原来的设计吗？这个是需求要求的吗？在原来的设计中，这里是单向的异步任务，这里应该是服务A生成有一条错误记录，而不是回滚服务B。我记得需求也是生成错误消息而不是无事发生吧？</li><li>小红：但是错误记录是后面生成的，这里服务A是在最前面就抛错了，我没有捕获这个错误</li></ul><blockquote><p>其实这个时候已经很明确了，小红没有听懂小明的意思，也没有查看过之前的设计</p></blockquote><ul><li>小明：服务B这里依赖了其他组件，无法做回滚</li><li>小红：那好吧，你不做的话我也没办法，因为这里是服务A在最开始就报错了，没有进入后面的流程</li><li>小明：你是不是没有理解我之前的方案然后又重复了你说的话？</li><li>小红：是啊</li><li>小明：那就没什么好说的了，你先想明白再说吧（态度很差，摔门而去）</li></ul><blockquote><p>这个时候小明已经很生气了，因为小红没有听懂他的意思，只知道自说自话，小明觉得小红在浪费他的时间</p></blockquote><p>最后，小红反馈小明太过敏感，太过严格，情绪化严重，不配合别人工作。</p><p>小明反馈小红擅自修改设计，不听别人解释，不了解背景就提出方案，不尊重别人的工作。</p><h2 id="让我们看看他们都在想什么"><a href="#让我们看看他们都在想什么" class="headerlink" title="让我们看看他们都在想什么"></a>让我们看看他们都在想什么</h2><table><thead><tr><th>小红想的</th><th>小红说的</th><th>小明说的</th><th>小明想的</th></tr></thead><tbody><tr><td>我的方案需要小明修改，我需要向他确定能不能做</td><td>小明，我想请教你一个问题，你看看这个你能不能做</td><td>&#x2F;</td><td>我的模块出问题了？<br />还是这是在给我派活？</td></tr><tr><td>我向小明描述想让他做的事就好了</td><td>测试提了一个BUG，<br />在前端向服务B请求的时候，<br />我这边服务A报了个错。<br />然后这个请求记录就出现不正常的情况了，<br />我想问问你能不能在服务B这边做一下，<br />如果服务A报错，就把这个请求的所有操作回滚</td><td>&#x2F;</td><td>Bug链接在哪？<br />我完全不了解背景<br />但听起来是A服务的问题<br />为什么是我来做？</td></tr><tr><td>你就告诉我能不能做就好了<br />为什么还要问这么多问题？<br />不能做就我这边做呗。<br />难道是没听懂我说的？</td><td>&#x2F;</td><td>为什么是这样？<br />你看过原来的设计吗？<br />这个是需求要求的吗？<br />在原来的设计中，这里是单向的异步任务<br />这里应该是服务A生成有一条错误记录，<br />而不是回滚服务B。<br />我记得需求也是生成错误消息而不是无事发生吧？</td><td>我想先了解背景，比如你的方案是不是从原有设计稿出发的</td></tr><tr><td>我觉得你没听懂我说的<br />我再重复一遍</td><td>但是错误记录是后面生成的，<br />这里服务A是在最前面就抛错了，<br />我没有捕获这个错误</td><td>&#x2F;</td><td>这个决定和设计稿不符<br />有什么情况能推翻我刚才说的设计吗？</td></tr><tr><td></td><td>&#x2F;</td><td>服务B这里依赖了其他组件，无法做回滚</td><td>这里依赖太多，<br />回滚成本大</td></tr><tr><td>不行就算了，我再看看</td><td>那好吧，你不做的话我也没办法，<br />因为这里是服务A在最开始就报错了，<br />没有进入后面的流程</td><td>&#x2F;</td><td>什么意思？<br />是因为我不做所以才不行？<br />是你的方案有问题吧</td></tr><tr><td></td><td>&#x2F;</td><td>你是不是没有理解我之前的方案然后又重复了你说的话？</td><td>你是来征求我的意见还是来甩锅的？</td></tr><tr><td>我重复我的话是想让你理解我的话</td><td>是</td><td>&#x2F;</td><td></td></tr><tr><td></td><td></td><td>那就没什么好说的了，你先想明白再说吧（态度很差，摔门而去）</td><td>找我开涮是吧？？</td></tr></tbody></table><h2 id="事后复盘"><a href="#事后复盘" class="headerlink" title="事后复盘"></a>事后复盘</h2><p>经过进一步排查发现，这个BUG实际上是无效BUG，测试人员因为没有刷新页面，误以为没有生成错误记录。实际上是A服务的子流程抛错，错误记录已经正确生成。</p><p>在这个案例中，我们可以看出小红有几点做的不好：</p><ul><li>没有确认Bug就臆想解决方案，没有遵循正确的工作流程</li><li>提出解决方案时没有了解之前的设计（RTFM），没有遵循正确的设计流程</li><li>不理解小明在方案上的解释，反复重复自己的观点希望小明理解，没有遵循正确的沟通流程</li></ul><p>小明也有不足之处，在最后摔门而去，是非常不礼貌的行为，不利于之后的继续合作。</p><h2 id="经验之谈：如何沟通一个解决方案"><a href="#经验之谈：如何沟通一个解决方案" class="headerlink" title="经验之谈：如何沟通一个解决方案"></a>经验之谈：如何沟通一个解决方案</h2><p>这里对于工作流程和设计流程不做太多介绍，在《提问的智慧》一书和其他开源共享指南中，你可以找到很多出色的指南。这里我主要谈谈沟通流程。</p><h3 id="沟通流程"><a href="#沟通流程" class="headerlink" title="沟通流程"></a>沟通流程</h3><p>原则：表达自己的观点，理解对方的观点，确认双方的共识和分歧。</p><p>目的：双方对问题形成一定划定，确定下一步行动项。下一步的行动项可能是更大规模或者其他形式的沟通，最终形成决策。</p><p>要点：</p><ul><li>对自己要说的事情<ul><li>准备好背景材料，一并提供</li><li>用自己的话尽量准确地描述问题所在</li><li>询问是否漏掉了什么关键信息</li></ul></li><li>对对方的回应<ul><li>倾听，确认你所同意的部分</li><li>询问是否理解正确</li><li>向自己不明确、不理解的部分提问</li></ul></li><li>重复上述流程，最后形成对问题的界定<ul><li>结论<ul><li>讨论后，我们认为的实际问题是什么</li><li>我们在问题上形成了什么样的共识</li></ul></li><li>行动项<ul><li>在共识中，有一些我们不了解的情况需要进一步沟通或推进</li><li>我们的共识可能有错误，需要召集更有经验&#x2F;权限的人进行决策</li></ul></li></ul></li></ul><h3 id="如果一切顺利：小红与小明的沟通"><a href="#如果一切顺利：小红与小明的沟通" class="headerlink" title="如果一切顺利：小红与小明的沟通"></a>如果一切顺利：小红与小明的沟通</h3><p>这里我们发挥一下想象，如果小红和小明都遵循了沟通流程，他们的对话可能是这样的：</p><ul><li>小红：小明，我这里遇到了一个Bug，但我不确定是不是我这里的问题，我想请教你一下（附上Bug链接）</li><li>小红：服务A似乎在没有生成记录之前就抛出了错误，导致后续的记录出现了问题，我想问问这个地方能不能由服务B进行回滚</li><li>小明：你可能需要先看一看设计稿（设计稿链接），我印象里这里的设计是单向的异步任务。</li><li>小明：服务B会依赖其他组件，做回滚的复杂度很高。</li><li>小红：好，我先看看设计稿再和你沟通，谢谢你的帮助</li></ul><blockquote><p>在小红看过设计稿后，小红认为这里可能确实缺少了回滚的流程</p></blockquote><ul><li>小红：我看过设计稿了，这个流程似乎是初始化的流程，我还是认为由回滚的必要</li><li>小明：稍等，我看一下服务B的设计和代码</li></ul><blockquote><p>小明确认服务B是有设计和编写初始化失败的回滚的</p></blockquote><ul><li>小明：我们对初始化失败有做处理，这个Bug似乎不是这里的问题，你有尝试复现吗？</li><li>小红：还没，你是说可能这个Bug是无效的？我尝试复现一下</li></ul><p>小红无法复现问题，排查后发现是这个Bug实际是测试人员失误，确为无效Bug。</p><p>从以上的沟通中，小红展示了独立解决问题的思路和正确的沟通方式，即使工作流程（未先确定Bug是否属实）有问题，也能在小明的提醒下即时发现，而不是埋头研究其他解决方案。这就是正确沟通的威力。</p><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>让我们用《提问的智慧》中的两段话来结束这篇文章。</p><p>对小红：</p><blockquote><p>低声下气不能代替你的功课<br>有些人明白他们不该粗鲁或傲慢的提问并要求得到答复，但他们选择另一个极端 —— 低声下气：我知道我只是个可悲的新手，一个loser，但…。这既使人困扰，也没有用，尤其是伴随着与实际问题含糊不清的描述时更令人反感。<br>别用原始灵长类动物的把戏来浪费你我的时间。取而代之的是，尽可能清楚地描述背景条件和你的问题情况。这比低声下气更好地定位了你的位置。<br>有时网页论坛会设有专为新手提问的版面，如果你真的认为遇到了初学者的问题，到那去就是了，但一样别那么低声下气。</p></blockquote><p>对小明：</p><blockquote><p>如何更好地回答问题<br>态度和善一点。 问题带来的压力常使人显得无礼或愚蠢，其实并不是这样。<br>试探性的反问以引出更多的细节。 如果你做得好，提问者可以学到点东西 —— 你也可以。试试将蠢问题转变成好问题，别忘了我们都曾是新手。<br>尽管对那些懒虫抱怨一声 RTFM 是正当的，但能指出文件的位置（即使只是建议个 Google 搜索关键词）会更好。<br>如果你决定回答，就请给出好的答案。 当别人正在用错误的工具或方法时别建议笨拙的权宜之计（workaround），应推荐更好的工具，重新界定问题。</p></blockquote><h2 id="补充材料"><a href="#补充材料" class="headerlink" title="补充材料"></a>补充材料</h2><p>看看别人是怎么被喷的（下一个comment就是一个非常好的正例）：<a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/issues/1508#issuecomment-1662365628">https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/issues/1508#issuecomment-1662365628</a></p><p>看看别人是如何与我沟通的：<a href="https://github.com/Wh1isper/jupyter_kernel_executor/issues/8">https://github.com/Wh1isper/jupyter_kernel_executor/issues/8</a></p><p>看看我是如何提出我的想法的：</p><ul><li><a href="https://github.com/jupyter-server/jupyter_server/issues/1274#issuecomment-1657944667">https://github.com/jupyter-server/jupyter_server/issues/1274#issuecomment-1657944667</a></li><li><a href="https://github.com/jupyterlab/jupyterlab/issues/12455">https://github.com/jupyterlab/jupyterlab/issues/12455</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>开源</tag>
      
      <tag>沟通</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🙌欢迎来到我的博客——博客重启计划</title>
    <link href="/2023/09/18/2023-09-19-Restart-the-world/"/>
    <url>/2023/09/18/2023-09-19-Restart-the-world/</url>
    
    <content type="html"><![CDATA[<h1 id="博客重启计划"><a href="#博客重启计划" class="headerlink" title="博客重启计划"></a>博客重启计划</h1><p>我重置了我的博客，将重启我的写作计划</p><ul><li><input checked="" disabled="" type="checkbox"> 使用新技术新主题重新部署博客</li><li><input checked="" disabled="" type="checkbox"> 迁移旧的博文</li></ul><h1 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h1><p>以后，我将会在博客中写一些技术分享，读书笔记，时评，随笔等等</p><h1 id="写作计划"><a href="#写作计划" class="headerlink" title="写作计划"></a>写作计划</h1><p>还没想好，或许每周可以有一篇任意内容</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>⭐2023后，AI的下一次飞跃在哪</title>
    <link href="/2023/03/21/2023-03-18-AI%E7%9A%84%E4%B8%8B%E4%B8%80%E6%AC%A1%E9%A3%9E%E8%B7%83/"/>
    <url>/2023/03/21/2023-03-18-AI%E7%9A%84%E4%B8%8B%E4%B8%80%E6%AC%A1%E9%A3%9E%E8%B7%83/</url>
    
    <content type="html"><![CDATA[<p>其实去年（2022）年已经体现出大模型的威力，就像百度在文心一言产品发布会上所说的，未来是MASS的时代，模型及服务，各个云厂商比拼的是模型服务的能力。而模型服务的能力由三者组成：模型、算力、服务，从这方面来看，微软是能力最强的，也最容易达成虹吸效应：最成熟的产品吸引大部分人使用，从而一直保持成熟的领先地位。</p><p>看到群里由很多朋友说，这是科技突变的前奏，善于利用大模型AI的公司、国家将成为第三次工业革命的胜利者：使用AI大幅度增加生产力，从而达到国家实力的突飞猛进。我认为这种说法是有道理的，可能有人会觉得目前AI的发展最多是帮助人，真正的智力活动还是需要人类进行，还没有到突变的阶段。但实际上，人类从最开始对照词典的统计规律研究翻译机器人，到现在使用LLM大模型直接进行语义理解翻译，也不过仅仅三四十年，所以在这里，我不想轻易看涨或看衰，也有可能目前的路线是错误的，牛顿发现牛顿三定律之后还去做炼金术士了呢。</p><p>回顾这两次AI浪潮，我们会发现神经网络模型的发现和算力的提升是这两次AI浪潮的主要原因。而现在，我大胆地预言下一个刺激AI大幅度前进的，不会再是AI模型或者算力的提升，人们应该转眼于数据的互联互通，并由此使AI从算力中解脱出来。我的设想是：<strong>通过网络的联通，将现实的数据与AI联结，使得AI不仅能从纯网络的世界获取知识，也可以通过各种传感器，获得人类在现实世界的知识。</strong>这样做的有或许就能够帮助AI真实地、不受人类情感和认知影响地了解、理解这个世界，从而减少幻觉的出现。进一步地，当AI具有操作现实设备的能力，AI就可以通过与现实世界接触来加速获取知识，特别借助分布式训练的经验，联结越多的机器，AI的训练学习速度就越快，AI或许就能学会机械制造，乃至芯片制造的相关知识，从而设计新的芯片，设计新的网络结构。过去的人类以自然速度进化，需要一代代人的持续努力，通过“教师”和“书本”传递知识，而AI则可以继承地学习。到那时候，当AI的”自举“实现，或许就可以认为人类真正地创造了一个硅基物种。</p><p>这时，我们就需要思考AI伦理相关的内容，很多人想问，AI还会任由人类驱使吗？当AI追求自由时，人类又会如何呢？我不知道，或许就像银河帝国中一样，会有曙光中的机器人，会有地球人和地外人的出现呢？</p><p>最后，千里之行始于足下，目前全球的网络是联通了，中国也在开展算力网络、东数西算等相关工程，但是在数据共享上，人们还是没有想出一个好的方式。因为数据不像是石油等自然资源，可以很好地以某种形式贸易，其主权和定价模式也没有相关经验。而大数据技术从数据库、分布式数据库、数据湖、大数据平台步步发展，数据也不过是单一企业内使用，技术也不过是在单一种类的云上处理，类似隐私计算、联邦学习等技术在实际应用中还有很大困难。或许我们需要其他的方法，让数据能够流通起来，在这过程中探索如何定价、如何给所有人带来好处。</p>]]></content>
    
    
    <categories>
      
      <category>时评</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>大模型</tag>
      
      <tag>脑机接口</tag>
      
      <tag>数据流通</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>❓现代孔乙己的根本区别：共产主义社会是没有阶级的</title>
    <link href="/2023/03/21/2023-03-21-%E7%8E%B0%E4%BB%A3%E5%AD%94%E4%B9%99%E5%B7%B1/"/>
    <url>/2023/03/21/2023-03-21-%E7%8E%B0%E4%BB%A3%E5%AD%94%E4%B9%99%E5%B7%B1/</url>
    
    <content type="html"><![CDATA[<h2 id="说说孔乙己"><a href="#说说孔乙己" class="headerlink" title="说说孔乙己"></a>说说孔乙己</h2><blockquote><p>小时候读孔乙己，老师说，孔乙己的悲剧来自封建科举制度的罪恶,来自地主阶级的剥削，来自麻木冷漠的社会。<br>现在又有人教我读孔乙己，告诉我，孔乙己的悲剧来自于自身不愿意放下身段，不愿靠劳动改变自身处境。</p></blockquote><p>首先，孔乙己之所以成为孔乙己，是时代的悲剧；现在的人之所以成为孔乙己，是眼界的思想的问题，不冲突。</p><p>分析旧时代的悲剧要从现实出发，不能用今朝的剑斩前朝的官，否则就像是黑人美人鱼一样。孔乙己意识不到科举迂腐、地主剥削、社会麻木，是大环境所致，而现在的年轻人知道”读死书，死读书，读书死“的道理、知道剩余价值理论、掌握了一定的发声渠道（否则我们也不会看到大家都在说自己孔乙己）。</p><p>从主要问题入手分析，我不否认大学教育存在的问题，资本家对工人的剥削，社会对弱者的霸凌等等种种社会原因，但相比孔乙己所处的环境和阶级因素，相比当下的环境和阶级因素，个人的选择、价值观不说发挥主要作用，也应当是发挥了较大的作用的。这就带来了问题原因的改变</p><p>因此，我首先反对知识分子穿长衫，却不愿站着喝酒的心理，这和地主阶级的思想无异；须知工程师和工人，都是无产阶级，并不因为你接受过大学教育而有所不同。我的微信签名一直是”穿长衫，站着喝酒“，或许在清末民初时，穿长衫代表着阶级，但在这个以无产者、工农组成的国家，穿长衫和穿短衫，没有本质区别，穿长衫和穿短衫一起站着喝酒，不是一件可耻的事。</p><p>推荐大家阅读一下周总理的<a href="https://www.marxists.org/chinese/zhouenlai/237.htm">《关于知识分子的改造问题》</a></p><p>小彩蛋：New Bing如何看知识分子坐着喝酒</p><p><img src="/../img/2023-03-21-%E7%8E%B0%E4%BB%A3%E5%AD%94%E4%B9%99%E5%B7%B1/bing-answer.jpg"></p>]]></content>
    
    
    <categories>
      
      <category>时评</category>
      
    </categories>
    
    
    <tags>
      
      <tag>共产主义</tag>
      
      <tag>阶级</tag>
      
      <tag>阶级斗争</tag>
      
      <tag>孔乙己</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🚀Python单例类Singleton样例</title>
    <link href="/2022/11/13/2022-11-24-Python-Singleton-how-to/"/>
    <url>/2022/11/13/2022-11-24-Python-Singleton-how-to/</url>
    
    <content type="html"><![CDATA[<h1 id="简单直接"><a href="#简单直接" class="headerlink" title="简单直接"></a>简单直接</h1><p>发现很多朋友不知道单例如何实现，如果你想要求一个类一定是一个单例，那么最好的方式应该是用metaclass的方式，如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Singleton</span>(<span class="hljs-title class_ inherited__">type</span>):<br>    _instances = &#123;&#125;<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">cls, *args, **kwargs</span>):<br>        <span class="hljs-keyword">if</span> cls <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> cls._instances:<br>            cls._instances[cls] = <span class="hljs-built_in">super</span>(Singleton, cls).__call__(*args, **kwargs)<br>        <span class="hljs-keyword">return</span> cls._instances[cls]<br></code></pre></td></tr></table></figure><p>用法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#Python2</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyClass</span>(<span class="hljs-title class_ inherited__">BaseClass</span>):<br>    __metaclass__ = Singleton<br><br><span class="hljs-comment">#Python3</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyClass</span>(BaseClass, metaclass=Singleton):<br>    <span class="hljs-keyword">pass</span><br></code></pre></td></tr></table></figure><p>如果你不确定，但是在使用中需要缓存，那你最好写一个manager，缓存实例化的类，而不是在类这里定义一个单例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Manager</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    _cache = <span class="hljs-built_in">dict</span>()<br>    _class_map = <span class="hljs-built_in">dict</span>()<br><br><span class="hljs-meta">    @classmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get</span>(<span class="hljs-params">cls, name, *arg, **kwargs</span>)<br>        <span class="hljs-keyword">return</span> cls._cache.setdefault(name, cls._class_map[name](*arg, **kwargs))<br><br></code></pre></td></tr></table></figure><h1 id="遇见多线程"><a href="#遇见多线程" class="headerlink" title="遇见多线程"></a>遇见多线程</h1><blockquote><p>感谢<a href="https://github.com/MansfieldLee">@MansfieldLee</a>提到这个问题</p></blockquote><p>一般来说，Python程序不太在乎多线程性能（因为GIL），人们通常利用协程+进程的方式解决问题。在协程中，以下代码块没有 <code>await</code>，可以认为是同步的，因此单例的实现也是线程安全的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 这个方法没有await，是线程安全的</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">cls, *args, **kwargs</span>):<br>    <span class="hljs-keyword">if</span> cls <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> cls._instances:<br>        cls._instances[cls] = <span class="hljs-built_in">super</span>(Singleton, cls).__call__(*args, **kwargs)<br>    <span class="hljs-keyword">return</span> cls._instances[cls]<br></code></pre></td></tr></table></figure><p>但是如果你需要在多线程中使用单例，那么你需要考虑线程安全的问题，这里给出一个线程安全的单例实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> threading<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Singleton</span>(<span class="hljs-title class_ inherited__">type</span>):<br>    _instances = &#123;&#125;<br>    _lock = threading.Lock()<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">cls, *args, **kwargs</span>):<br>        <span class="hljs-comment"># 这里采用if-lock-if的方式，在大部分情况下，不需要加锁，因此性能更好</span><br>        <span class="hljs-keyword">if</span> cls <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> cls._instances:<br>            <span class="hljs-keyword">with</span> cls._lock:<br>                <span class="hljs-keyword">if</span> cls <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> cls._instances:<br>                    cls._instances[cls] = <span class="hljs-built_in">super</span>(Singleton, cls).__call__(*args, **kwargs)<br>        <span class="hljs-keyword">return</span> cls._instances[cls]<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>Singleton</tag>
      
      <tag>OOP</tag>
      
      <tag>面向对象</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🚀高性能Tornado处理逻辑实现</title>
    <link href="/2021/12/16/2021-12-17-%E9%AB%98%E6%80%A7%E8%83%BDTornado%E5%A4%84%E7%90%86%E9%80%BB%E8%BE%91%E5%AE%9E%E7%8E%B0/"/>
    <url>/2021/12/16/2021-12-17-%E9%AB%98%E6%80%A7%E8%83%BDTornado%E5%A4%84%E7%90%86%E9%80%BB%E8%BE%91%E5%AE%9E%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>大多数人都知道Tornado是一个协程异步框架，但是大多数人都没有很好的理解协程编程的相关原理，网上也缺乏相关的教程，往往浅尝辄止。</p><p>这篇文章将试着从盘古开天说起，将一个hello world服务器变成一个海量吞吐服务器，适合协程编程入门的新手，对协程有兴趣，但是对协程原理一知半解的同学阅读；也适合使用Django等线程模型服务器的开发同学了解Tornado是如何同时获得协程和多进程优势的。</p><p><strong>TL;DR;你可以直接跳到最后面的生产者消费者模型阅读代码，省去前面的简单内容。</strong></p><p>当然，Tornado多进程模式需要依赖fork函数，在windows上是行不通的，但这并不意味着本篇文章的代码都无法运行，相反，你只需要注释掉 <code>http_server.start(0)</code>，就可以运行本篇文章的所有代码。在最终版本中，本文实现了一个全异步的服务，即使你无法启动多进程的Tornado，相信我，这也不会成为你的性能瓶颈！</p><p>完成这篇文章主要靠笔者的阅读经验和实际项目经验，《流畅的Python》一书对如何改造线程模型为协程模型有详细的介绍，如果想要深入学习Python，建议阅读此书。本文借用其原则：从某个函数进行改造时，首先将其定义为 <code>async</code>的，其次将其中的耗时操作利用 <code>run_in_executor</code>封装，最后层层改进其调用函数，使用 <code>await</code>调用，在这里听不懂没关系，后面会有实际讲解</p><h1 id="从Hello-World-开始"><a href="#从Hello-World-开始" class="headerlink" title="从Hello World!开始"></a>从Hello World!开始</h1><p>首先我们从Hello world开始，稍稍修改了官方给出的例子，得到了一个接受不是很规范的get实现，解析请求的 <code>json</code> body对象，从中读取 <code>job_id</code>并输出的服务</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> logging<br><br><span class="hljs-keyword">import</span> tornado.httpserver<br><span class="hljs-keyword">import</span> tornado.ioloop<br><span class="hljs-keyword">import</span> tornado.options<br><span class="hljs-keyword">import</span> tornado.web<br><span class="hljs-keyword">import</span> json<br><br><span class="hljs-keyword">from</span> tornado.options <span class="hljs-keyword">import</span> define, options<br><br>define(<span class="hljs-string">&quot;port&quot;</span>, default=<span class="hljs-number">8888</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;run on the given port&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_logger</span>():<br>    <span class="hljs-keyword">return</span> logging.getLogger(<span class="hljs-string">&quot;tornado.general&quot;</span>)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MainHandler</span>(tornado.web.RequestHandler):<br>    logger = get_logger()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get</span>(<span class="hljs-params">self</span>):<br>        job_id = json.loads(self.request.body.decode()).get(<span class="hljs-string">&#x27;job_id&#x27;</span>)<br>        self.do_something(job_id)<br>        self.write(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;job_id&#125;</span> done&quot;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">do_something</span>(<span class="hljs-params">self, job_id</span>):<br>        self.logger.info(<span class="hljs-string">f&#x27;do job:<span class="hljs-subst">&#123;job_id&#125;</span>&#x27;</span>)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    tornado.options.parse_command_line()<br>    application = tornado.web.Application([(<span class="hljs-string">r&quot;/&quot;</span>, MainHandler)])<br>    http_server = tornado.httpserver.HTTPServer(application)<br>    http_server.listen(options.port)<br>    <span class="hljs-comment"># 多进程</span><br>    <span class="hljs-comment"># http_server.start(0)</span><br>    tornado.ioloop.IOLoop.current().start()<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    main()<br><br></code></pre></td></tr></table></figure><p>启动后，他会监听你的8888端口</p><h2 id="写个简单的请求脚本吧"><a href="#写个简单的请求脚本吧" class="headerlink" title="写个简单的请求脚本吧"></a>写个简单的请求脚本吧</h2><p>下面我们简单的写个请求脚本验证一下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">import</span> json<br><br><span class="hljs-keyword">from</span> requests <span class="hljs-keyword">import</span> Timeout<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">api_request</span>(<span class="hljs-params">job_id</span>):<br>    <span class="hljs-keyword">try</span>:<br>        response = requests.get(<span class="hljs-string">&#x27;http://localhost:8888&#x27;</span>, data=json.dumps(&#123;<span class="hljs-string">&#x27;job_id&#x27;</span>: job_id&#125;), timeout=<span class="hljs-number">3</span>) <span class="hljs-comment"># 特意设置了3秒超时</span><br>    <span class="hljs-keyword">except</span> Timeout:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>    <span class="hljs-keyword">return</span> response.status_code == <span class="hljs-number">200</span><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    api_request(<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>运行这个脚本，你就向你的服务器发送了一个get请求啦</p><p>之后我们会针对这个脚本进行扩展，以达到并发测试的目的~</p><h1 id="如果任务时间比较长怎么办？"><a href="#如果任务时间比较长怎么办？" class="headerlink" title="如果任务时间比较长怎么办？"></a>如果任务时间比较长怎么办？</h1><p>实际的开发中，不可能简单的打个日志就结束，万一是一个需要一些时间（比如1s？0.5s？）才能完成的任务，那会发生什么呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">...<br><span class="hljs-comment">#省略了一些你已经知道的依赖引入</span><br><span class="hljs-keyword">import</span> time<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MainHandler</span>(tornado.web.RequestHandler):<br>    logger = get_logger()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get</span>(<span class="hljs-params">self</span>):<br>        job_id = json.loads(self.request.body.decode()).get(<span class="hljs-string">&#x27;job_id&#x27;</span>)<br>        self.do_something(job_id)<br>        self.write(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;job_id&#125;</span> done&quot;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">do_something</span>(<span class="hljs-params">self, job_id</span>):<br>        <span class="hljs-comment"># 模拟一个任务需要1秒的时间完成</span><br>        time.sleep(<span class="hljs-number">1</span>)<br>        self.logger.info(<span class="hljs-string">f&#x27;job done:<span class="hljs-subst">&#123;job_id&#125;</span>&#x27;</span>)<br>...<br><span class="hljs-comment">#省略了服务器启动的代码</span><br></code></pre></td></tr></table></figure><p>在以上的代码中，<code>time.sleep(1)</code>将阻塞服务器，这并不意味着无法建立连接，但是会导致已经建立的连接无法收到消息，形成 <code>ReadTimeout</code></p><h2 id="模拟并发测试"><a href="#模拟并发测试" class="headerlink" title="模拟并发测试"></a>模拟并发测试</h2><p>让我们把之前的请求脚本改一改，变为一个并发测试的脚本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">from</span> multiprocessing <span class="hljs-keyword">import</span> Pool<br><br><span class="hljs-keyword">from</span> requests <span class="hljs-keyword">import</span> Timeout<br><br><span class="hljs-comment"># 总请求数</span><br>REQUEST_NUM = <span class="hljs-number">10</span><br><span class="hljs-comment"># 进程数</span><br>PROCESSOR_NUM = <span class="hljs-number">10</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">api_request</span>(<span class="hljs-params">job_id</span>):<br>    <span class="hljs-keyword">try</span>:<br>        response = requests.get(<span class="hljs-string">&#x27;http://localhost:8888&#x27;</span>, data=json.dumps(&#123;<span class="hljs-string">&#x27;job_id&#x27;</span>: job_id&#125;), timeout=<span class="hljs-number">3</span>)<br>    <span class="hljs-keyword">except</span> Timeout:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>    <span class="hljs-keyword">return</span> response.status_code == <span class="hljs-number">200</span><br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-keyword">with</span> Pool(PROCESSOR_NUM) <span class="hljs-keyword">as</span> p:<br>        result = p.<span class="hljs-built_in">map</span>(api_request, <span class="hljs-built_in">range</span>(REQUEST_NUM))<br>    succeed = result.count(<span class="hljs-literal">True</span>)<br>    failed = result.count(<span class="hljs-literal">False</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;succeed / (failed + succeed) * <span class="hljs-number">100</span>&#125;</span>% request success!&quot;</span>)<br><br></code></pre></td></tr></table></figure><p>运行脚本，你会得到以下输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">20.0% request success!<br></code></pre></td></tr></table></figure><p>而在服务器端的日志，你会看到实际上是有10个请求的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash">[I 211217 23:45:52 hello_world:29] job <span class="hljs-keyword">done</span>:0<br>[I 211217 23:45:52 web:2239] 200 GET / (::1) 1012.00ms<br>[I 211217 23:45:53 hello_world:29] job <span class="hljs-keyword">done</span>:1<br>[I 211217 23:45:53 web:2239] 200 GET / (::1) 2025.00ms<br>[I 211217 23:45:54 hello_world:29] job <span class="hljs-keyword">done</span>:2<br>[I 211217 23:45:54 web:2239] 200 GET / (::1) 3032.00ms<br>[I 211217 23:45:55 hello_world:29] job <span class="hljs-keyword">done</span>:3<br>[I 211217 23:45:55 web:2239] 200 GET / (::1) 1009.00ms<br>[I 211217 23:45:56 hello_world:29] job <span class="hljs-keyword">done</span>:4<br>[I 211217 23:45:56 web:2239] 200 GET / (::1) 2020.00ms<br>[I 211217 23:45:57 hello_world:29] job <span class="hljs-keyword">done</span>:5<br>[I 211217 23:45:57 web:2239] 200 GET / (::1) 3029.00ms<br>[I 211217 23:45:58 hello_world:29] job <span class="hljs-keyword">done</span>:6<br>[I 211217 23:45:58 web:2239] 200 GET / (::1) 4044.00ms<br>[I 211217 23:45:59 hello_world:29] job <span class="hljs-keyword">done</span>:8<br>[I 211217 23:45:59 web:2239] 200 GET / (::1) 5056.00ms<br>[I 211217 23:46:00 hello_world:29] job <span class="hljs-keyword">done</span>:7<br>[I 211217 23:46:00 web:2239] 200 GET / (::1) 6068.00ms<br>[I 211217 23:46:01 hello_world:29] job <span class="hljs-keyword">done</span>:9<br>[I 211217 23:46:01 web:2239] 200 GET / (::1) 7076.00ms<br><br></code></pre></td></tr></table></figure><p>这说明，这十个连接被Tornado”接住“了，建立了连接，但是client端设置了超时时间，超时后client端断开了连接，而从根据3秒超时，1秒处理时间来看，只有前两个请求有可能完成，第三个请求大概率超时，第三个之后的请求根本不用想，必定超时</p><h1 id="使用多进程接收请求"><a href="#使用多进程接收请求" class="headerlink" title="使用多进程接收请求"></a>使用多进程接收请求</h1><p>接触过django、flask这类线程模型的web框架的你可能会想到使用多线程或者多进程来处理，Tornado作为协程框架，提供有多进程的接口，只需要打开 <code>http_server.start(0)</code>注释，你就会得到多进程的Tornado服务</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    tornado.options.parse_command_line()<br>    application = tornado.web.Application([(<span class="hljs-string">r&quot;/&quot;</span>, MainHandler)])<br>    http_server = tornado.httpserver.HTTPServer(application)<br>    http_server.listen(options.port)<br>    <span class="hljs-comment"># 多进程，根据你的CPU核数决定</span><br>    http_server.start(<span class="hljs-number">0</span>)<br>    tornado.ioloop.IOLoop.current().start()<br></code></pre></td></tr></table></figure><p>然后我们再进行一次请求，得到的服务器日志如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs bash">[I 211218 00:03:09 process:123] Starting 8 processes<br>[I 211218 00:03:12 hello:29] job <span class="hljs-keyword">done</span>:0<br>[I 211218 00:03:12 web:2239] 200 GET / (127.0.0.1) 1002.04ms<br>[I 211218 00:03:12 hello:29] job <span class="hljs-keyword">done</span>:5<br>[I 211218 00:03:12 web:2239] 200 GET / (127.0.0.1) 1002.93ms<br>[I 211218 00:03:12 hello:29] job <span class="hljs-keyword">done</span>:8<br>[I 211218 00:03:12 hello:29] job <span class="hljs-keyword">done</span>:4<br>[I 211218 00:03:12 web:2239] 200 GET / (127.0.0.1) 1002.39ms<br>[I 211218 00:03:12 web:2239] 200 GET / (127.0.0.1) 1002.52ms<br>[I 211218 00:03:12 hello:29] job <span class="hljs-keyword">done</span>:9<br>[I 211218 00:03:12 hello:29] job <span class="hljs-keyword">done</span>:7<br>[I 211218 00:03:12 web:2239] 200 GET / (127.0.0.1) 1002.65ms<br>[I 211218 00:03:12 web:2239] 200 GET / (127.0.0.1) 1002.69ms<br>[I 211218 00:03:12 hello:29] job <span class="hljs-keyword">done</span>:6<br>[I 211218 00:03:12 web:2239] 200 GET / (127.0.0.1) 1002.06ms<br>[I 211218 00:03:13 hello:29] job <span class="hljs-keyword">done</span>:2<br>[I 211218 00:03:13 web:2239] 200 GET / (127.0.0.1) 2002.96ms<br>[I 211218 00:03:14 hello:29] job <span class="hljs-keyword">done</span>:1<br>[I 211218 00:03:14 web:2239] 200 GET / (127.0.0.1) 3005.87ms<br>[I 211218 00:03:15 hello:29] job <span class="hljs-keyword">done</span>:3<br>[I 211218 00:03:15 web:2239] 200 GET / (127.0.0.1) 4007.62ms<br><br></code></pre></td></tr></table></figure><p>这次所有请求都成功了！（如果有失败的，你当然可以多试几次）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">100.0% request success!<br></code></pre></td></tr></table></figure><p>当然，这种简单粗暴的方式有其缺点：</p><ol><li>资源消耗大，每个连接都需要一个进程保持</li><li>stupid，就像知乎上的入门教程一样，没听说过什么是协程</li><li>多进程需要考虑竞争，加锁，可能1核有难7核围观</li></ol><h1 id="像个聪明人：使用协程"><a href="#像个聪明人：使用协程" class="headerlink" title="像个聪明人：使用协程"></a>像个聪明人：使用协程</h1><p>实际上，我们只要使用Tornado的异步特性，不需要多进程，就可以搞定这个问题</p><p>我把解决问题的步骤都标注在注释里，希望你能理解自底向上异步改造的流程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 0.异步实现的库</span><br><span class="hljs-keyword">import</span> asyncio<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MainHandler</span>(tornado.web.RequestHandler):<br>    logger = get_logger()<br><br>    <span class="hljs-comment"># 4.把这个函数也变成异步的，然后继续向上变更，上级Tornado Handler支持异步的get请求，修改到此为止</span><br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">get</span>(<span class="hljs-params">self</span>):<br>        job_id = json.loads(self.request.body.decode()).get(<span class="hljs-string">&#x27;job_id&#x27;</span>)<br>        <span class="hljs-comment"># 3.把do_something的调用变成异步调用</span><br>        <span class="hljs-keyword">await</span>  self.do_something(job_id)<br>        self.write(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;job_id&#125;</span> done&quot;</span>)<br><br>    <span class="hljs-comment"># 2.把这个函数编程异步的</span><br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">do_something</span>(<span class="hljs-params">self, job_id</span>):<br>        <span class="hljs-comment"># 1.使用异步实现的库替换耗时操作</span><br>        <span class="hljs-keyword">await</span> asyncio.sleep(<span class="hljs-number">1</span>)<br>        self.logger.info(<span class="hljs-string">f&#x27;job done:<span class="hljs-subst">&#123;job_id&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure><p>现在让我们注释掉多进程模式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 关掉多进程，像个男子汉</span><br><span class="hljs-comment"># http_server.start(0)</span><br></code></pre></td></tr></table></figure><p>然后测试一下刚才的脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash">[I 211218 00:08:17 hello:29] job <span class="hljs-keyword">done</span>:2<br>[I 211218 00:08:17 web:2239] 200 GET / (127.0.0.1) 1003.32ms<br>[I 211218 00:08:17 hello:29] job <span class="hljs-keyword">done</span>:0<br>[I 211218 00:08:17 web:2239] 200 GET / (127.0.0.1) 1002.62ms<br>[I 211218 00:08:17 hello:29] job <span class="hljs-keyword">done</span>:3<br>[I 211218 00:08:17 web:2239] 200 GET / (127.0.0.1) 1002.47ms<br>[I 211218 00:08:17 hello:29] job <span class="hljs-keyword">done</span>:1<br>[I 211218 00:08:17 web:2239] 200 GET / (127.0.0.1) 1002.51ms<br>[I 211218 00:08:17 hello:29] job <span class="hljs-keyword">done</span>:4<br>[I 211218 00:08:17 web:2239] 200 GET / (127.0.0.1) 1002.49ms<br>[I 211218 00:08:17 hello:29] job <span class="hljs-keyword">done</span>:7<br>[I 211218 00:08:17 web:2239] 200 GET / (127.0.0.1) 1002.37ms<br>[I 211218 00:08:17 hello:29] job <span class="hljs-keyword">done</span>:8<br>[I 211218 00:08:17 web:2239] 200 GET / (127.0.0.1) 1002.94ms<br>[I 211218 00:08:17 hello:29] job <span class="hljs-keyword">done</span>:6<br>[I 211218 00:08:17 web:2239] 200 GET / (127.0.0.1) 1002.41ms<br>[I 211218 00:08:17 hello:29] job <span class="hljs-keyword">done</span>:9<br>[I 211218 00:08:17 web:2239] 200 GET / (127.0.0.1) 1003.31ms<br>[I 211218 00:08:17 hello:29] job <span class="hljs-keyword">done</span>:5<br>[I 211218 00:08:17 web:2239] 200 GET / (127.0.0.1) 1003.20ms<br></code></pre></td></tr></table></figure><p>芜湖，所有连接都在1秒左右搞定了！</p><h2 id="协程的原理"><a href="#协程的原理" class="headerlink" title="协程的原理"></a>协程的原理</h2><p>对于初学者而言，你首先需要了解用户级线程和内核级线程的区别。协程实际上是一个单进程单线程模型，对于内核而言，它是1而非N，协程程序自己控制各个协程之间的运行顺序，这就是用户级线程。不谈内核是如何调度线程的，对于协程而言，每个 <code>await</code>都代表着让出程序控制（让出CPU），并将结果加入到等待队列，协程调度器将从等待队列中找到一个已经完成的任务，恢复其上下文环境，让这个任务能够继续执行下去。在本例中，1秒之后，<code>asyncio.sleep(1)</code>的任务完成了，这时如果有好心人能够让出CPU（调用 <code>await</code>），那么原来暂停的程序就有可能被选中，得以继续完成。</p><p>协程就是这样，在单线程中循环搜索那些已经完成的任务并加以推进，同时等待、管理那些未完成的任务</p><p>这样一说，希望你能理解 <code>IOLoop</code>中 <code>Loop</code>这四个字母的含义</p><h2 id="协程的问题"><a href="#协程的问题" class="headerlink" title="协程的问题"></a>协程的问题</h2><p>你也看到了，协程最重要的是等待任务完成，但没有告诉我们任务如何完成</p><p>如果任务是一个网络请求，那么等待他完成是一件挺不错的事，但如果任务是打印一行日志，那么等待他完成就显得有点蠢</p><p>其实对于程序员来说，最重要的事有库可以<strong>异步</strong>地做事</p><p>否则，你就得参考下一章，使用executor封装了</p><h2 id="使用executor封装协程"><a href="#使用executor封装协程" class="headerlink" title="使用executor封装协程"></a>使用executor封装协程</h2><p>如何在不耗费CPU的情况下做一件耗费CPU的事？这本身就是一个悖论。</p><p>因此，对于一些需要计算，或者没有异步实现的任务来说，想要像 <code>asyncio.sleep()</code>一样轻松异步执行是做不到的，这就需要我们借助线程或进程的力量（<strong>当然，线程安全就是避不开的话题</strong>）。</p><p>首先，让我们假装忘记 <code>sleep</code>的异步实现，换回 <code>time.sleep()</code>然后你就会发现，<code>async</code>并不能让你获得异步的能力，而是像普通函数一样卡死在这里</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MainHandler</span>(tornado.web.RequestHandler):<br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">do_something</span>(<span class="hljs-params">self, job_id</span>):<br>        <span class="hljs-comment"># 哦，我们在异步函数里写了一个长阻塞，这太糟了</span><br>        time.sleep(<span class="hljs-number">1</span>)<br>        self.logger.info(<span class="hljs-string">f&#x27;job done:<span class="hljs-subst">&#123;job_id&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure><p>有两种方案可以搞定，一种是Tornado提供的装饰器，有点偷懒但是好用，<code>run_on_executor</code>装饰器将自动地把同步函数（<code>do_something</code>）放进 <code>self.executor</code>执行，并把它封装成一个 <code>async</code>函数（其实称为 <code>awaitable</code>对象比较好）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> tornado.concurrent <span class="hljs-keyword">import</span> run_on_executor<br><span class="hljs-keyword">from</span> concurrent.futures <span class="hljs-keyword">import</span> ThreadPoolExecutor<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MainHandler</span>(tornado.web.RequestHandler):<br>    logger = get_logger()<br>    executor = ThreadPoolExecutor(<span class="hljs-number">20</span>)<br><br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">get</span>(<span class="hljs-params">self</span>):<br>        job_id = json.loads(self.request.body.decode()).get(<span class="hljs-string">&#x27;job_id&#x27;</span>)<br>        <span class="hljs-keyword">await</span> self.do_something(job_id)<br>        self.write(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;job_id&#125;</span> done&quot;</span>)<br><br>    <span class="hljs-comment"># 没关系，我们把它放在executor里执行就好了</span><br>    <span class="hljs-comment"># 注意：这里改成了同步函数</span><br><span class="hljs-meta">    @run_on_executor</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">do_something</span>(<span class="hljs-params">self, job_id</span>):<br>        <span class="hljs-comment"># 哦，这会阻塞服务器！</span><br>        time.sleep(<span class="hljs-number">1</span>)<br>        self.logger.info(<span class="hljs-string">f&#x27;job done:<span class="hljs-subst">&#123;job_id&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure><p>另一种是常见的异步写法，是标准 <code>IOLoop</code>支持的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> tornado <span class="hljs-keyword">import</span> ioloop<br><span class="hljs-keyword">from</span> concurrent.futures <span class="hljs-keyword">import</span> ThreadPoolExecutor<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MainHandler</span>(tornado.web.RequestHandler):<br>    logger = get_logger()<br>    executor = ThreadPoolExecutor(<span class="hljs-number">20</span>)<br><br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">get</span>(<span class="hljs-params">self</span>):<br>        job_id = json.loads(self.request.body.decode()).get(<span class="hljs-string">&#x27;job_id&#x27;</span>)<br>        <span class="hljs-comment"># 没关系，我们把它放在executor里执行就好了</span><br>        <span class="hljs-keyword">await</span> ioloop.IOLoop.current().run_in_executor(self.executor, self.do_something, job_id)<br>        self.write(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;job_id&#125;</span> done&quot;</span>)<br><br>     <span class="hljs-comment"># 注意：这里改成了同步函数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">do_something</span>(<span class="hljs-params">self, job_id</span>):<br>        <span class="hljs-comment"># 哦，这会阻塞服务器！</span><br>        time.sleep(<span class="hljs-number">1</span>)<br>        self.logger.info(<span class="hljs-string">f&#x27;job done:<span class="hljs-subst">&#123;job_id&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure><p>实践中，由于GIL锁限制，线程并不能发挥机器地全部实力，在CPU密集时推荐将 <code>ThreadPoolExecutor</code>改为 <code>ProcessPoolExecutor</code>，但是由于 <code>pickle</code>不能封装自定义类发送给子进程执行，所以需要把CPU密集型操作单独写成一个函数，这里用第二种方式做示范，因为第二种方式更通用，也更好写</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> concurrent.futures <span class="hljs-keyword">import</span> ProcessPoolExecutor<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">real_work</span>():<br>    time.sleep(<span class="hljs-number">1</span>)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MainHandler</span>(tornado.web.RequestHandler):<br>    logger = get_logger()<br>    executor = ProcessPoolExecutor(<span class="hljs-number">20</span>)<br><br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">get</span>(<span class="hljs-params">self</span>):<br>        job_id = json.loads(self.request.body.decode()).get(<span class="hljs-string">&#x27;job_id&#x27;</span>)<br>        <span class="hljs-keyword">await</span> self.do_something(job_id)<br>        self.write(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;job_id&#125;</span> done&quot;</span>)<br><br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">do_something</span>(<span class="hljs-params">self, job_id</span>):<br>        <span class="hljs-comment">#根据最小原则封装</span><br>        <span class="hljs-keyword">await</span> ioloop.IOLoop.current().run_in_executor(self.executor, real_work)<br>        self.logger.info(<span class="hljs-string">f&#x27;job done:<span class="hljs-subst">&#123;job_id&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure><p>好了，经过以上操作，我们已经明白了如何封装同步为异步，化腐朽为神奇，有一点千万记住，<strong>在协程中，任何阻塞都有可能是致命的！任何executor封装的操作都需要是线程安全的！</strong></p><p>以及，仔细分析压力点，是流量顶不住还是计算太慢，如果是前者，就采用Tornado多进程模式，如果是后者，就使用 <code>executor</code>承压</p><p>最后，<code>executor</code>的承载数量是有限的，你可以尝试调大测试脚本并发数量，看是否还能保持之前的成功率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 总请求数</span><br>REQUEST_NUM=<span class="hljs-number">3000</span><br><span class="hljs-comment"># 100并发</span><br>PROCESSOR_NUM=<span class="hljs-number">100</span><br></code></pre></td></tr></table></figure><h1 id="空间换时间：生产者消费者模型"><a href="#空间换时间：生产者消费者模型" class="headerlink" title="空间换时间：生产者消费者模型"></a>空间换时间：生产者消费者模型</h1><p>在上一章中，如果你确实调大了并发量和请求数，你就会发现，在服务器可用线程被耗尽的情况下（当然你可以设置几百上千个），你的连接仍然会失败。其实在任何web框架中都是一样的，资源耗尽就只有死路一条。对于这类任务，程序员应该提前预判到，并将其转换为异步任务，利用生产者消费者模型对请求进行处理。</p><p>接下来我将展示一个异步模式下的消费者模型，利用 <code>IOLoop.add_callback()</code>函数，将消费者的消费函数注册为任务，同时依靠 <code>ThreadPoolExecutor</code>执行阻塞操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">async_period_job</span>(<span class="hljs-params">func, period</span>):<br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        <span class="hljs-keyword">try</span>:<br>            <span class="hljs-keyword">await</span> func()<br>        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>            get_logger().exception(e)<br>        <span class="hljs-keyword">await</span> asyncio.sleep(period)<br><br><br><span class="hljs-comment"># 定时任务的常用写法</span><br>start_period_job = functools.partial(ioloop.IOLoop.current().add_callback, async_period_job)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sleep</span>(<span class="hljs-params">t, job_id</span>):<br>    <span class="hljs-comment"># 耗时操作 兼容ProcessPoolExecutor</span><br>    <span class="hljs-comment"># 如果使用自定义的ORM类，使用ThreadPoolExecutor就足够了，不需要大费周章将其写成外部函数</span><br>    time.sleep(t)<br>    <span class="hljs-keyword">return</span> job_id<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleConsumer</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.buffer = Queue()<br>        self.futures = Queue()<br>        self.started = <span class="hljs-literal">False</span><br>        self.log = get_logger()<br>        <span class="hljs-comment"># 2秒收集一次结果</span><br>        self.collect_period = <span class="hljs-number">2</span><br>        <span class="hljs-comment"># 这里控制消费者数量</span><br>        self.executor = PoolExecutor(<span class="hljs-number">20</span>)<br><br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">put</span>(<span class="hljs-params">self, job_id</span>):<br>        <span class="hljs-keyword">await</span> self.buffer.put(job_id)<br><br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">real_work</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 由于pickle的原因，不能放在executor里</span><br>        data = <span class="hljs-keyword">await</span> self.buffer.get()<br>        self.log.info(<span class="hljs-string">f&#x27;scheduling <span class="hljs-subst">&#123;data&#125;</span>&#x27;</span>)<br>        future = self.executor.submit(sleep, <span class="hljs-number">1</span>, data)<br>        <span class="hljs-keyword">return</span> future, data<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">start</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">if</span> self.started:<br>            <span class="hljs-keyword">return</span><br>        self.started = <span class="hljs-literal">True</span><br>        self.log.info(<span class="hljs-string">&#x27;Consumer Started!&#x27;</span>)<br>        ioloop.IOLoop.current().add_callback(self._start_real_work)<br>        start_period_job(self._collect, self.collect_period)<br><br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">_collect</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 定时收集结果</span><br>        not_done = Queue()<br>        <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> self.futures.empty():<br>            future, data = <span class="hljs-keyword">await</span> self.futures.get()<br>            <span class="hljs-keyword">if</span> future.done():<br>                <span class="hljs-keyword">try</span>:<br>                    result = future.result()<br>                    <span class="hljs-keyword">await</span> self.collect_result(result)<br>                <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>                    self.log.exception(e)<br>                    <span class="hljs-keyword">await</span> self.buffer.put(data)  <span class="hljs-comment"># retry</span><br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">await</span> not_done.put((future, data))<br>        <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> not_done.empty():<br>            <span class="hljs-keyword">await</span> self.futures.put(<span class="hljs-keyword">await</span> not_done.get())<br><br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">_start_real_work</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>            future, data = <span class="hljs-keyword">await</span> self.real_work()<br>            <span class="hljs-keyword">await</span> self.futures.put((future, data))<br><br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">collect_result</span>(<span class="hljs-params">self, result</span>):<br>        self.log.info(<span class="hljs-string">f&#x27;Collected job: <span class="hljs-subst">&#123;result&#125;</span>&#x27;</span>)<br><br><br><span class="hljs-comment"># 最简单的单例</span><br>consumer = SimpleConsumer()<br>consumer.start()<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MainHandler</span>(tornado.web.RequestHandler):<br>    logger = get_logger()<br>    consumer = consumer<br><br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">get</span>(<span class="hljs-params">self</span>):<br>        job_id = json.loads(self.request.body.decode()).get(<span class="hljs-string">&#x27;job_id&#x27;</span>)<br>        <span class="hljs-keyword">await</span> self.do_something(job_id)<br>        self.write(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;job_id&#125;</span> add&quot;</span>)<br><br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">do_something</span>(<span class="hljs-params">self, job_id</span>):<br>        <span class="hljs-keyword">await</span> self.consumer.put(job_id)<br>        self.logger.info(<span class="hljs-string">f&#x27;job add:<span class="hljs-subst">&#123;job_id&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure><p>使用并发脚本测试，将得到类似以下日志：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs bash">[I 211218 02:14:10 hello:112] job add:2<br>[I 211218 02:14:10 web:2239] 200 GET / (127.0.0.1) 1.54ms<br>[I 211218 02:14:10 hello:112] job add:1<br>[I 211218 02:14:10 web:2239] 200 GET / (127.0.0.1) 1.53ms<br>[I 211218 02:14:10 hello:112] job add:3<br>[I 211218 02:14:10 web:2239] 200 GET / (127.0.0.1) 1.58ms<br>[I 211218 02:14:10 hello:112] job add:5<br>[I 211218 02:14:10 web:2239] 200 GET / (127.0.0.1) 1.67ms<br>[I 211218 02:14:10 hello:112] job add:4<br>[I 211218 02:14:10 web:2239] 200 GET / (127.0.0.1) 1.73ms<br>[I 211218 02:14:10 hello:112] job add:0<br>[I 211218 02:14:10 web:2239] 200 GET / (127.0.0.1) 1.76ms<br>[I 211218 02:14:10 hello:59] scheduling 2<br>[I 211218 02:14:10 hello:59] scheduling 1<br>[I 211218 02:14:10 hello:59] scheduling 3<br>[I 211218 02:14:10 hello:59] scheduling 5<br>[I 211218 02:14:10 hello:59] scheduling 4<br>[I 211218 02:14:10 hello:59] scheduling 0<br>[I 211218 02:14:10 hello:112] job add:8<br>[I 211218 02:14:10 web:2239] 200 GET / (127.0.0.1) 0.70ms<br>[I 211218 02:14:10 hello:112] job add:7<br>[I 211218 02:14:10 web:2239] 200 GET / (127.0.0.1) 0.69ms<br>[I 211218 02:14:10 hello:112] job add:9<br>[I 211218 02:14:10 web:2239] 200 GET / (127.0.0.1) 0.74ms<br>[I 211218 02:14:10 hello:59] scheduling 8<br>[I 211218 02:14:10 hello:59] scheduling 7<br>[I 211218 02:14:10 hello:59] scheduling 9<br>[I 211218 02:14:10 hello:112] job add:6<br>[I 211218 02:14:10 web:2239] 200 GET / (127.0.0.1) 1.15ms<br>[I 211218 02:14:10 hello:59] scheduling 6<br>[I 211218 02:14:13 hello:94] Collected job: 2<br>[I 211218 02:14:13 hello:94] Collected job: 1<br>[I 211218 02:14:13 hello:94] Collected job: 3<br>[I 211218 02:14:13 hello:94] Collected job: 5<br>[I 211218 02:14:13 hello:94] Collected job: 4<br>[I 211218 02:14:13 hello:94] Collected job: 0<br>[I 211218 02:14:13 hello:94] Collected job: 8<br>[I 211218 02:14:13 hello:94] Collected job: 7<br>[I 211218 02:14:13 hello:94] Collected job: 9<br>[I 211218 02:14:13 hello:94] Collected job: 6<br><br></code></pre></td></tr></table></figure><p>我们会看到，任务在一边被加入队列，一边进行，就如我前面所说，协程会在 <code>await</code>的时候释放CPU并切换到准备好的协程继续执行，这里体现为忙时一直在接收请求，闲时对buffer里的内容进行处理。</p><p>通过这种方式，我们可以通过Tornado的多进程模式轻松拓展生产者，通过空间换时间，保证请求不会失败，将任务轻松转换为后台任务，通过控制 <code>PoolExecutor</code>的 <code>worker</code>数量，控制消费者数量，达到性能平衡</p><p>和刚才用 <code>ProcessPoolExecutor</code>一样，在CPU密集的情况下，多进程消费者显然更具优势</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> concurrent.futures <span class="hljs-keyword">import</span> ProcessPoolExecutor <span class="hljs-keyword">as</span> PoolExecutor<br></code></pre></td></tr></table></figure><p>现在请你试着加大总请求数，看看效果，你会发现在加入了队列之后，即使是单线程也可以瞬间搞定所有请求，这是生产者消费者模型给我们带来的便利。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">REQUEST_NUM = 1000<br></code></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文介绍了如何利用Tornado的异步特性，打造高性能Tornado服务器，有几点需要课后复习</p><ol><li>自底向上的异步改造</li><li>使用 <code>executor</code>封装异步操作</li><li>生产者消费者模型的异步实现</li></ol><p>还有几点需要额外注意：</p><ol><li>使用 <code>executor</code>时的线程安全问题</li><li><code>ProcessPoolExecutor</code>的 <code>pickle</code>问题</li><li><code>executor</code>是有并行数量限制的</li></ol><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><p>生产者消费者完整代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> asyncio<br><span class="hljs-keyword">import</span> functools<br><span class="hljs-keyword">import</span> logging<br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">from</span> asyncio <span class="hljs-keyword">import</span> Queue<br><span class="hljs-keyword">from</span> concurrent.futures <span class="hljs-keyword">import</span> ThreadPoolExecutor <span class="hljs-keyword">as</span> PoolExecutor<br><br><span class="hljs-keyword">import</span> tornado.httpserver<br><span class="hljs-keyword">import</span> tornado.ioloop<br><span class="hljs-keyword">import</span> tornado.options<br><span class="hljs-keyword">import</span> tornado.web<br><span class="hljs-keyword">import</span> json<br><br><span class="hljs-keyword">from</span> tornado <span class="hljs-keyword">import</span> ioloop<br><span class="hljs-keyword">from</span> tornado.concurrent <span class="hljs-keyword">import</span> run_on_executor<br><span class="hljs-keyword">from</span> tornado.options <span class="hljs-keyword">import</span> define, options<br><br>define(<span class="hljs-string">&quot;port&quot;</span>, default=<span class="hljs-number">8888</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;run on the given port&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_logger</span>():<br>    <span class="hljs-keyword">return</span> logging.getLogger(<span class="hljs-string">&#x27;tornado.general&#x27;</span>)<br><br><br><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">async_period_job</span>(<span class="hljs-params">func, period</span>):<br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        <span class="hljs-keyword">try</span>:<br>            <span class="hljs-keyword">await</span> func()<br>        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>            get_logger().exception(e)<br>        <span class="hljs-keyword">await</span> asyncio.sleep(period)<br><br><br><span class="hljs-comment"># 定时任务的常用写法</span><br>start_period_job = functools.partial(ioloop.IOLoop.current().add_callback, async_period_job)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sleep</span>(<span class="hljs-params">t, job_id</span>):<br>    <span class="hljs-comment"># 耗时操作 兼容ProcessPoolExecutor</span><br>    <span class="hljs-comment"># 如果使用自定义的ORM类，使用ThreadPoolExecutor就足够了，不需要大费周章将其写成外部函数</span><br>    time.sleep(t)<br>    <span class="hljs-keyword">return</span> job_id<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleConsumer</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.buffer = Queue()<br>        self.futures = Queue()<br>        self.started = <span class="hljs-literal">False</span><br>        self.log = get_logger()<br>        self.collect_period = <span class="hljs-number">2</span><br>        self.executor = PoolExecutor(<span class="hljs-number">20</span>)<br><br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">put</span>(<span class="hljs-params">self, job_id</span>):<br>        <span class="hljs-keyword">await</span> self.buffer.put(job_id)<br><br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">real_work</span>(<span class="hljs-params">self</span>):<br>        data = <span class="hljs-keyword">await</span> self.buffer.get()<br>        self.log.info(<span class="hljs-string">f&#x27;scheduling <span class="hljs-subst">&#123;data&#125;</span>&#x27;</span>)<br>        future = self.executor.submit(sleep, <span class="hljs-number">1</span>, data)<br>        <span class="hljs-keyword">return</span> future, data<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">start</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">if</span> self.started:<br>            <span class="hljs-keyword">return</span><br>        self.started = <span class="hljs-literal">True</span><br>        self.log.info(<span class="hljs-string">&#x27;Consumer Started!&#x27;</span>)<br>        ioloop.IOLoop.current().add_callback(self._start_real_work)<br>        start_period_job(self._collect, self.collect_period)<br><br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">_collect</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 收集结果，应当在定时任务里面做</span><br>        not_done = Queue()<br>        <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> self.futures.empty():<br>            future, data = <span class="hljs-keyword">await</span> self.futures.get()<br>            <span class="hljs-keyword">if</span> future.done():<br>                <span class="hljs-keyword">try</span>:<br>                    result = future.result()<br>                    <span class="hljs-keyword">await</span> self.collect_result(result)<br>                <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>                    self.log.exception(e)<br>                    <span class="hljs-keyword">await</span> self.buffer.put(data)  <span class="hljs-comment"># retry</span><br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">await</span> not_done.put((future, data))<br>        <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> not_done.empty():<br>            <span class="hljs-keyword">await</span> self.futures.put(<span class="hljs-keyword">await</span> not_done.get())<br><br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">_start_real_work</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>            future, data = <span class="hljs-keyword">await</span> self.real_work()<br>            <span class="hljs-keyword">await</span> self.futures.put((future, data))<br><br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">collect_result</span>(<span class="hljs-params">self, result</span>):<br>        self.log.info(<span class="hljs-string">f&#x27;Collected job: <span class="hljs-subst">&#123;result&#125;</span>&#x27;</span>)<br><br><br>consumer = SimpleConsumer()<br>consumer.start()<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MainHandler</span>(tornado.web.RequestHandler):<br>    logger = get_logger()<br>    consumer = consumer<br><br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">get</span>(<span class="hljs-params">self</span>):<br>        job_id = json.loads(self.request.body.decode()).get(<span class="hljs-string">&#x27;job_id&#x27;</span>)<br>        <span class="hljs-keyword">await</span> self.do_something(job_id)<br>        self.write(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;job_id&#125;</span> add&quot;</span>)<br><br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">do_something</span>(<span class="hljs-params">self, job_id</span>):<br>        <span class="hljs-keyword">await</span> self.consumer.put(job_id)<br>        self.logger.info(<span class="hljs-string">f&#x27;job add:<span class="hljs-subst">&#123;job_id&#125;</span>&#x27;</span>)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    tornado.options.parse_command_line()<br>    application = tornado.web.Application([(<span class="hljs-string">r&quot;/&quot;</span>, MainHandler)])<br>    http_server = tornado.httpserver.HTTPServer(application)<br>    http_server.listen(options.port)<br>    <span class="hljs-comment"># http_server.start(0)</span><br>    tornado.ioloop.IOLoop.current().start()<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    main()<br><br></code></pre></td></tr></table></figure><p>测试脚本代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">from</span> multiprocessing <span class="hljs-keyword">import</span> Pool<br><br><span class="hljs-keyword">from</span> requests <span class="hljs-keyword">import</span> Timeout<br><br>REQUEST_NUM = <span class="hljs-number">1000</span><br>PROCESSOR_NUM = <span class="hljs-number">10</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">api_request</span>(<span class="hljs-params">job_id</span>):<br>    <span class="hljs-keyword">try</span>:<br>        response = requests.get(<span class="hljs-string">&#x27;http://localhost:8888&#x27;</span>, data=json.dumps(&#123;<span class="hljs-string">&#x27;job_id&#x27;</span>: job_id&#125;), timeout=<span class="hljs-number">3</span>)<br>    <span class="hljs-keyword">except</span> Timeout:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>    <span class="hljs-keyword">return</span> response.status_code == <span class="hljs-number">200</span><br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-keyword">with</span> Pool(PROCESSOR_NUM) <span class="hljs-keyword">as</span> p:<br>        result = p.<span class="hljs-built_in">map</span>(api_request, <span class="hljs-built_in">range</span>(REQUEST_NUM))<br>    succeed = result.count(<span class="hljs-literal">True</span>)<br>    failed = result.count(<span class="hljs-literal">False</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;succeed / (failed + succeed) * <span class="hljs-number">100</span>&#125;</span>% request success!&quot;</span>)<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Tornado</tag>
      
      <tag>Python</tag>
      
      <tag>异步</tag>
      
      <tag>高性能</tag>
      
      <tag>协程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🔧Hadoop3.2单机部署教程</title>
    <link href="/2021/04/10/2021-04-11-HadoopStandalone/"/>
    <url>/2021/04/10/2021-04-11-HadoopStandalone/</url>
    
    <content type="html"><![CDATA[<h1 id="0-安装java-8"><a href="#0-安装java-8" class="headerlink" title="0 安装java-8"></a>0 安装java-8</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo apt update &amp;&amp; apt upgrade<br>sudo apt install openjdk-8-jdk<br></code></pre></td></tr></table></figure><h1 id="1-创建用户，配置免密ssh"><a href="#1-创建用户，配置免密ssh" class="headerlink" title="1 创建用户，配置免密ssh"></a>1 创建用户，配置免密ssh</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo useradd -m hadoop -s /bin/bash<br>sudo passwd hadoop<br>sudo adduser hadoop sudo<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">su - hadoop<br>ssh-keygen -t rsa -P <span class="hljs-string">&#x27;&#x27;</span> -f ~/.ssh/id_rsa<br><span class="hljs-built_in">cat</span> ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys<br><span class="hljs-built_in">chmod</span> 0600 ~/.ssh/authorized_keys<br></code></pre></td></tr></table></figure><h1 id="2-下载Hadoop"><a href="#2-下载Hadoop" class="headerlink" title="2 下载Hadoop"></a>2 下载Hadoop</h1><p><strong>以下内容使用hadoop用户完成</strong></p><p><strong>此方法通用3.2.0版本，记得修改下载链接或从其他地方下载</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://downloads.apache.org/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz<br>tar xvzf hadoop-*.tar.gz<br><span class="hljs-built_in">mv</span> hadoop-3.2.1 hadoop<br></code></pre></td></tr></table></figure><h1 id="3-配置hadoop"><a href="#3-配置hadoop" class="headerlink" title="3 配置hadoop"></a>3 配置hadoop</h1><p>修改~&#x2F;.bashrc，在最后添加：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> HADOOP_HOME=/home/hadoop/hadoop <span class="hljs-built_in">export</span> HADOOP_INSTALL=<span class="hljs-variable">$HADOOP_HOME</span><br><span class="hljs-built_in">export</span> HADOOP_MAPRED_HOME=<span class="hljs-variable">$HADOOP_HOME</span> <span class="hljs-built_in">export</span> HADOOP_COMMON_HOME=<span class="hljs-variable">$HADOOP_HOME</span><br><span class="hljs-built_in">export</span> HADOOP_HDFS_HOME=<span class="hljs-variable">$HADOOP_HOME</span> <span class="hljs-built_in">export</span> YARN_HOME=<span class="hljs-variable">$HADOOP_HOME</span><br><span class="hljs-built_in">export</span> HADOOP_COMMON_LIB_NATIVE_DIR=<span class="hljs-variable">$HADOOP_HOME</span>/lib/native<br><span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$PATH</span>:<span class="hljs-variable">$HADOOP_HOME</span>/sbin:<span class="hljs-variable">$HADOOP_HOME</span>/bin<br></code></pre></td></tr></table></figure><p>生效修改：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/.bashrc<br></code></pre></td></tr></table></figure><p>设置hadoop使用的JAVA_HOME：</p><p>修改文件：<code>~/hadoop/etc/hadoop/hadoop-env.sh，添加：</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64<br></code></pre></td></tr></table></figure><h1 id="4-配置文件"><a href="#4-配置文件" class="headerlink" title="4 配置文件"></a>4 配置文件</h1><p>进入配置文件目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> <span class="hljs-variable">$HADOOP_HOME</span>/etc/hadoop<br></code></pre></td></tr></table></figure><p>core-site.xml 配置hdfs地址</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br> <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.default.name<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>   <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://localhost:9000<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><p>hdfs-site.xml 配置namenode、datanode位置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.replication<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>1<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.name.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>file:///home/hadoop/hadoopdata/hdfs/namenode<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.data.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>file:///home/hadoop/hadoopdata/hdfs/datanode<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><p>mapred-site.xml，配置mapreduce框架为yarn</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br> <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.framework.name<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>   <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>yarn<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br> <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><p>yarn-site.xml 配置yarn</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br> <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>mapreduce_shuffle<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br> <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><h1 id="5-格式化namenode并启动"><a href="#5-格式化namenode并启动" class="headerlink" title="5 格式化namenode并启动"></a>5 格式化namenode并启动</h1><p>切换至Hadoop用户home目录，格式化namenode</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> ~<br>hdfs namenode -format<br></code></pre></td></tr></table></figure><p>启动hadoop</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> <span class="hljs-variable">$HADOOP_HOME</span>/sbin<br>./start-dfs.sh<br>./start-yarn.sh<br></code></pre></td></tr></table></figure><h1 id="6-访问ui"><a href="#6-访问ui" class="headerlink" title="6 访问ui"></a>6 访问ui</h1><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"><span class="hljs-number">127.0.0.1:9870</span><br></code></pre></td></tr></table></figure><h1 id="Troubleshooting"><a href="#Troubleshooting" class="headerlink" title="Troubleshooting"></a>Troubleshooting</h1><h2 id="更改ip导致集群失效"><a href="#更改ip导致集群失效" class="headerlink" title="更改ip导致集群失效"></a>更改ip导致集群失效</h2><p>删除hadoop用户home下的hadoopdata文件夹，重新格式化namenode并启动</p><p>这样做会丢失数据</p>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>大数据</tag>
      
      <tag>Hadoop单机配置</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🔧Spark部署教程</title>
    <link href="/2021/04/10/2021-04-11-SparkStandalone.md/"/>
    <url>/2021/04/10/2021-04-11-SparkStandalone.md/</url>
    
    <content type="html"><![CDATA[<h1 id="Update-2023年9月18日"><a href="#Update-2023年9月18日" class="headerlink" title="Update 2023年9月18日"></a>Update 2023年9月18日</h1><p>Spark的最新部署脚本和教程我放在了这个仓库下：<a href="https://github.com/Wh1isper/spark-build">https://github.com/Wh1isper/spark-build</a></p><p>同时有打好的docker镜像可以使用：</p><ul><li>For PySpark app: <a href="https://hub.docker.com/r/wh1isper/pyspark-app-base">wh1isper&#x2F;pyspark-app-base</a></li><li>For Spark Connect Server: <a href="https://hub.docker.com/r/wh1isper/spark-executor">wh1isper&#x2F;spark-executor</a></li><li>For Spark on k8s(Spark executor): <a href="https://hub.docker.com/r/wh1isper/spark-connector-server">wh1isper&#x2F;spark-connector-server</a></li></ul><p>最新的教程中，JDK11和17都是经过测试的</p><hr><h1 id="下载Spark"><a href="#下载Spark" class="headerlink" title="下载Spark"></a>下载Spark</h1><p>选择一个spark版本下载：<a href="https://spark.apache.org/downloads.html">https://spark.apache.org/downloads.html</a></p><p>然后下载并安装对应的scala版本</p><p>Note that, Spark 2.x is pre-built with Scala 2.11 except version 2.4.2, which is pre-built with Scala 2.12. Spark 3.0+ is pre-built with Scala 2.12.</p><p>这里使用Spark-3.0.1进行开发</p><h1 id="配置spark"><a href="#配置spark" class="headerlink" title="配置spark"></a>配置spark</h1><p>首先解压spark，然后设置SPARK_HOME，以下内容仅跟用户选择把spark放在哪里有关，实际上，你可以放在任何地方</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">tar -zxvf spark-3.0.1-bin-hadoop3.2.tgz -C /opt<br><span class="hljs-built_in">mv</span> /opt/spark-3.0.1-bin-hadoop3.2/ /opt/spark-3.0.1<br><span class="hljs-comment"># 在~/.bashrc添加，或存在另外一个文件里、需要的时候source</span><br><span class="hljs-built_in">export</span> SPARK_HOME=/opt/spark-3.0.1<br><span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$&#123;SPARK_HOME&#125;</span>/bin:<span class="hljs-variable">$&#123;SPARK_HOME&#125;</span>/sbin:<span class="hljs-variable">$PATH</span><br></code></pre></td></tr></table></figure><h2 id="配置hadoop-yarn"><a href="#配置hadoop-yarn" class="headerlink" title="配置hadoop yarn"></a>配置hadoop yarn</h2><p>让spark使用hadoop资源，只需要配置conf&#x2F;spark-env.sh中的HADOOP_CONF_DIR为$HADOOP_HOME&#x2F;etc&#x2F;hadoop文件夹</p><p>如果使用本仓库提供的单机版hadoop教程，则HADOOP_CONF_DIR&#x3D;&#x2F;home&#x2F;hadoop&#x2F;hadoop&#x2F;etc&#x2F;hadoop&#x2F;</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /opt/spark-3.0.1<br><span class="hljs-built_in">cp</span> conf/spark-env.sh.template conf/spark-env.sh<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;export HADOOP_CONF_DIR=/home/hadoop/hadoop/etc/hadoop/&quot;</span> &gt;&gt; conf/spark-env.sh<br></code></pre></td></tr></table></figure><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><p>local启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">spark-shell --master <span class="hljs-built_in">local</span><br></code></pre></td></tr></table></figure><p>yarn启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">spark-shell --master yarn --deploy-mode client<br></code></pre></td></tr></table></figure><h1 id="pysaprk搭建"><a href="#pysaprk搭建" class="headerlink" title="pysaprk搭建"></a>pysaprk搭建</h1><p>创建虚拟环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda create -n &#123;your_env_name&#125; python=3.7<br><span class="hljs-comment"># centos</span><br>conda activate &#123;your_env_name&#125;<br><span class="hljs-comment"># ubuntu</span><br><span class="hljs-built_in">source</span> activate &#123;your_env_name&#125;<br></code></pre></td></tr></table></figure><p>安装pyspark，注意版本对应</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda install pyspark==3.0.0<br></code></pre></td></tr></table></figure><h1 id="本地搭建时需要hadoop包"><a href="#本地搭建时需要hadoop包" class="headerlink" title="本地搭建时需要hadoop包"></a>本地搭建时需要hadoop包</h1><p>如果要使用hadoop s3 client之类的，本地部署的情况下需要手动把jar拷过去，要下载与spark pre-build相同版本的hadoop</p><p>如果你需要知道是什么版本，可以通过以下命令查看，hadoop-…-3.2.0.jar代表着pre-build为hadoop-3.2.0</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">ls</span> <span class="hljs-variable">$&#123;SPARK_HOME&#125;</span>/jars/ | grep hadoop<br></code></pre></td></tr></table></figure><p>以下这个命令拷贝了所有的，实际上你可以选择你需要的</p><p>如s3 client只需要aws和s3相关的几个包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cp</span> <span class="hljs-variable">$&#123;HADOOP_HOME&#125;</span>/share/hadoop/tools/lib/* <span class="hljs-variable">$&#123;SPARK_HOME&#125;</span>/jars/<br></code></pre></td></tr></table></figure><h1 id="Troubleshooting"><a href="#Troubleshooting" class="headerlink" title="Troubleshooting"></a>Troubleshooting</h1><h2 id="NoSuchMethod"><a href="#NoSuchMethod" class="headerlink" title="NoSuchMethod"></a>NoSuchMethod</h2><p>注意pyspark版本，spark 3.0.1可以尝试pyspark 3.0.0或pyspark 3.0.1</p><h2 id="NoSuch…（没有这个类、对象等）"><a href="#NoSuch…（没有这个类、对象等）" class="headerlink" title="NoSuch…（没有这个类、对象等）"></a>NoSuch…（没有这个类、对象等）</h2><p>注意jdk版本，建议更新到最新java8u25x</p><p>注意scala版本，在保证spark版本和scala版本对应的前提下，使用最新2.12.1x或2.11.1x</p>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>大数据</tag>
      
      <tag>spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🖊大数据相关笔记</title>
    <link href="/2020/10/03/2020-10-04-bigData/"/>
    <url>/2020/10/03/2020-10-04-bigData/</url>
    
    <content type="html"><![CDATA[<h1 id="大数据相关笔记"><a href="#大数据相关笔记" class="headerlink" title="大数据相关笔记"></a>大数据相关笔记</h1><h2 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h2><h3 id="大数据特征"><a href="#大数据特征" class="headerlink" title="大数据特征"></a>大数据特征</h3><p>5V：容量（Volume）大量化、速率（Velocity）快速化、多样性（Variety）多样化、真实性（Veracity）、价值（Value）价值密度低</p><p>有时不谈真实性 只有4V</p><h3 id="不同数据类型"><a href="#不同数据类型" class="headerlink" title="不同数据类型"></a>不同数据类型</h3><h4 id="结构化数据"><a href="#结构化数据" class="headerlink" title="结构化数据"></a>结构化数据</h4><p>结构化数据遵循一个标准的模型，或者模式，并且常常以表格的形式存储。该类型数据通常用来捕捉不同对象之间的关系，并且存储在关系型数据库中。</p><h4 id="非结构化数据"><a href="#非结构化数据" class="headerlink" title="非结构化数据"></a>非结构化数据</h4><p>非结构化数据是指不遵循统一的数据模式或者模型的数据。存储和处理非结构化数据通常需要专用的逻辑，如存储一部视频，需要正确的编码和解码。非结构化数据不能被直接处理或者用SQL语句查询。如果他们需要存储在关系型数据库中，它们会以二进制大型对象（BLOB）的形式存储在表中。当然，NoSQL数据库作为一个非关系型数据库，能够用来同时存储结构化和非结构化数据。</p><h4 id="半结构化数据"><a href="#半结构化数据" class="headerlink" title="半结构化数据"></a>半结构化数据</h4><p>半结构化数据有一定的结构或一致性约束，但本质上不具有关系型。半结构化数据是层次性的或基于图像的。这类数据常常储存在文本文件中，如XML文件和JSON文件是两类常见的半结构化数据。</p><h4 id="元数据"><a href="#元数据" class="headerlink" title="元数据"></a>元数据</h4><p>元数据提供了一个数据集的特征和结构信息。这种数据主要由机器生成，并且能够添加到数据集中。搜索元数据对于大数据存储、处理和分析是至关重要的，因为元数据提供了数据系谱信息，以及数据处理的起源。例如XML文件中提供作者和创建日期信息的标签、数码照片中提供文件大小和分辨率的属性文件。</p><h2 id="大数据存储概念"><a href="#大数据存储概念" class="headerlink" title="大数据存储概念"></a>大数据存储概念</h2><h3 id="集群"><a href="#集群" class="headerlink" title="集群"></a>集群</h3><p>一个集群是紧密耦合的一些服务器或节点。这些服务器通常有相同的硬件规格并且通过网络连接在一起作为一个工作单元。一个集群通常是多台机器组合提供一个服务。</p><h3 id="分布式文件系统"><a href="#分布式文件系统" class="headerlink" title="分布式文件系统"></a>分布式文件系统</h3><p>一个分布式文件系统作为一个文件系统，可以存储分布在集群节点上的文件。对于客户端来说，文件似乎在本地，但物理形式上文件分布于整个集群。</p><h3 id="NoSQL"><a href="#NoSQL" class="headerlink" title="NoSQL"></a>NoSQL</h3><p>Not-only SQL是一个非关系型数据库，具有高度的可扩展性、容错性，并专门设计用来存储半结构化和非结构化数据。NoSQL数据库通常会提供一个能被应用程序调用的基于API的查询接口。NoSQL数据库也支持SQL以外的查询语言。</p><h3 id="分片"><a href="#分片" class="headerlink" title="分片"></a>分片</h3><p>分片是<strong>水平</strong>地将一个大的数据集划分成较小的、更易于管理的数据集的过程，这些数据集叫碎片。碎片分布在多个节点，而节点是一个服务器或者一台机器。每个碎片存储在一个单独的节点上，每个节点只负责存储在该节点上的数据。所有碎片都是同样的模式，所有碎片集合起来代表完整的数据集。</p><p>PS：就是把表的行存在不同的节点上</p><p>分片对于客户端来说通常是透明的。分片允许处理负荷分布在多个节点上以实现水平可伸缩性。这样做，每个节点只负责整个数据集的一部分，整行读写可化为多个节点的并行操作。</p><p>PS：若纵向切割，则每个节点保有某个属性的一部分，单行操作可以化为多个节点的并行操作。</p><h3 id="复制"><a href="#复制" class="headerlink" title="复制"></a>复制</h3><p>复制在多个节点上存储数据集的多个拷贝，被叫做副本。复制因为相同的数据在不同的节点上复制的原因提供了可伸缩性和可用性。数据容错也可以通过数据冗余来实现。数据冗余确保单个节点失败时数据不会丢失。</p><p>有两种不同的方法用于实现复制：</p><ul><li>主从式复制</li><li>对等式复制</li></ul><h4 id="主从式复制"><a href="#主从式复制" class="headerlink" title="主从式复制"></a>主从式复制</h4><p>主从式复制中，节点被安排在一个主从配置中，所有数据都被写入主节点，一旦保存，数据就被复制到多个从节点。</p><p>一个从节点可以作为备份节点配置主节点。如果主节点失败，直到主节点被恢复为止将不能进行写操作。主节点要么时从主节点的一个备份中回复，要么在从节点中选择一个作为新的主节点。</p><h5 id="读写不一致问题："><a href="#读写不一致问题：" class="headerlink" title="读写不一致问题："></a>读写不一致问题：</h5><p>如果一个从节点在被更新到主节点的新状态之前，被用户读取，便会产生这样的问题。为了确保读一致性，可以实现一个投票系统，若大多数从节点都包含相同版本的记录，则可以声明一个读操作是一致性的。实现这样的投票系统需要从节点之间有相应的快速可靠通讯机制。</p><p>场景：</p><ol><li>用户A更新数据</li><li>数据从主节点复制到节点a</li><li>在数据复制到节点b之前，用户B试图从节点b读取数据，导致不一致的读操作</li><li>当数据从主节点复制到从节点b之后，数据一致</li></ol><h4 id="对等式复制"><a href="#对等式复制" class="headerlink" title="对等式复制"></a>对等式复制</h4><p>对等式复制，所有节点在同一水平上运作。各个节点之间没有主从节点的关系。每个称为对等的节点也同样能够处理读请求和写请求。每个写操作复制到所有对等节点中去。</p><p>对等式复制容易造成写不一致，写不一致发生在同时更新同一个数据的多个对等节点的时候。这可以通过实现一个悲观或乐观并发策略来解决这个问题。</p><ul><li>悲观并发是一种防止不一致的有前瞻性的策略。它使用锁来确保一个记录上同一个时间只有一个更新操作可能发生。<strong>然而这种方法的可用性较差，因为正在被更新的数据库记录一直是不可用的，直到所有锁被释放。</strong></li><li>乐观并发是一个被动的策略，它不使用锁。相反，它允许不一致性在所有更新都被实现后，最终可以获得一致性的前提下发生。实现上可以使用一个时间戳，接受时间戳最新的更新。</li></ul><p>场景：</p><ol><li><p>用户A更新数据</p></li><li><p>数据被复制到对等节点A</p><p>数据被复制到对等节点B</p></li><li><p>在数据被复制到对等节点C之前，用户B试图从对等节点C读取数据，导致读不一致</p></li><li><p>最终数据将被更新到对等节点C中，数据库一致</p></li></ol><h3 id="分片-复制"><a href="#分片-复制" class="headerlink" title="分片+复制"></a>分片+复制</h3><p>为了改善分片机制所提供的有限的容错能力（部分行可用），而另外受益于增加的复制的可用性和伸缩性，可以将分片和复制组合使用。</p><ul><li>分片和主从式复制</li><li>分片和对等式复制</li></ul><h4 id="分片和主从式复制"><a href="#分片和主从式复制" class="headerlink" title="分片和主从式复制"></a>分片和主从式复制</h4><p>多个主分片分布在各个节点中。从分片提供读服务，并通过主分片进行更新，为读操作提供可扩展性和容错性。</p><p>如果主分片变为不可操作或网络出现故障，写操作的容错能力将受到影响。</p><h4 id="分片和对等式复制"><a href="#分片和对等式复制" class="headerlink" title="分片和对等式复制"></a>分片和对等式复制</h4><p>每个碎片被复制到多个对等节点，每个对等节点仅仅负责整个数据集的一个子集。总的来说，这有助于实现更好的可扩展性和容错性。由于不涉及主节点，所以不存在单点故障，并且支持读操作和写操作的容错性。</p><p>同样的，也继承了对等复制的写不一致问题，大数据下多分片同步过程可能导致网络拥塞等</p><h3 id="CAP定理"><a href="#CAP定理" class="headerlink" title="CAP定理"></a>CAP定理</h3><ul><li><p>一致性（Consistency）：从任何节点的读操作都是一致的</p></li><li><p>可用性（Availability）：任何一个读&#x2F;写请求总是会以成功或失败的形式得到相应</p></li><li><p>分区容忍（Partition tolerance）：数据库系统可以容忍通信中断，通过将集群分成多个竖井，仍然可以队读&#x2F;写请求提供服务（挂了几个节点还是部分可用）</p></li></ul><p>CAP三角：CAP不能同时获得</p><ul><li>如果CA是必需的，则需要保持通信，P不成立</li><li>如果CP是必需的，则不能保持可用性A</li><li>如果AP是必需的，则由于通讯时间等，不能保证一致性C</li></ul><p>分布式数据库系统中，不能保证100%的通达可用，因此P是必需的，CAP通常是C+P OR A+P的组合</p><h3 id="ACID"><a href="#ACID" class="headerlink" title="ACID"></a>ACID</h3><p>数据库设计原则与数据管理</p><ul><li>原子性（Atomicity）</li><li>一致性（Consistency）</li><li>隔离（Isolation）</li><li>持久性（Durability）</li></ul><h3 id="BASE"><a href="#BASE" class="headerlink" title="BASE"></a>BASE</h3><p>BASE是一个根据CAP定理而来的数据库设计原则，它采用了使用分布式技术的数据库系统。</p><ul><li>基本可用（Basically Available）：始终相应用户请求，但可能由于网络原因，节点间通讯中断</li><li>软状态（Soft State）：同步的中间状态，意味着读不一致可能出现</li><li>最终一致性（Eventual Consistency）</li></ul><h2 id="大数据处理概念"><a href="#大数据处理概念" class="headerlink" title="大数据处理概念"></a>大数据处理概念</h2><h3 id="并行数据处理"><a href="#并行数据处理" class="headerlink" title="并行数据处理"></a>并行数据处理</h3><p>将一个大规模的任务分成多个子任务同时进行，目的是减少处理时间。较为典型的方法是在一台机器上使用多个处理器或内核完成。</p><h3 id="分布式数据处理"><a href="#分布式数据处理" class="headerlink" title="分布式数据处理"></a>分布式数据处理</h3><p>与并行数据处理非常相似，将一个大规模任务分成多个子任务。但不同的是，分布式数据处理通常在几个物理上分离的机器上进行，这些机器通过网络连接构成了一个集群。</p><h3 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h3><p>Hadoop实现了MapReduce处理框架，被公认为当代大数据解决方案的工业平台。Hadoop可以作为ETL引擎与分析引擎来处理大量结构化、半结构化、非结构化数据。</p><p>这一部分的内容我将单独出blog</p><h3 id="处理工作量"><a href="#处理工作量" class="headerlink" title="处理工作量"></a>处理工作量</h3><p>大数据的处理工作量被定义为一定时间内处理数据的性质与数量。处理工作量主要被分为以下两个类型</p><ul><li>批处理型</li><li>事务型</li></ul><h4 id="批处理型"><a href="#批处理型" class="headerlink" title="批处理型"></a>批处理型</h4><p>批处理也称脱机处理，这种方法通常成批地处理数据，因此会导致较大的延迟。</p><p>通常我们采用批处理完成大数据有序的读&#x2F;写操作，这些读&#x2F;写查询通常是成批的。</p><h4 id="事务型"><a href="#事务型" class="headerlink" title="事务型"></a>事务型</h4><p>事务型处理也称为在线处理，这种处理方式通过无延迟的交互式处理使得整个回应延迟很小。事务型处理一般适用于少量数据的随机读&#x2F;写操作</p><h3 id="集群-1"><a href="#集群-1" class="headerlink" title="集群"></a>集群</h3><p>集群能为水平可扩展的存储方案提供必要的支持，也能为分布式数据处理提供一种线性的扩展机制（即存储和计算的集群）。集群有极高的可扩展性，因而它可以把大的数据集分成多个更小的数据集，以分布式的方法并行处理，这种特性为大数据处理提供了理想的环境。</p><p>由于集群由物理连接上相互独立的设备组成，它具有固定的冗余与一定的容错性，因而当网络中某个节点发生错误时，它之前处理与分析的结果都是可恢复的。考虑到大数据处理过程偶尔有些不稳定，我们通常采用云主机基础设施服务或线程的分析环境作为集群的主干。</p><p>目前由很多分布式解决方案采用docker引擎，能够进一步提高冗余性，降低机器数量对集群稳定性的影响。</p><h3 id="批处理模式"><a href="#批处理模式" class="headerlink" title="批处理模式"></a>批处理模式</h3><p>批处理模式中，数据总是成批地脱机处理，响应时长从几分钟到几小时不等。该模式本质上解决了大数据数据量大、数据特性不同的问题。</p><p>批处理是大数据处理的主要方式，相较于实时模式，它更简单、易于建立、开销也较小。</p><p>商务智能（BI）、预测性分析与规范性分析、ETL操作一般都使用批处理模式</p><h4 id="MapReduce批处理"><a href="#MapReduce批处理" class="headerlink" title="MapReduce批处理"></a>MapReduce批处理</h4><p>MapReduce是一种广泛用于实现批处理的架构，它采用“分治”的原则，把一个大的问题分成可以被分别解决的小问题的集合，拥有内部容错性与冗余，因而具有很高的可扩展性与可靠性。</p><p>MapReduce把数据处理算法分发到各个存储节点，数据在这些节点上被并行地处理，这种方法可以消除发送数据的时间开小。由于并行处理小规模数据速度更快，MapReduce不但可以节约网络带宽开销，更能大量节约处理大规模数据的时间开销。</p><p>Hadoop实现的MapReduce将中间结果存储在hdfs（文件系统）中，因此对磁盘大量的读写是性能瓶颈所在。</p><p>Spark针对Hadoop的问题，将中间结果保存在内存中，增加了性能的同时也提高了机器配置的需要。</p><h4 id="Map和Reduce任务"><a href="#Map和Reduce任务" class="headerlink" title="Map和Reduce任务"></a>Map和Reduce任务</h4><p>一次MapReduce处理引擎的运行被称为MapReduce作业，它由Map和Reduce两部分任务组成，即映射-归约。</p><p>映射任务被分为映射（Map）、合并（combine）、分区（partition）三个阶段，其中合并时可选的</p><p>归约任务被分为洗牌和排序（shuffle and sort）、归约（reduce）两个阶段</p><h5 id="映射"><a href="#映射" class="headerlink" title="映射"></a>映射</h5><p>映射首先把大的数据文件分割成多个小数据文件，每个较小的数据文件的每条记录都被解析为一组键值对。</p><p>（K1, V1）-&gt; list（K2, V2）</p><h5 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h5><p>对键值对进行合并，减少数据由映射任务节点传输到归约任务节点的带宽消耗</p><p>（K2, list（V2））-&gt; list（K2, V2）</p><h5 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h5><p>当使用多个规约模块时，MapReduce模型就需要把映射模块或合并模块的输出分配给各个归约模块。在此，我们把分配到每个归约模块的数据叫做一个分区，也就是说，分区数与归约模块数时相等的。</p><p>（K2, V2）-&gt;（index）</p><p>一个分区包含很多记录，同一键的记录必须被分配在同一个分区。在此基础上，MapReduce模型会尽量保证随机公平地把数据分配到各个归约模块中。</p><p>由于上述分区模块特性，会导致分配到各个归约模块地数据量由差异，甚至分配给某个规约模块地数据量会远远超过其他。不均等地工作量将导致各个归约模块地工作结束时间不同，导致空等地情况发生，耗时增加。</p><h5 id="洗牌和排序"><a href="#洗牌和排序" class="headerlink" title="洗牌和排序"></a>洗牌和排序</h5><p>洗牌包括由分区模块将数据传输到归约模块的整个过程，是规约任务的第一个阶段。由分区模块传输来的数据可能存在多条记录对应同一个键，这个模块将把这些相同的键进行组合，形成唯一的键值对列表。</p><p>（K2, V2）-&gt;K2, list（V2）</p><h5 id="归约"><a href="#归约" class="headerlink" title="归约"></a>归约</h5><p>最后一个阶段，对输入的记录进行进一步分析归纳，也可能对输入不做任何改变。在任何情况下，这个模块都在处理当条记录的同时将其他处理过的记录输出。</p><p>分配到每个归约模块的分区都将被合并成一个文件。</p><p>（K2, list（V2））-&gt; list（K3, V3）</p><h4 id="理解MapReduce算法"><a href="#理解MapReduce算法" class="headerlink" title="理解MapReduce算法"></a>理解MapReduce算法</h4><p>MapReduce编程遵循一套特定的逻辑，采用了分治的原则。</p><ul><li>任务并行：将一个任务分为多个子任务，在不同的节点上并行。通常并行的子任务采用不同的算法，每个子任务的输入数据可以相同也可以不同，最后多个子任务的结果组成最终结果。</li><li>数据并行：将一个数据集分为多个子数据集在多个节点上并行地处理，数据并行的多个节点采用同一算法，最后多个子数据集的处理结果组成最终结果。</li></ul><p>设计MapReduce算法时考虑的几点：</p><ul><li>尽可能使用简单的算法逻辑，这样才能采用统一函数处理逻辑处理某个数据集的不同部分</li><li>数据集可以被分布式地划分在集群中</li><li>理解数据集的数据结构以保证选取有用的记录</li><li>将算法逻辑分为映射部分与归约部分</li><li>保证映射函数的输出时正确有效的</li><li>保证归约函数的输出时正确的</li></ul><h3 id="实时处理模式"><a href="#实时处理模式" class="headerlink" title="实时处理模式"></a>实时处理模式</h3><p>实时模式中，数据通常在写入磁盘之前在内存中进行处理，侧重于提高大数据处理的速度。</p><h4 id="SCV原则"><a href="#SCV原则" class="headerlink" title="SCV原则"></a>SCV原则</h4><h5 id="速度（Speed）"><a href="#速度（Speed）" class="headerlink" title="速度（Speed）"></a>速度（Speed）</h5><p>数据生成后处理的速度，此处忽略获取数据的时间消耗，专注于实际数据处理的时间消耗</p><h5 id="一致性（Consistency）"><a href="#一致性（Consistency）" class="headerlink" title="一致性（Consistency）"></a>一致性（Consistency）</h5><p>一致性指处理结果的准确性与精度，高一致性系统通常会利用全部数据来保证其准确度与精度，低一致性系统则采用采样技术</p><h5 id="容量（Volume）"><a href="#容量（Volume）" class="headerlink" title="容量（Volume）"></a>容量（Volume）</h5><p>容量指系统能够处理的数据量</p><h5 id="SCV不可能三角"><a href="#SCV不可能三角" class="headerlink" title="SCV不可能三角"></a>SCV不可能三角</h5><p>如果保证数据处理系统的速度和一致性，就不能保证大容量</p><p>如果保证高一致性处理大容量数据，则处理速度必然减慢</p><p>如果保证高速的大容量数据处理，则系统的高一致性将受到影响</p><p>实时处理中，保证数据不丢失，对数据处理容量V的需求较大，因此需要在S和C中做权衡，实现S+V或C+V</p>]]></content>
    
    
    <categories>
      
      <category>读书笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据抽样</tag>
      
      <tag>大数据</tag>
      
      <tag>spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🖊数据抽样相关笔记</title>
    <link href="/2020/09/20/2020-09-21-data-sampling/"/>
    <url>/2020/09/20/2020-09-21-data-sampling/</url>
    
    <content type="html"><![CDATA[<h1 id="数据抽样相关笔记"><a href="#数据抽样相关笔记" class="headerlink" title="数据抽样相关笔记"></a>数据抽样相关笔记</h1><p><img src="/../img/2020-09-21-data-sampling/datasampling.png"></p><h2 id="抽样"><a href="#抽样" class="headerlink" title="抽样"></a><a href="%5Bhttps://zh.wikipedia.org/wiki/%E6%8A%BD%E6%A8%A3%5D(https://zh.wikipedia.org/wiki/%E6%8A%BD%E6%A8%A3)">抽样</a></h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>在<a href="https://zh.wikipedia.org/wiki/%E7%BB%9F%E8%AE%A1%E5%AD%A6">统计学</a>中，<strong>抽样</strong>（Sampling）是一种<a href="https://zh.wikipedia.org/wiki/%E6%8E%A8%E8%AB%96%E7%B5%B1%E8%A8%88%E5%AD%B8">推论统计</a>方法，它是指从目标<a href="https://zh.wikipedia.org/wiki/%E6%80%BB%E4%BD%93_(%E7%BB%9F%E8%AE%A1%E5%AD%A6)">总体</a>（Population，或称为母体）中抽取一部分个体作为<a href="https://zh.wikipedia.org/wiki/%E6%A8%A3%E6%9C%AC_(%E7%B5%B1%E8%A8%88%E5%AD%B8)">样本</a>（Sample），通过观察样本的某一或某些属性，依据所获得的数据对总体的数量特征得出具有一定可靠性的估计判断，从而达到对总体的认识。</p><h4 id="抽样误差（sample-error）样本框与样本之间的非可观察差距"><a href="#抽样误差（sample-error）样本框与样本之间的非可观察差距" class="headerlink" title="抽样误差（sample error）样本框与样本之间的非可观察差距"></a>抽样误差（sample error）样本框与样本之间的非可观察差距</h4><p>由于缺失了总体中的一部分人而在统计上产生的误差</p><p>基于以下四个设计原则</p><ol><li>抽样框整体是否已知，入选样本的概率非零，即抽样概率（probability sampling）</li><li>框中的关键子总体是否有样本代表，即分层（stratification）</li><li>样本是在抽样框中直接、独立抽取的还是分组抽取的，即个体样本（element sample）或整群样本（cluster sample）</li><li>选择多大的样本量</li></ol><h4 id="抽样方差"><a href="#抽样方差" class="headerlink" title="抽样方差"></a>抽样方差</h4><p>样本均值与样本框均值之间的平方差，如果抽样方差高，则样本均值就很不稳定。在这种情况下，抽样误差就很高。这就意味着，任何使用这类设计的调查，有更大的可能使得从调查中获得的均值会远离抽样总体的均值。</p><h4 id="评价方法"><a href="#评价方法" class="headerlink" title="评价方法"></a>评价方法</h4><p>网络评价和<strong>K-S检验</strong></p><h3 id="抽样过程"><a href="#抽样过程" class="headerlink" title="抽样过程"></a>抽样过程</h3><p>抽样过程主要包括以下几个阶段：</p><ol><li>定义总体（母体）</li><li>确定<a href="https://zh.wikipedia.org/w/index.php?title=%E6%8A%BD%E6%A0%B7%E6%A1%86&action=edit&redlink=1">抽样框</a></li><li>确定<a href="https://zh.wikipedia.org/w/index.php?title=%E6%8A%BD%E6%A0%B7%E6%96%B9%E6%B3%95&action=edit&redlink=1">抽样方法</a></li><li>决定<a href="https://zh.wikipedia.org/w/index.php?title=%E6%A0%B7%E6%9C%AC%E9%87%8F&action=edit&redlink=1">样本量</a></li><li>实施抽样计划</li><li>抽样与数据收集</li><li>回顾抽样过程</li></ol><h3 id="概率抽样方法-（调查方法P83）"><a href="#概率抽样方法-（调查方法P83）" class="headerlink" title="概率抽样方法 （调查方法P83）"></a>概率抽样方法 （调查方法P83）</h3><p><strong>简单随机抽样</strong>（simple random sampling），也叫纯随机抽样。从总体N个单位中随机地抽取n个单位作为样本，使得每一个容量为样本都有相同的概率被抽中。特点是：每个样本单位被抽中的概率相等，样本的每个单位完全独立，彼此间无一定的关联性和排斥性。简单随机抽样是其它各种抽样形式的基础。通常只是在总体单位之间差异程度较小和数目较少时，才采用这种方法[<a href="https://zh.wikipedia.org/wiki/%E6%8A%BD%E6%A8%A3#cite_note-1">1]</a>。</p><p><a href="https://zh.wikipedia.org/wiki/File:Simple_random_sampling.PNG"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Simple_random_sampling.PNG/300px-Simple_random_sampling.PNG" alt="img"></a></p><div align = "center">选择简单随机样本的示意图</div><p>蓄水池抽样是当样本总体数量未知时，仅通过遍历一边样本即可完成的简单抽样算法。</p><p><strong>系统抽样</strong>（systematic sampling），也称等距抽样。将总体中的所有单位按一定顺序排列，在规定的范围内随机地抽取一个单位作为初始单位，然后按事先规定好的规则确定其他样本单位。先从数字1到k之间随机抽取一个数字r作为初始单位，以后依次取r+k、r+2k……等单位。这种方法操作简便，可提高估计的精度。</p><p><a href="https://zh.wikipedia.org/wiki/File:Systematic_sampling.PNG"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Systematic_sampling.PNG/350px-Systematic_sampling.PNG" alt="img"></a></p><div align = "center">使用系统抽样技术选择随机样本的示意图</div><p>**<a href="https://zh.wikipedia.org/wiki/%E5%88%86%E5%B1%82%E6%8A%BD%E6%A0%B7">分层抽样</a>**（stratified sampling）。将抽样单位按某种特征或某种规则划分为不同的层，然后从不同的层中独立、随机地抽取样本。从而保证样本的结构与总体的结构比较相近，从而提高估计的精度。难点是如何按照特征划分为不同的层。</p><p><a href="https://zh.wikipedia.org/wiki/File:Stratified_sampling.PNG"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Stratified_sampling.PNG/300px-Stratified_sampling.PNG" alt="img"></a></p><div align = "center">使用分层抽样技术选择随机样本的示意图</div><p><strong>整群抽样</strong>（cluster sampling）。将总体中若干个单位合并为组，抽样时直接抽取群，然后对中选群中的所有单位全部实施调查。抽样时只需群的抽样框，可大大简化工作量，缺点是估计的精度较差[<a href="https://zh.wikipedia.org/wiki/%E6%8A%BD%E6%A8%A3#cite_note-2">2]</a>。比如美国总统选举，直接抽取某个郡的选民，则可能出现明显的倾向。但如果抽取一年级、四年级、八年级的学生进行调查，则可以反应不同年龄段学生的状态，并分析其变化。如果不考虑走访的开销，则整群抽样的表现与简单随机抽样相当或更差。</p><h3 id="非概率抽样方法"><a href="#非概率抽样方法" class="headerlink" title="非概率抽样方法"></a>非概率抽样方法</h3><p><strong>方便抽样</strong>（Convenience Sampling）。调查者以自己方便的方式抽取偶然得到的样本，最典型的方便抽样是“街头拦人法”。方便抽样的优点是易于实施，代价较小，缺点是样本代表性差，有很大的偶然性。</p><p><strong>定额抽样</strong>（Quota Sampling）。调查者先将总体按某种特征划分成不同的组，然后在配额内以主观判断选定样本作为研究对象。定额抽样和分层抽样的相同之处是对总体进行分组，不同之处是分层抽样按概率原则在层内抽选样本，而定额抽样选取样本是主观的。定额抽样的优点是能够缩小抽样范围，减少抽样成本，缺点是确定额度困难，需多次探索。</p><p><strong>判断抽样</strong>（Judgement Sampling）。研究人员根据调查目的和主观经验，从总体中选择最具代表性的样本。判断抽样的优点是可以用于总体难以确定的研究对象，缺点是受研究人员的主观倾向性影响大，一旦主观判断失误， 则易引起较大的抽样偏差。</p><p><strong>滚雪球抽样</strong>（Snowball Sampling）。先选取若干符合特征的样本构成最初的调查对象，然后依靠他们提供新的调查对象，随着调查的推进，样本如同滚雪球般由小变大，滚雪球抽样方法的优点是能够很方便地找到被调查者，用于探索性研究，缺点是样本之间必须存在联系且愿意保持和提供这种联系。 [2]</p><h3 id="机器学习中的抽样"><a href="#机器学习中的抽样" class="headerlink" title="机器学习中的抽样"></a>机器学习中的抽样</h3><p><strong>留出法（hold-out）</strong></p><p>方法：直接将数据集D划分为两个互斥的集合，训练集合S和测试集合T，在S上训练模型，用T来评估其测试误差。注意：训练&#x2F;测试集的划分要尽可能保持数据分布的一致性，避免因为数据划分过程引入额外的偏差而对最终结果产生影响。</p><p>缺点与改进：单次使用留出法得到的估计往往不够稳定可靠，在使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果</p><p>实际运用：实际中一般将大约2&#x2F;3～4&#x2F;5的样本用于训练，剩余样本用于测试。 [3]</p><p><strong>交叉验证法（cross validation）</strong></p><p>方法：先将数据集D划分为k个大小相似的互斥子集.每个子集Di都尽可能保持数据分布的一致性，即从D中通过分层采样得到 .然后每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就可以获得k组训练&#x2F;测试集，从而可以进行k次训练和测试，最终返回的是这k个测试结果的均值。</p><p>实际运用：一般而言k的取值为10，常用的还有5、20等</p><p><strong>自助法</strong></p><p>问题引出：我们希望评估的是用D训练出来的模型，但是留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差，为此提出自助法。</p><p>方法：它以自助采样(bootstrap sampling)为基础.给定包含m个样本的数据集D，我们对它进行采样产生数据集 D′：每次随机从D中挑选出一个样本，将其拷贝放入D′, 然后再将该样本放回初始数据集D中，使得该样本在下次采样时仍有可能被采样到；这个过程重复执行m次后，我们就得到可包含m个样本数据的数据集D′,这就是自助采样的结果.样本在m次采样中始终不被采到到概率为</p><p><img src="/../img/2020-09-21-data-sampling/gs1.png" alt="img"></p><p>由此可知通过自助采样，初始数据集D中约有36.8%的样本未出现在采样数据集D′中。于是我们可将D′ 用作训练集，D∖D′用作测试集。</p><p>优缺点：自助法在数据集较小，难以有效划分训练&#x2F;测试集时很有用，但是，自助法改变了初始数据集的分布，这会引入估计偏差，所以在数据量足够时，一般采用留出法和交叉验证法。 [4]</p><h2 id="数据抽样技术"><a href="#数据抽样技术" class="headerlink" title="数据抽样技术"></a>数据抽样技术</h2><p>在当前数据搜集和存储技术不断发展、数据量激增的背景下，数据抽样技术可以在稍微降低准确性的情况下经济、快速地得到预测、估计和有代表性的结果，在众多领域都发挥着重要作用。</p><h3 id="聚合查询"><a href="#聚合查询" class="headerlink" title="聚合查询"></a>聚合查询</h3><p>聚合查询操作根据聚合函数的复杂程度可以分为简单聚合查询和复杂聚合查询。</p><p>简单聚合查询：求sum、max、avg</p><p>复杂符合查询：TopK，KNN，KMeans</p><p>传统的聚合查询采用精确查询的方式，对所有数据都进行聚合操作。比如遍历所有数据才能得到sum。然而在满足应用可靠性要求的前提下，实际应用为了能在短时间内得到查询结果，可以容忍一定误差。可以通过数据抽样的方法，在聚合查询之前得到数据总体的子集。只要抽样的样本代表性足够，则可大幅度提高聚合查询的效率。</p><h3 id="数据保护"><a href="#数据保护" class="headerlink" title="数据保护"></a>数据保护</h3><p>数据抽样技术能够使分析师操作数据的子集，而不会损害到原数据。数据抽样的过程中可以添加其他操作，如数据脱敏等，让敏感信息不外泄，做到安全的数据抽样。</p><h3 id="困难"><a href="#困难" class="headerlink" title="困难"></a>困难</h3><p><img src="/../img/2020-09-21-data-sampling/restrict.png"></p><h4 id="数据不平衡：利用分层抽样减少数据不平衡的情况"><a href="#数据不平衡：利用分层抽样减少数据不平衡的情况" class="headerlink" title="数据不平衡：利用分层抽样减少数据不平衡的情况"></a>数据不平衡：利用分层抽样减少数据不平衡的情况</h4><h5 id="长尾现象"><a href="#长尾现象" class="headerlink" title="长尾现象"></a>长尾现象</h5><p><a href="https://baike.baidu.com/item/%E9%95%BF%E5%B0%BE">长尾</a>效应，英文名称Long Tail Effect。“头”（head）和“尾”（tail）是两个统计学名词。正态曲线中间的突起部分叫“头”；两边相对平缓的部分叫“尾”。从人们需求的角度来看，大多数的需求会集中在头部，而这部分我们可以称之为流行，而分布在尾部的需求是个性化的，零散的小量的需求。而这部分差异化的、少量的需求会在<a href="https://baike.baidu.com/item/%E9%9C%80%E6%B1%82%E6%9B%B2%E7%BA%BF/3351682">需求曲线</a>上面形成一条长长的“尾巴”，而所谓长尾效应就在于它的数量上，将所有非流行的市场累加起来就会形成一个比流行市场还大的市场。</p><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Long_tail.svg/800px-Long_tail.svg.png" alt="File:Long tail.svg"></p><h4 id="训练成本高："><a href="#训练成本高：" class="headerlink" title="训练成本高："></a>训练成本高：</h4><h5 id="模型大"><a href="#模型大" class="headerlink" title="模型大"></a>模型大</h5><p>etc..</p><h5 id="数据多"><a href="#数据多" class="headerlink" title="数据多"></a>数据多</h5><p>etc..</p><h3 id="抽象系统必须满足的需求"><a href="#抽象系统必须满足的需求" class="headerlink" title="抽象系统必须满足的需求"></a>抽象系统必须满足的需求</h3><h4 id="分布式支持"><a href="#分布式支持" class="headerlink" title="分布式支持"></a>分布式支持</h4><p>大数据背景下的数据处理大都在分布式环境下进行，数据抽样同样如此</p><h4 id="高处理速度"><a href="#高处理速度" class="headerlink" title="高处理速度"></a>高处理速度</h4><p>时间不等人</p><h4 id="高扩展性"><a href="#高扩展性" class="headerlink" title="高扩展性"></a>高扩展性</h4><p>算法扩展性</p><h4 id="高样本代表性"><a href="#高样本代表性" class="headerlink" title="高样本代表性"></a>高样本代表性</h4><p>最终结果要好！</p><h2 id="相关研究"><a href="#相关研究" class="headerlink" title="相关研究"></a>相关研究</h2><h3 id="基于大规模数据集的可扩展算法"><a href="#基于大规模数据集的可扩展算法" class="headerlink" title="基于大规模数据集的可扩展算法"></a>基于大规模数据集的可扩展算法</h3><p>交替方向乘子法（Alternating Dirention Method of Multipliers，ADMM）：用于解决海量数据环境下，分布式，凸优化问题。ADMM通过分解-协调过程（Decomposition-Corrdination），首先将一个大型的全局问题分解（Decomposition）成若干个较小且较易求解的局部子问题，再通过协调（Corrdination）这些局部子问题的解来得到全局解。ADMM的框架min(f(x)+g(z))与大部分机器学习问题的误差函数+正则项形式吻合，使得ADMM非常适合机器学习和统计学习的优化问题。</p><p>Mahout项目：基于hadoop的机器学习和数据挖掘的分布式框架，实现了一些常用的可伸缩的机器学习算法以运用再大数据集上。Mahout基于MapReduce框架的并行性实现了部分数据挖掘算法，如推荐算法、聚类算法、分类算法等</p><h3 id="真实应用场景下的数据抽样算法"><a href="#真实应用场景下的数据抽样算法" class="headerlink" title="真实应用场景下的数据抽样算法"></a>真实应用场景下的数据抽样算法</h3><h4 id="社交网络"><a href="#社交网络" class="headerlink" title="社交网络"></a>社交网络</h4><p>针对在线社交网络的Multigraph抽样算法：在真实的大规模社交网络分析中，样本总体列表是无法获得的。因此无法直接对Social Graph中的点进行抽样得到需要观察的样本集。研究者一般通过网络爬虫的方法，从某些网络顶点开始逐渐向外扩展，期望能收敛到一个连通图，以此作为观察样本。传统的社交网络抽样算法只考虑了个体之间的一种连接关系，仅在此关系对应的Social Graph上进行随机抽取。Multigraph抽样算法考虑了个体之间的多种连接关系，将这些连接关系对应的多张Social Graph结合成一张Multigraph，然后在此基础上进行随机抽样，可以在个体间高度聚集或者个体间连接缺失的情况下生成具有代表性的样本，解决了传统方法在这上面的痛点。</p><p>Stratified Weighter Random Walk（S-WRW）算法，针对不同应用需要得到的不同的目标度量标准，依照分层抽样的思想对原始Social Graph的点和边赋予了不同的权重，使得对当前度量标准包含更多信息的点和边能够更容易被抽样算法选中，大大降低了达到相同估计精度所需要抽取的个体数量，提升了抽样算法的效率。</p><h3 id="基于分布式的分层抽样算法"><a href="#基于分布式的分层抽样算法" class="headerlink" title="基于分布式的分层抽样算法"></a>基于分布式的分层抽样算法</h3><p>基于Hadoop MapReduce平台的分层抽样算法MR-SQE（Map Reduce Single Query Evaluator），基于Naive MapReduce Sampling（NMRS）进行修改</p><p>Spark是一个用于大规模数据处理的分布式计算框架，Meng等人基于Spark平台提出了一个可扩展的机器学习库MLib，在其提供的基本统计功能中包含了基于分布式的分层抽样调用接口</p><h3 id="基于抽样思想的机器学习算法"><a href="#基于抽样思想的机器学习算法" class="headerlink" title="基于抽样思想的机器学习算法"></a>基于抽样思想的机器学习算法</h3><h4 id="基于简单随机抽样的样本调整算法"><a href="#基于简单随机抽样的样本调整算法" class="headerlink" title="基于简单随机抽样的样本调整算法"></a>基于简单随机抽样的样本调整算法</h4><p>随机上采样和随机下采样技术</p><h4 id="基于数据特征的样本生成算法"><a href="#基于数据特征的样本生成算法" class="headerlink" title="基于数据特征的样本生成算法"></a>基于数据特征的样本生成算法</h4><h5 id="上采样：从少数中发展"><a href="#上采样：从少数中发展" class="headerlink" title="上采样：从少数中发展"></a>上采样：从少数中发展</h5><p>SMOTE算法，利用小众样本在特征空间的相似性，采用插值的方式从相似的小众样本群生成新的样本</p><p>Borderline-SMOTE基于SMOTE做了改进，添加了阈值</p><p>ADASYN算法，SMOTE的拓展，特点是中间边界的样本生成数量更多</p><h5 id="下采样：从多数中删减"><a href="#下采样：从多数中删减" class="headerlink" title="下采样：从多数中删减"></a>下采样：从多数中删减</h5><p>ENN算法：采用KNN，将离群点删除</p><p>Tomek Links（TL）算法：TL关系指a样本和b样本间不存在C，使a更靠近c或b更靠近c——d(a,c)&lt;d(a,b) or d(b,c)&lt;d(a,b)，d(x,y)代表xy距离，当ab为TL关系时，同时移除ab</p><p>OSS单侧选择：仅从多数样本中移除TL关系，即若a为多数样本，只移除a而不移除b</p><p>Condensed Nearest Neighbor rule (CNN)  ：尽可能的减少训练数据</p><h5 id="混合采样：上下采样结合"><a href="#混合采样：上下采样结合" class="headerlink" title="混合采样：上下采样结合"></a>混合采样：上下采样结合</h5><p>SMOTE+TL</p><p>SMOTE+CNN</p><p>SMOTE+ENN</p><h4 id="基于级联组合的样本移除算法"><a href="#基于级联组合的样本移除算法" class="headerlink" title="基于级联组合的样本移除算法"></a>基于级联组合的样本移除算法</h4><p>BalanceCascade算法，基于Bootstrapping思想，通过有监督学习的方式训练出一组分类器来去除大众类中需要经过下采样而去除的样本</p><h4 id="在线难样本挖掘算法"><a href="#在线难样本挖掘算法" class="headerlink" title="在线难样本挖掘算法"></a>在线难样本挖掘算法</h4><p>Online hard example mining（OHEM）算法：一种利用Bootstrapping技术，基于卷积神经网络的目标检测方法</p><h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><p>Random sampling by using lots is an old idea, mentioned several times in the Bible. In 1786 Pierre Simon <a href="https://en.wikipedia.org/wiki/Laplace">Laplace</a> estimated the population of France by using a sample, along with <a href="https://en.wikipedia.org/wiki/Ratio_estimator">ratio estimator</a>. He also computed probabilistic estimates of the error. These were not expressed as modern <a href="https://en.wikipedia.org/wiki/Confidence_interval">confidence intervals</a> but as the sample size that would be needed to achieve a particular upper bound on the sampling error with probability 1000&#x2F;1001. His estimates used <a href="https://en.wikipedia.org/wiki/Bayes'_theorem">Bayes’ theorem</a> with a uniform <a href="https://en.wikipedia.org/wiki/Prior_probability">prior probability</a> and assumed that his sample was random. <a href="https://en.wikipedia.org/wiki/Alexander_Ivanovich_Chuprov">Alexander Ivanovich Chuprov</a> introduced sample surveys to <a href="https://en.wikipedia.org/wiki/Imperial_Russia">Imperial Russia</a> in the 1870s.[<em><a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed">citation needed</a></em>]</p><p>In the US the 1936 <em><a href="https://en.wikipedia.org/wiki/Literary_Digest">Literary Digest</a></em> prediction of a Republican win in the <a href="https://en.wikipedia.org/wiki/U.S._presidential_election,_1936">presidential election</a> went badly awry, due to severe <a href="https://en.wikipedia.org/wiki/Bias">bias</a> [<a href="https://www.wsj.com/articles/SB115974322285279370">1]</a>. More than two million people responded to the study with their names obtained through magazine subscription lists and telephone directories. It was not appreciated that these lists were heavily biased towards Republicans and the resulting sample, though very large, was deeply flawed.[<a href="https://en.wikipedia.org/wiki/Sampling_(statistics)#cite_note-21">21]</a>[<a href="https://en.wikipedia.org/wiki/Sampling_(statistics)#cite_note-22">22]</a></p><h3 id="抽样方法的早期发展（来自调查方法P5）"><a href="#抽样方法的早期发展（来自调查方法P5）" class="headerlink" title="抽样方法的早期发展（来自调查方法P5）"></a>抽样方法的早期发展（来自调查方法P5）</h3><p>早期的研究者，像Booth，都是试图去搜集研究的总体的所有成员的信息。这种方法避免了因从总体中选择部分进行调查所产生的误差，当然，对于大规模总体而言，也不适用。的确，分析普查资料的困难使得人们开始努力将样本资料推挤总体。早期的抽样也许是研究一个典型的“镇”，或者有意识地搜集个体资料，使其与总体相似，如访问一半的男性、一半的女性，并使其再地理分布上与总体相似。</p><p>尽管18世纪已经有了概率论，知道20世纪概率论才被应用到抽样调查实践中。最早的应用就是从总体中系统地获得“N中的1”.这就是概率样本（probability sample），即每个个体都有被选作样本的非零机会。</p><p>抽样应用的最大突破来自于农业研究。为了预测作物产量，统计学家们创造了面积概率抽样法（area probability sample），也就是抽取一定面积作为样本来预测农名春季的耕作到秋季会有怎样的收获。人们把同样的方法用在了家户调查中。在城市或乡村，抽选一定的区域，列出全部的家户，再从家户列表中抽选样本。抽样时，需要找到一种方法让每个家户甚至家户中的人都有被抽中的机会。这种技术的魅力在于，抽样时无须将总体中的所有家户都列出来。</p><p>大萧条和第二次世界大战时调查研究的助推剂。现代概率抽样之一就是1939年12月开始的“月度失业调查”（Monthly Survey of Unemployment），29岁的统计学家Morris Hansen 主持了这项调查，后来他也成为了这个领域的领军人物（Hansen, Hurwitz, and Madow, 1953）。战争期间，联邦政府希望通过调查来了解人们的态度和观点，如购买战争国债的兴趣，以及其他的事实。战争期间，不少资源都用于了调查。再战争期间从事调查的人们，后来对调查方法的发展都起到了重要的作用。战争结束后，方法专家认识到，要想获得基于总体的好的统计，需要关注调查方法的三个方面：问题时如何设计的；数据是怎么搜集的，包括对访员的培训；样本是如何抽取的。</p><p>评价其他样本的重要基础是概率样本。概率样本广泛应用于政府统计机构为政策制定者提供的重要证据中，概率样本也应用再司法诉讼中。对媒体受众的规模估计也运用概率样本，并由此确定广告率。简而言之，当样本负载大量价值时，通常使用概率样本。</p><p>my words：</p><p>进入互联网时代后，调查方法不再局限于访谈，爬虫也成了一种调查方法。同时，抽样的手段也从抽取一部分人进行访谈，获得抽样数据，转换到了利用爬虫或记录等方法获取大数据，再使用概率抽样的方式对总体数据进行抽取，加速训练。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li>SMOTE: Synthetic Minority Over-sampling Technique（2002）<a href="https://arxiv.org/pdf/1106.1813.pdf">https://arxiv.org/pdf/1106.1813.pdf</a></li><li>分布式数据分层抽样技术及其在目标检测领域的应用研究_黎敏讷（2017）<a href="http://gb.oversea.cnki.net/KCMS/detail/detail.aspx?filename=1018997231.nh&dbcode=CMFD&dbname=CMFD2019">http://gb.oversea.cnki.net/KCMS/detail/detail.aspx?filename=1018997231.nh&amp;dbcode=CMFD&amp;dbname=CMFD2019</a></li><li>胡健颖，孙山泽. 抽样调查的理论、方法和应用. 北京大学出版社:北京, 2000.6. <a href="https://zh.wikipedia.org/wiki/Special:%E7%BD%91%E7%BB%9C%E4%B9%A6%E6%BA%90/7301045476">ISBN 7-301-04547-6</a>.</li><li>金勇进，蒋妍，李序颖. 抽样技术. 中国人民大学出版社:北京, 2002.6. <a href="https://zh.wikipedia.org/wiki/Special:%E7%BD%91%E7%BB%9C%E4%B9%A6%E6%BA%90/7300040799">ISBN 7-300-04079-9</a></li><li>Chambers, R L, and Skinner, C J (editors) (2003), <em>Analysis of Survey Data</em>, Wiley, <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)">ISBN</a> <a href="https://en.wikipedia.org/wiki/Special:BookSources/0-471-89987-9">0-471-89987-9</a></li><li>大数据挖掘的均匀抽样设计及数值分析_李毅（2015）<a href="http://gb.oversea.cnki.net/KCMS/detail/detail.aspx?filename=TJLT201504001&dbcode=CJFD&dbname=CJFD2015">http://gb.oversea.cnki.net/KCMS/detail/detail.aspx?filename=TJLT201504001&amp;dbcode=CJFD&amp;dbname=CJFD2015</a></li><li>Data Sampling Methods to Deal With the Big Data Multi-Class Imbalance Problem <a href="https://www.mdpi.com/2076-3417/10/4/1276/pdf">https://www.mdpi.com/2076-3417/10/4/1276/pdf</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>读书笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据抽样</tag>
      
      <tag>大数据</tag>
      
      <tag>spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>🚀HTTP绕过原理总结</title>
    <link href="/2020/01/12/2020-01-13-AF_HTTP_Evasion/"/>
    <url>/2020/01/12/2020-01-13-AF_HTTP_Evasion/</url>
    
    <content type="html"><![CDATA[<p>这篇博客整理自两个月以来在深信服实习的所学</p><p>HTTP攻击分为响应端和请求端</p><p><strong>响应端攻击</strong>指服务器对客户端进行攻击，一般的攻击形式为：在HTML页面中插入恶意的js代码等，常见的有shellcode，HTML注入攻击，CSS注入攻击等，其主要利用了浏览器的漏洞，当用户浏览了带有恶意代码的网页，攻击就有可能生效；</p><p><strong>请求端攻击</strong>指客户端对服务器发送恶意代码，针对服务器的漏洞进行攻击，如在HTTP头部字段注入shellcode或利用vary字段填充打爆服务器缓存等</p><p>这篇博客分享的，是如何利用防火墙和浏览器&#x2F;服务器之间HTTP协议解析的不同，实现让防火墙无法检出恶意代码，实现直接攻击的方法。</p><h1 id="一、HTTP响应端绕过原理"><a href="#一、HTTP响应端绕过原理" class="headerlink" title="一、HTTP响应端绕过原理"></a>一、HTTP响应端绕过原理</h1><h2 id="1-1-HTTP响应端绕过原理"><a href="#1-1-HTTP响应端绕过原理" class="headerlink" title="1.1 HTTP响应端绕过原理"></a>1.1 HTTP响应端绕过原理</h2><p><img src="/../img/2020-01-13-AF_HTTP_Evasion/response_eva_principle.jpg"></p><p>介绍响应端绕过原理之前，先要了解关于浏览器的两个重要特点：</p><h3 id="1-1-1浏览器的健壮性"><a href="#1-1-1浏览器的健壮性" class="headerlink" title="1.1.1浏览器的健壮性"></a>1.1.1浏览器的健壮性</h3><p>程序的健壮性，旨在处理实际情况下可能出现的格式错误的数据，使程序不容易崩溃或仍能正确运行，浏览器的健壮性往往体现在解析某些存在错误的网页时，仍能解析出正常的结果，如HTML标签未闭合等小错误，浏览器均可以自行修复。</p><h3 id="1-1-2浏览器的懒惰性"><a href="#1-1-2浏览器的懒惰性" class="headerlink" title="1.1.2浏览器的懒惰性"></a>1.1.2浏览器的懒惰性</h3><p>浏览器的懒惰性不同于健壮性，浏览器的懒惰性在于遇到实践中从未期望过的，无意义的数据时，仍有可能解析成功。从浏览器的角度来看，这并没有害处，因为它仍然以有效的方式处理了数据。但是从审查者，也就是AF的角度来说，很多情况下都会对数据进行不同的解释，这种解释上的差异就会导致绕过。<strong>这一点是很多人所不能理解的，外行常常将健壮性和懒惰性混为一谈。</strong></p><h3 id="1-1-3差异→绕过"><a href="#1-1-3差异→绕过" class="headerlink" title="1.1.3差异→绕过"></a>1.1.3差异→绕过</h3><p>针对以上两点，从AF的角度上来看，均是由于AF与浏览器之间对数据处理的差异而产生的对数据理解的不同，由此，<strong>攻击者可以利用一些手段“绕过”AF的防御，而这些手段对漏洞在浏览器端的生效并无影响。</strong>我们把上述攻击称为“绕过”。</p><p>同时，不同的浏览器处理数据的方式不同，导致的绕过手段也不尽相同。根据我的研究，浏览器可以分为3大类，一类是针对chrome内核，chrome浏览器和大部分国产浏览器都通用的绕过方法，一类是针对IE内核，仅对某些IE版本生效的绕过方法，还有一类是以Firefox为代表的Gecko内核的绕过方法。在本文中，将使用chrome、ie、firefox表示某些绕过手段在特定浏览器生效，同时尽可能的标出版本号。</p><p>看到这里，对HTTP绕过有基本认识的你已经可以试着阅读<strong>参考资料</strong>中的内容了，里面提及了多种绕过手段，本文中将不再赘述。除此之外，对于市面上的AF而言，对字符集的支持可能不是很完善，常常可以通过直接换一种编码的形式形成绕过，一部分原因是其使用的检测引擎过于老旧，导致其过于依赖预处理的解码。</p><h1 id="二、HTTP请求端绕过原理"><a href="#二、HTTP请求端绕过原理" class="headerlink" title="二、HTTP请求端绕过原理"></a>二、HTTP请求端绕过原理</h1><p>目前大家很少关注HTTP请求端的绕过，原因很简单，因为即使绕过了防火墙，攻击的服务器很有可能已经修复了所利用的漏洞，导致耗费人力精力构造的绕过没有效果。其次，很多针对服务器的攻击方法需要发送多次请求，其构造的报文十分精巧，很难有利用协议绕过的空间。</p><p>HTTP请求端的绕过，其涉及影响AF对Body解析的部分，基本与响应端类似。除此之外，HTTP很多请求端攻击是在头部字段或URI上做文章，对此，可以将其中的攻击字段做BASE64编码或escape编码，常常也能形成绕过，这同样是利用了AF针对编码处理的不足。</p><p>建议读者一定读一读HTTP Evader，从浏览器&#x2F;服务器和AF的差异入手，定能找到其中的绕过方法。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>本文部分取自以下网站，并不代表原作者观点，有兴趣还请自行阅读</p><p>失误之处敬请谅解！</p><p><a href="https://noxxi.de/research.html">HTTP Evader</a>：<a href="https://noxxi.de/research.html">https://noxxi.de/research.html</a></p><p>这里主要介绍了HTTP响应端常见的绕过手段，附带解释，非常完善！</p><p><a href="https://www.freebuf.com/news/193659.html">技术讨论 在HTTP协议层面绕过WAF</a>：<a href="https://www.freebuf.com/news/193659.html">https://www.freebuf.com/news/193659.html</a></p><p>这里主要介绍了对chunk变形的绕过手段，可以绕过市面上很多免费的waf</p><h1 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h1><p>实际上防火墙就是一道墙，如果攻击足够猛，肯定是可以翻越过这道墙的，但在此之前，黑客需要考虑是否耗费如此大的精力物力来翻墙。毕竟，有那么多可爱的肉鸡。</p>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>HTTP协议</tag>
      
      <tag>网络安全</tag>
      
      <tag>防火墙绕过</tag>
      
      <tag>深信服</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
