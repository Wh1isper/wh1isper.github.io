---
layout: post # 使用的布局（不需要改）
title: 大数定律下的llm应用问题 # 标题
subtitle: 大数定律下的llm应用问题 #副标题
date: 2025-02-11 # 时间
author: Wh1isper # 作者
banner_img: /img/post-bg-unix-linux.jpg
catalog: true # 是否归档
tags: #标签
  - 技术分享
  - LLM
  - 大数定律
category:
  # - 随笔
  #   - 时评
  - 读书笔记
  - 技术分享
---

随着Deepseek R1的推出，高质量的Agent即将取代偏重工程化的Workflow，更多的工具提供给AI的情况下，细微的bad case在大数定律下将会变得更加明显。

### 推理模型不代表幻觉的消失

一个理论基础是：推理模型仍然是Next token prediction，而不是真的通过思考来规划路径。COT并不能避免其中的幻觉所在，而如何识别幻觉，仍然依赖工程化手段

### 大数定律下bad case无法避免

在大数定律下llm模型的bad case无法避免，一方面我们可以设计一套所谓的结构来尽可能地限制模型的输出，但这会导致模型性能的下降和局限，同时，这方面的工作带来的收益可能因为模型能力的进步而被抹平。另一方面，我们可以积极地设立一套监察机制，尽可能地识别、处理bad case，其中可以通过归一化的手段消化bad case、也可以通过更好的prompt设计、微调等方式尝试端到端的解决bad case

### 大参数MoE成为Agent的必要基础

从Deepseek和kimi的技术报告可以看出，让模型在推理侧使用更多的算力，将显著提升模型的性能。另外，研究人员也发现，当给LLM更少限制的时候，reasoning的威力更强。有趋势表明，在足够的推理token的情况下，不需要人工限制就可以让模型reasoning出合适的结果。而对于强人工限制的Agent，增加推理算力并不能显著提升其性能。这可能是LLM应用在这一阶段的“苦涩的教训”。

### 通过reasoning构建LLM原生应用

只是一个想法，根据上面的原理，我们可以推断出，通用Agent应当是下一阶段的重点发展方向，对于Agent的MoE应当基于reasoning模型实现
