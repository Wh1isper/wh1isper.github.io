---
layout:     post                    # ä½¿ç”¨çš„å¸ƒå±€ï¼ˆä¸éœ€è¦æ”¹ï¼‰
title:      ğŸ”§Sparkéƒ¨ç½²æ•™ç¨‹	# æ ‡é¢˜
subtitle:   Deploy Spark in a simple way	 #å‰¯æ ‡é¢˜
date:       2021-04-11              # æ—¶é—´
author:     Wh1isper                      # ä½œè€…
catalog: true                       # æ˜¯å¦å½’æ¡£
banner_img: /img/post-bg-unix-linux.jpg    #æŠ€æœ¯åˆ†äº«-ç¼–ç¨‹
category:
    # - éšç¬”
    # - æ—¶è¯„
    # - è¯»ä¹¦ç¬”è®°
    - æŠ€æœ¯åˆ†äº«
tags:                               #æ ‡ç­¾
    - å¤§æ•°æ®
    - spark
---

# Update 2023å¹´9æœˆ18æ—¥

Sparkçš„æœ€æ–°éƒ¨ç½²è„šæœ¬å’Œæ•™ç¨‹æˆ‘æ”¾åœ¨äº†è¿™ä¸ªä»“åº“ä¸‹ï¼šhttps://github.com/Wh1isper/spark-build

åŒæ—¶æœ‰æ‰“å¥½çš„dockeré•œåƒå¯ä»¥ä½¿ç”¨ï¼š

- For PySpark app: [wh1isper/pyspark-app-base](https://hub.docker.com/r/wh1isper/pyspark-app-base)
- For Spark Connect Server: [wh1isper/spark-executor](https://hub.docker.com/r/wh1isper/spark-executor)
- For Spark on k8s(Spark executor): [wh1isper/spark-connector-server](https://hub.docker.com/r/wh1isper/spark-connector-server)

æœ€æ–°çš„æ•™ç¨‹ä¸­ï¼ŒJDK11å’Œ17éƒ½æ˜¯ç»è¿‡æµ‹è¯•çš„

--------------

# ä¸‹è½½Spark

é€‰æ‹©ä¸€ä¸ªsparkç‰ˆæœ¬ä¸‹è½½ï¼šhttps://spark.apache.org/downloads.html

ç„¶åä¸‹è½½å¹¶å®‰è£…å¯¹åº”çš„scalaç‰ˆæœ¬

Note that, Spark 2.x is pre-built with Scala 2.11 except version 2.4.2, which is pre-built with Scala 2.12. Spark 3.0+ is pre-built with Scala 2.12.

è¿™é‡Œä½¿ç”¨Spark-3.0.1è¿›è¡Œå¼€å‘

# é…ç½®spark

é¦–å…ˆè§£å‹sparkï¼Œç„¶åè®¾ç½®SPARK_HOMEï¼Œä»¥ä¸‹å†…å®¹ä»…è·Ÿç”¨æˆ·é€‰æ‹©æŠŠsparkæ”¾åœ¨å“ªé‡Œæœ‰å…³ï¼Œå®é™…ä¸Šï¼Œä½ å¯ä»¥æ”¾åœ¨ä»»ä½•åœ°æ–¹

```bash
tar -zxvf spark-3.0.1-bin-hadoop3.2.tgz -C /opt
mv /opt/spark-3.0.1-bin-hadoop3.2/ /opt/spark-3.0.1
# åœ¨~/.bashrcæ·»åŠ ï¼Œæˆ–å­˜åœ¨å¦å¤–ä¸€ä¸ªæ–‡ä»¶é‡Œã€éœ€è¦çš„æ—¶å€™source
export SPARK_HOME=/opt/spark-3.0.1
export PATH=${SPARK_HOME}/bin:${SPARK_HOME}/sbin:$PATH
```

## é…ç½®hadoop yarn

è®©sparkä½¿ç”¨hadoopèµ„æºï¼Œåªéœ€è¦é…ç½®conf/spark-env.shä¸­çš„HADOOP_CONF_DIRä¸º$HADOOP_HOME/etc/hadoopæ–‡ä»¶å¤¹

å¦‚æœä½¿ç”¨æœ¬ä»“åº“æä¾›çš„å•æœºç‰ˆhadoopæ•™ç¨‹ï¼Œåˆ™HADOOP_CONF_DIR=/home/hadoop/hadoop/etc/hadoop/

```bash
cd /opt/spark-3.0.1
cp conf/spark-env.sh.template conf/spark-env.sh
echo "export HADOOP_CONF_DIR=/home/hadoop/hadoop/etc/hadoop/" >> conf/spark-env.sh
```

## å¯åŠ¨

localå¯åŠ¨ï¼š

```bash
spark-shell --master local
```

yarnå¯åŠ¨ï¼š

```bash
spark-shell --master yarn --deploy-mode client
```

# pysaprkæ­å»º

åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
conda create -n {your_env_name} python=3.7
# centos
conda activate {your_env_name}
# ubuntu
source activate {your_env_name}
```

å®‰è£…pysparkï¼Œæ³¨æ„ç‰ˆæœ¬å¯¹åº”

```bash
conda install pyspark==3.0.0
```

# æœ¬åœ°æ­å»ºæ—¶éœ€è¦hadoopåŒ…

å¦‚æœè¦ä½¿ç”¨hadoop s3 clientä¹‹ç±»çš„ï¼Œæœ¬åœ°éƒ¨ç½²çš„æƒ…å†µä¸‹éœ€è¦æ‰‹åŠ¨æŠŠjaræ‹·è¿‡å»ï¼Œè¦ä¸‹è½½ä¸spark pre-buildç›¸åŒç‰ˆæœ¬çš„hadoop

å¦‚æœä½ éœ€è¦çŸ¥é“æ˜¯ä»€ä¹ˆç‰ˆæœ¬ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹ï¼Œhadoop-...-3.2.0.jarä»£è¡¨ç€pre-buildä¸ºhadoop-3.2.0

```bash
ls ${SPARK_HOME}/jars/ | grep hadoop
```



ä»¥ä¸‹è¿™ä¸ªå‘½ä»¤æ‹·è´äº†æ‰€æœ‰çš„ï¼Œå®é™…ä¸Šä½ å¯ä»¥é€‰æ‹©ä½ éœ€è¦çš„

å¦‚s3 clientåªéœ€è¦awså’Œs3ç›¸å…³çš„å‡ ä¸ªåŒ…

```bash
cp ${HADOOP_HOME}/share/hadoop/tools/lib/* ${SPARK_HOME}/jars/
```

# Troubleshooting

## NoSuchMethod

æ³¨æ„pysparkç‰ˆæœ¬ï¼Œspark 3.0.1å¯ä»¥å°è¯•pyspark 3.0.0æˆ–pyspark 3.0.1

## NoSuch...ï¼ˆæ²¡æœ‰è¿™ä¸ªç±»ã€å¯¹è±¡ç­‰ï¼‰

æ³¨æ„jdkç‰ˆæœ¬ï¼Œå»ºè®®æ›´æ–°åˆ°æœ€æ–°java8u25x

æ³¨æ„scalaç‰ˆæœ¬ï¼Œåœ¨ä¿è¯sparkç‰ˆæœ¬å’Œscalaç‰ˆæœ¬å¯¹åº”çš„å‰æä¸‹ï¼Œä½¿ç”¨æœ€æ–°2.12.1xæˆ–2.11.1x
