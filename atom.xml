<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Wh1isper&#39;s Blog</title>
  
  <subtitle>Wh1isper&#39;s Blog</subtitle>
  <link href="https://blog.wh1isper.top/atom.xml" rel="self"/>
  
  <link href="https://blog.wh1isper.top/"/>
  <updated>2026-02-18T04:43:56.821Z</updated>
  <id>https://blog.wh1isper.top/</id>
  
  <author>
    <name>Wh1isper</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>构建 Effective Cloud Agent</title>
    <link href="https://blog.wh1isper.top/2026/02/18/2026-02-18-cloud-agent-implementation-details/"/>
    <id>https://blog.wh1isper.top/2026/02/18/2026-02-18-cloud-agent-implementation-details/</id>
    <published>2026-02-18T02:00:00.000Z</published>
    <updated>2026-02-18T04:43:56.821Z</updated>
    
    <content type="html"><![CDATA[<h1 id="从SSE服务说起"><a href="#从SSE服务说起" class="headerlink" title="从SSE服务说起"></a>从SSE服务说起</h1><p>先从一个可运行的 Chatbot 服务开始：前端通过 <code>POST /chat</code> 发起请求，通过 <code>GET /stream/:conversation_id</code> 接收 <code>SSE</code> 事件流。<br>这里把职责拆成两层：</p><ul><li>Display Layer：向 UI 输出事件，服务展示与回放。</li><li>Business Layer：维护消息历史，服务 prompt 构造与推理。</li></ul><pre><code class=" mermaid">flowchart LR    UI[Web UI]    API[Chat Service]    LLM[LLM API]    DL[(Display Events)]    BL[(Business Messages)]    UI --&gt;|chat request| API    API --&gt;|sse stream| UI    API --&gt; BL    API --&gt; LLM    LLM --&gt; API    API --&gt; DL</code></pre><p>对应的最小存储模型：</p><ul><li><code>conversation(id, created_at)</code></li><li><code>message(id, conversation_id, role, content, created_at)</code></li></ul><p>单轮写路径：</p><ol><li>接收用户输入并落库 <code>role=user</code>。</li><li>读取最近 <code>N</code> 条消息构造 prompt。</li><li>调用 LLM，按 token 生成 <code>SSE</code> 事件并写入 Display Layer。</li><li>推理结束后落库 <code>role=assistant</code>。</li></ol><pre><code class=" mermaid">sequenceDiagram    participant U as User    participant API as Chat Service    participant DB as Message Store    participant LLM as LLM API    U-&gt;&gt;API: POST /chat    API-&gt;&gt;DB: insert user message    API-&gt;&gt;DB: load recent messages    API-&gt;&gt;LLM: inference    LLM--&gt;&gt;API: token stream    API--&gt;&gt;U: SSE events    API-&gt;&gt;DB: insert assistant message</code></pre><p>约束保持简单：单进程、单副本、不处理并发写冲突。</p><p>下一段再进入“引入文件系统后”的架构分叉：<code>stateless agent service + sandbox</code> vs <code>agent in vm</code>。</p><h1 id="扩展文件系统"><a href="#扩展文件系统" class="headerlink" title="扩展文件系统"></a>扩展文件系统</h1><h2 id="Agent-in-VM"><a href="#Agent-in-VM" class="headerlink" title="Agent in VM"></a>Agent in VM</h2><blockquote><p>这个模式下，Agent没有多租户的概念，面对的是一台专用的虚拟机，所有多租户的概念在外部业务层实现</p></blockquote><p>当 Agent 需要稳定操作本地开发环境、浏览器和长生命周期进程时，<code>Agent in VM</code> 是最直接的方案。<br>它的核心思想是：平台做“VM 分配与调度”，Agent 只面对“单租户执行环境”。</p><pre><code class=" mermaid">flowchart LR    U[User]    APP[Multi-tenant App]    SCH[VM Scheduler]    VM1[Tenant VM]    AG[Agent Process]    FS[(Local FS)]    SH[Shell/Browser]    LLM[LLM API]    U --&gt; APP    APP --&gt; SCH    SCH --&gt; VM1    VM1 --&gt; AG    AG --&gt; FS    AG --&gt; SH    AG --&gt; LLM</code></pre><p>设计重点：</p><ol><li>租户隔离天然清晰：每个会话绑定一台 VM。</li><li>状态恢复简单：文件、进程、缓存都在 VM 内。</li><li>代价是资源成本和调度时延：空闲 VM 成本高，启动&#x2F;恢复慢于无状态服务。</li></ol><p>案例：Manus、利用Claude Code SDK构建多租户agent</p><h2 id="Stateless-Agent-Service-Sandbox"><a href="#Stateless-Agent-Service-Sandbox" class="headerlink" title="Stateless Agent Service + Sandbox"></a>Stateless Agent Service + Sandbox</h2><p>该方案将控制逻辑与执行环境解耦：Agent Service 保持无状态，Sandbox 承载会话执行状态。</p><p>分层如下：</p><ul><li>Agent Service：无状态，负责规划、路由、工具编排。</li><li>Sandbox：有状态，负责执行命令和访问会话文件。</li><li>Shared Storage：跨 Sandbox 挂载同一会话目录。</li></ul><pre><code class=" mermaid">flowchart LR    U[User]    GW[API Gateway]    AS[Stateless Agent Service]    LLM[LLM API]    SB[Session Sandbox Pod/Lambda]    NFS[(Shared Storage)]    TOOLS[External Tools]    U --&gt; GW --&gt; AS    AS --&gt; LLM    AS --&gt; SB    SB --&gt; NFS    SB --&gt; TOOLS    AS --&gt; TOOLS</code></pre><p>可行性前提：Agentic loop 依赖“状态可引用”，不依赖“进程常驻”。只要 session 可以绑定到可恢复的文件和执行上下文，服务就可以无状态扩缩容。</p><p>关键约束：</p><ol><li>边界必须明确：哪些工具在 Agent Service 执行，哪些必须进 Sandbox。</li><li>安全默认收敛：出网控制、域名&#x2F;IP 白名单、速率限制、租户目录隔离。</li><li>存储性能决定体验：高频读写场景要评估本地盘缓存或更高性能分布式存储。</li></ol><h1 id="Durable-Execution（Stateless-Sandbox-的可靠性拓展）"><a href="#Durable-Execution（Stateless-Sandbox-的可靠性拓展）" class="headerlink" title="Durable Execution（Stateless + Sandbox 的可靠性拓展）"></a>Durable Execution（Stateless + Sandbox 的可靠性拓展）</h1><p>在 <code>Stateless Agent Service + Sandbox</code> 中，实例漂移是常态。重试不能依赖进程内存。<br>Durable Execution 的目标是将一次 loop 分解为可恢复步骤，并在步骤边界写入 checkpoint。</p><p>引入原因：</p><ol><li>实例漂移是常态：Pod 重建、请求重路由都会打断内存态执行。</li><li>工具调用有副作用：整轮重放会产生重复写入或重复外部调用。</li><li>长任务跨多分钟：必须允许中断后继续，而不是从头开始。</li></ol><p>最小状态拆分建议：</p><ol><li><code>message_state</code>：会话消息、摘要、token budget。</li><li><code>filesystem_state_ref</code>：工作目录版本、挂载路径、快照 ID。</li><li><code>execution_state</code>：当前 step、计划版本、上一步输出。</li><li><code>effect_log</code>：已执行副作用记录（外部请求 ID、写操作哈希）。</li></ol><pre><code class=" mermaid">flowchart LR    STEP[Run Step]    CKPT[Write Checkpoint]    EFFECT[Record Side Effect]    FAIL[Failure]    RESUME[Load Last Checkpoint]    NEXT[Run Next Step]    STEP --&gt; CKPT    STEP --&gt; EFFECT    STEP --&gt; FAIL    FAIL --&gt; RESUME    RESUME --&gt; NEXT</code></pre><p>实现规则：</p><ol><li>每个 step 必须可重入：相同输入重复执行，结果语义不变。</li><li>每个副作用必须可去重：通过 <code>idempotency_key</code> 或外部事务键避免重复提交。</li></ol><h2 id="语义恢复"><a href="#语义恢复" class="headerlink" title="语义恢复"></a>语义恢复</h2><p>在工程实践中，很多工具调用不可回滚，或回滚成本远高于收益。<br>因此，失败处理可以优先采用“语义恢复”，而不是“强事务回滚”。</p><p>例如三个工具并行执行时其中一个报错，不必强制撤销其余两个工具的副作用。可采用以下策略：</p><ol><li>将消息历史恢复到这组并行工具调用之前。</li><li>保留本次工具调用返回（包含不确定状态）。</li><li>对失败或不确定结果注入系统说明：<code>上次调用出错，可能已部分执行成功，请先确认状态再继续</code>。</li><li>让模型基于这条说明继续规划下一步（先确认，再补偿，或跳过）。</li></ol><blockquote><p>这里还可以向外暴露重试事件，以确保显示层不会出现未定义行为</p></blockquote><p>该策略的取舍很明确：放弃严格事务语义，换取更低实现复杂度和更高恢复连续性。</p><h1 id="Loop-级隔离编排（优化Agent-Service的可扩缩容性）"><a href="#Loop-级隔离编排（优化Agent-Service的可扩缩容性）" class="headerlink" title="Loop 级隔离编排（优化Agent Service的可扩缩容性）"></a>Loop 级隔离编排（优化Agent Service的可扩缩容性）</h1><p>这是一种中间态方案：保持 loop 代码结构不变，只隔离 loop 实例。<br>实现方式是“每个 turn 启动一个 Worker&#x2F;Lambda”，由队列驱动执行；SSE 由事件通道异步转发到流网关。</p><p>适用原因：</p><ol><li>保持调试模型稳定：单个 loop 代码路径不变，问题定位更直接。</li><li>缩小爆炸半径：单个任务异常只影响当前 worker。</li><li>提升发布速度：无状态 worker 镜像可快速滚动升级。</li></ol><pre><code class=" mermaid">flowchart LR    U[User]    API[Agent API]    Q[(Task Queue)]    W[Loop Worker Lambda/Pod]    LLM[LLM API]    SB[Sandbox]    EQ[(Event Queue)]    SG[SSE Gateway]    U --&gt; API    API --&gt; Q    Q --&gt; W    W --&gt; LLM    W --&gt; SB    W --&gt; EQ    EQ --&gt; SG    SG --&gt; U</code></pre><p>实现要点：</p><ol><li><code>turn_id</code> 作为任务主键，保证同一轮只有一个 active worker。</li><li>Worker 事件写入 <code>event queue</code>，由 <code>SSE gateway</code> 统一推送，避免函数实例直接持有长连接。</li><li>失败重试由队列控制，重试前先加载 checkpoint，继续执行而非整轮重跑。</li></ol><p>在演进路径中的位置：</p><ol><li>它依赖 Durable Execution 提供可恢复状态。</li><li>它不要求 Actor 拆分，适合作为高并发阶段的低改造增量方案。</li><li>当单 loop 内部并发仍成瓶颈，再演进到 Actor Model。</li></ol><h1 id="Actor-Model（优化Agent-Service的可扩缩容性）"><a href="#Actor-Model（优化Agent-Service的可扩缩容性）" class="headerlink" title="Actor Model（优化Agent Service的可扩缩容性）"></a>Actor Model（优化Agent Service的可扩缩容性）</h1><p>当 <code>Stateless Agent Service</code> 进入高并发、多工具、长链路阶段，单体 loop 的主要瓶颈是调度耦合和故障扩散。<br>Actor Model 是下一阶段的实现方式：将 loop 拆成可独立扩缩容的执行单元。</p><p>推荐的最小 Actor 拆分：</p><ol><li><code>Session Actor</code>：会话入口，维护 turn 生命周期。</li><li><code>Planner Actor</code>：生成步骤计划与重规划。</li><li><code>Model Actor</code>：封装模型调用策略（路由、重试、限流）。</li><li><code>Tool Actor</code>：执行工具调用（可按工具类型分片）。</li><li><code>State Actor</code>：集中处理 checkpoint、effect log、恢复逻辑。</li></ol><pre><code class=" mermaid">flowchart LR    SA[Session Actor]    PA[Planner Actor]    MA[Model Actor]    TA[Tool Actor]    STA[State Actor]    BUS[(Message Bus)]    SA --&gt; BUS    PA --&gt; BUS    MA --&gt; BUS    TA --&gt; BUS    STA --&gt; BUS    BUS --&gt; SA    BUS --&gt; PA    BUS --&gt; MA    BUS --&gt; TA    BUS --&gt; STA</code></pre><p>与前一节的关系：</p><ol><li>Durable Execution 依赖 <code>State Actor</code> 统一写入恢复点。</li><li>Sandbox 调度可以下沉到 <code>Tool Actor</code>，实现按工具类型弹性扩缩容。</li><li>故障隔离更细：单个 Tool Actor 失败不会直接拖垮整个会话调度。</li></ol><p>代价是消息一致性、乱序处理和链路追踪复杂度上升。<br>建议顺序是：先稳定单体 Stateless loop，再按瓶颈引入 Actor 化拆分。</p><h1 id="可观测性"><a href="#可观测性" class="headerlink" title="可观测性"></a>可观测性</h1><p>可观测性目标不是“记录更多日志”，而是回答三个问题：慢在哪里、错在哪里、哪类任务不稳定。</p><pre><code class=" mermaid">flowchart LR    APP[Agent Service]    SB[Sandbox]    OBS[Telemetry Pipeline]    LOG[(Logs)]    MET[(Metrics)]    TR[(Traces)]    UI[Ops Dashboard]    APP --&gt; OBS    SB --&gt; OBS    OBS --&gt; LOG    OBS --&gt; MET    OBS --&gt; TR    LOG --&gt; UI    MET --&gt; UI    TR --&gt; UI</code></pre><p>建议指标分两层：</p><ol><li>LLM 层：QPS、TTFT、P95 延迟、错误率、token 成本。</li><li>Tool 层：调用次数、成功率、执行时长、超时率。</li></ol><p>Trace 结构建议绑定 <code>session_id + turn_id + step_id</code>，以便还原完整 trajectory。</p><h2 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h2><p>有了 Trace 和可恢复执行能力后，可以把线上真实任务沉淀为 benchmark 回放集。</p><pre><code class=" mermaid">flowchart LR    TR[(Trace Store)]    SEL[Scenario Selector]    REP[Replay Runner]    EVAL[Scoring/Eval]    RPT[Model Report]    TR --&gt; SEL --&gt; REP --&gt; EVAL --&gt; RPT</code></pre><p>评测重点不只看“是否成功”，还要看：</p><ol><li>端到端时延与成本。</li><li>工具调用效率和失败恢复能力。</li><li>不同模型在同一任务族上的稳定性差异。</li></ol>]]></content>
    
    
    <summary type="html">这篇先从最小 SSE Chatbot 开始，明确显示层与业务层的基本实现，再逐步过渡到文件系统与沙箱架构。</summary>
    
    
    
    <category term="AI Engineering" scheme="https://blog.wh1isper.top/categories/AI-Engineering/"/>
    
    
    <category term="Agent" scheme="https://blog.wh1isper.top/tags/Agent/"/>
    
    <category term="CloudAgent" scheme="https://blog.wh1isper.top/tags/CloudAgent/"/>
    
    <category term="Engineering" scheme="https://blog.wh1isper.top/tags/Engineering/"/>
    
    <category term="实现细节" scheme="https://blog.wh1isper.top/tags/%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82/"/>
    
  </entry>
  
  <entry>
    <title>聊聊云上Agent的架构设计</title>
    <link href="https://blog.wh1isper.top/2026/02/17/2026-02-17-architecture-of-cloud-agent/"/>
    <id>https://blog.wh1isper.top/2026/02/17/2026-02-17-architecture-of-cloud-agent/</id>
    <published>2026-02-17T13:54:00.000Z</published>
    <updated>2026-02-18T04:43:56.821Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>总结一下Agent的发展历程和架构演进，聊聊几个抽象概念，最后说说我觉得未来Agent架构的一个可能方向。</p></blockquote><h1 id="Chatbot"><a href="#Chatbot" class="headerlink" title="Chatbot"></a>Chatbot</h1><p>最开始出现的是Chatbot应用。Chatbot 是一个状态系统。每轮请求执行四个步骤：读取上下文、构造 prompt、调用模型、写回状态。系统设计的重点不在模型接口，而在状态组织方式。</p><p>会话状态建议分为两层：</p><ul><li><strong>显示层（Display Layer）</strong>：保存原始事件流，面向 UI 展示、回放与审计。</li><li><strong>业务层（Business Layer）</strong>：保存推理就绪上下文，面向 prompt 构造与成本控制。</li></ul><p>该分层支持一条工程约束：业务层允许静默压缩与摘要，显示层保持完整历史，不做语义改写。</p><h2 id="1-分层职责图"><a href="#1-分层职责图" class="headerlink" title="1) 分层职责图"></a>1) 分层职责图</h2><pre><code class=" mermaid">flowchart LR    UI[Display Layer&lt;br/&gt;raw events / replay / audit]    BL[Business Layer&lt;br/&gt;processed context / prompt-ready]    UI -. isolated goals .- BL</code></pre><p>这张图定义职责边界。两层的输入输出、服务对象、演化策略均不同。混用单层存储会产生三类问题：</p><ol><li>prompt 长度随轮次线性增长，推理成本和时延同步上升。</li><li>展示格式、系统指令、工具中间消息进入模型输入，降低上下文信噪比。</li><li>压缩策略直接作用于用户可见历史，破坏回放一致性与审计可读性。</li></ol><h2 id="2-写路径图"><a href="#2-写路径图" class="headerlink" title="2) 写路径图"></a>2) 写路径图</h2><pre><code class=" mermaid">flowchart LR    U[User] --&gt; API[Chat API] --&gt; SVC[Conversation Service]    SVC --&gt; CP[Context Processor]    CP --&gt; LLM[LLM Inference]    LLM --&gt; SVC    SVC --&gt; DDB[(Display History Store)]    SVC --&gt; BDB[(Business Context Store)]</code></pre><p>写路径采用双写：同一轮请求同时更新显示层与业务层。</p><ul><li><code>Display History Store</code> 记录 raw user&#x2F;assistant events。</li><li><code>Business Context Store</code> 记录 processed context。</li></ul><p><code>Context Processor</code> 负责从“展示消息”生成“推理上下文”，常见操作为：</p><ul><li>truncate（窗口裁剪）</li><li>compress（信息压缩）</li><li>summarize（阶段摘要）</li><li>metadata filtering（移除展示噪声字段）</li></ul><h2 id="3-推理读路径图"><a href="#3-推理读路径图" class="headerlink" title="3) 推理读路径图"></a>3) 推理读路径图</h2><pre><code class=" mermaid">flowchart LR    SVC[Conversation Service] --&gt;|load prompt context| BDB[(Business Context Store)]    BDB --&gt; SVC    SVC --&gt; LLM[LLM Inference]</code></pre><p>推理路径只读取业务层。该约束保证三点：</p><ul><li>输入长度可预测。</li><li>关键语义可延续。</li><li>prompt 结构可标准化。</li></ul><h2 id="4-显示回放图"><a href="#4-显示回放图" class="headerlink" title="4) 显示回放图"></a>4) 显示回放图</h2><pre><code class=" mermaid">flowchart LR    UI[Chat UI] --&gt; API[Chat API]    API --&gt; DDB[(Display History Store)]    DDB --&gt; API --&gt; UI</code></pre><p>展示路径只读取显示层。UI 获取的是原始事件轨迹，不依赖推理优化后的上下文快照。该路径为回放、排障、审计提供稳定数据源。</p><h2 id="5-Context-Engineering"><a href="#5-Context-Engineering" class="headerlink" title="5) Context Engineering"></a>5) Context Engineering</h2><p>在 Chatbot 阶段，Context Engineering 关注两个问题：</p><ol><li><strong>上下文压缩</strong>：在 token budget 固定的前提下，保留任务相关信息。</li><li><strong>用户记忆 &#x2F; 偏好</strong>：在跨轮次交互中维持行为一致性。</li></ol><p>这一阶段的重点是定义问题边界和评估标准，不在于引入复杂机制。</p><p><strong>结论</strong></p><ul><li>Chatbot 架构已经形成显示层与业务层的初步分层：前者负责事件回放与审计，后者负责推理上下文组织。</li><li>Chatbot 阶段已经出现上下文管理问题：上下文压缩与用户记忆&#x2F;偏好管理成为后续架构演进的核心输入。</li></ul><h1 id="Agent-in-VM"><a href="#Agent-in-VM" class="headerlink" title="Agent in VM"></a>Agent in VM</h1><p>第二种形态是 <strong>Agent 与用户共处同一台 VM</strong>。与 Chatbot 阶段相比，这一形态把“对话系统”扩展为“执行系统”：Agent 不再只组织上下文，还直接操作 Shell、文件系统、浏览器和桌面环境。</p><p>这个形态的核心变化是：</p><ul><li>执行环境从 API 工具调用，转为真实操作系统环境。</li><li>状态边界从会话消息，扩展为 VM 全状态（文件、进程、浏览器、环境变量）。</li><li>用户可通过 VNC&#x2F;SSH 与 Agent 并行操作同一环境。</li></ul><h2 id="1-形态定义与架构边界"><a href="#1-形态定义与架构边界" class="headerlink" title="1) 形态定义与架构边界"></a>1) 形态定义与架构边界</h2><pre><code class=" mermaid">flowchart LR    U[User] --&gt;|VNC/SSH| VM[User VM]    WEB[Web Chat] --&gt; AG[Agent Process in VM]    subgraph VM[User VM]        AG        SH[Shell]        FS[(Filesystem)]        BR[Browser]        DE[Desktop]    end    AG --&gt; SH    AG --&gt; FS    AG --&gt; BR    DE --&gt; SH    DE --&gt; BR</code></pre><p>该架构的边界很直接：Agent 和用户共享同一运行时。系统一致性高，系统隔离弱。</p><h2 id="2-运行时交互流"><a href="#2-运行时交互流" class="headerlink" title="2) 运行时交互流"></a>2) 运行时交互流</h2><pre><code class=" mermaid">sequenceDiagram    participant User as User(VNC/SSH)    participant Web as Web Chat    participant Agent as Agent(in VM)    participant LLM as LLM API    participant Shell as Shell    participant FS as Filesystem    participant Browser as Browser    User-&gt;&gt;Shell: direct command    Shell-&gt;&gt;FS: read/write    Web-&gt;&gt;Agent: task request    Agent-&gt;&gt;LLM: planning + tool call    LLM--&gt;&gt;Agent: action    Agent-&gt;&gt;Shell: execute command    Shell-&gt;&gt;FS: read/write    Agent-&gt;&gt;Browser: open/click/type    Agent--&gt;&gt;Web: result    User-&gt;&gt;FS: inspect changes immediately</code></pre><p>这一形态的优势在于“可观察 + 可干预”：用户可以实时看到 Agent 的执行结果，也可以随时接管。</p><h2 id="3-状态持久化：从会话状态到-VM-快照"><a href="#3-状态持久化：从会话状态到-VM-快照" class="headerlink" title="3) 状态持久化：从会话状态到 VM 快照"></a>3) 状态持久化：从会话状态到 VM 快照</h2><p>Chatbot 阶段主要持久化消息与上下文。Agent-in-VM 阶段需要持久化完整环境状态，通常依赖 VM Snapshot。</p><pre><code class=" mermaid">sequenceDiagram    participant P as Platform    participant VM as VM    participant S as Snapshot Storage    P-&gt;&gt;VM: pause    VM--&gt;&gt;P: frozen    P-&gt;&gt;S: save memory snapshot    P-&gt;&gt;S: save disk snapshot    P-&gt;&gt;VM: terminate    P-&gt;&gt;S: load snapshots    S--&gt;&gt;P: memory + disk    P-&gt;&gt;VM: restore    VM--&gt;&gt;P: resumed</code></pre><p>快照带来的工程收益是“连续执行上下文”：文件、进程、浏览器状态可以跨会话恢复。典型场景是开发服务器和调试现场的延续。</p><h2 id="4-主要约束与风险"><a href="#4-主要约束与风险" class="headerlink" title="4) 主要约束与风险"></a>4) 主要约束与风险</h2><p>这个形态在通用性上很强，但成本与安全边界会前置成为架构问题：</p><ul><li><strong>成本约束</strong>：每用户独占 VM，空闲成本高，扩展效率低于共享计算架构。</li><li><strong>调度约束</strong>：VM 启停与快照恢复时延高于无状态容器调度。</li><li><strong>安全约束</strong>：Agent 进程与用户共域，密钥管理、权限边界、篡改防护更复杂。</li><li><strong>持久化风险</strong>：恶意进程或敏感凭证可能通过快照跨会话保留。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ul><li>Agent in VM 形态把系统能力从“对话组织”推进到“环境执行”，获得了强交互、强观察、强持久化能力。</li><li>这一形态同时引入了新的主问题：多租户安全边界、VM 成本模型与快照生命周期治理。</li></ul><h1 id="Agentic-LLM-API"><a href="#Agentic-LLM-API" class="headerlink" title="Agentic LLM API"></a>Agentic LLM API</h1><p>第三种形态可以定义为 <strong>Agentic LLM API</strong>：交互入口仍是 Chatbot，但执行能力已经升级为 Agent，并由模型 API 厂商托管执行环境。与 Agent in VM 的区别在于，用户不直接进入执行环境；与第一阶段 Chatbot 的区别在于，系统不仅维护消息状态，还维护可执行环境状态。</p><p>这一形态的目标是把“对话编排”与“环境执行”做成一条端到端链路：</p><ul><li>对用户暴露稳定的聊天接口。</li><li>对系统内部暴露可控的工具与执行环境。</li><li>在 token 成本、延迟与可观察性之间取得工程平衡。</li></ul><h2 id="1-形态定义与系统边界"><a href="#1-形态定义与系统边界" class="headerlink" title="1) 形态定义与系统边界"></a>1) 形态定义与系统边界</h2><pre><code class=" mermaid">flowchart LR    U[User]    CHAT[Chat Interface]    AGENT[Agent Orchestrator]    MEM[(Session &amp; Memory Store)]    CE[Code Execution Runtime]    TOOLS[Internal/External Tools]    U --&gt; CHAT --&gt; AGENT    AGENT &lt;--&gt; MEM    AGENT --&gt; CE    CE --&gt; TOOLS    TOOLS --&gt; CE --&gt; AGENT --&gt; CHAT --&gt; U</code></pre><p>边界定义：</p><ul><li><strong>Control Plane</strong>：会话、记忆、策略、路由。</li><li><strong>Execution Plane</strong>：代码执行容器或沙箱运行时。</li><li><strong>Tool Plane</strong>：数据库、检索、业务 API、浏览器等外部能力。</li></ul><h2 id="2-执行流：从直接工具调用到程序化工具调用"><a href="#2-执行流：从直接工具调用到程序化工具调用" class="headerlink" title="2) 执行流：从直接工具调用到程序化工具调用"></a>2) 执行流：从直接工具调用到程序化工具调用</h2><p>Agentic Chatbot 的关键变化，是把多步工具调用下沉到执行环境中，以减少模型轮次开销。典型实现是 Programmatic Tool Calling（PTC）模式：模型生成可执行代码，在运行时中循环调用工具、过滤中间结果，再回传高价值输出。</p><pre><code class=" mermaid">sequenceDiagram    participant U as User    participant A as Agent Orchestrator    participant L as LLM    participant R as Execution Runtime    participant T as Tool/API    U-&gt;&gt;A: task    A-&gt;&gt;L: planning prompt    L--&gt;&gt;A: code-oriented action    A-&gt;&gt;R: run code block    loop in runtime        R-&gt;&gt;T: call tool        T--&gt;&gt;R: raw result        R-&gt;&gt;R: filter/aggregate/branch    end    R--&gt;&gt;A: compact result    A-&gt;&gt;L: final reasoning    L--&gt;&gt;A: answer    A--&gt;&gt;U: response</code></pre><p>该流程的工程意义是：中间数据处理不进入主上下文窗口，模型只消费压缩后的关键结果。</p><h2 id="3-状态模型：消息状态-环境状态-运行状态"><a href="#3-状态模型：消息状态-环境状态-运行状态" class="headerlink" title="3) 状态模型：消息状态 + 环境状态 + 运行状态"></a>3) 状态模型：消息状态 + 环境状态 + 运行状态</h2><p>Chatbot 阶段主要管理消息状态；Agentic Chatbot 需要三类状态并存。</p><pre><code class=" mermaid">flowchart LR    M[(Message State)]    E[(Environment State)]    R[(Run State)]    M --&gt; C[Context Builder]    E --&gt; C    R --&gt; C    C --&gt; P[Prompt/Action Decision]</code></pre><ul><li><strong>Message State</strong>：对话历史、摘要、用户偏好。</li><li><strong>Environment State</strong>：容器 ID、文件系统变更、工具上下文。</li><li><strong>Run State</strong>：任务阶段、工具调用链、重试与超时信息。</li></ul><p>系统可恢复性依赖于这三类状态的一致性，而不是单一会话历史。</p><h2 id="4-主要约束"><a href="#4-主要约束" class="headerlink" title="4) 主要约束"></a>4) 主要约束</h2><p>Agentic LLM API 的主要工程约束集中在五点：</p><ul><li><strong>安全约束</strong>：执行环境必须隔离，工具调用需要 caller 权限边界与 allowlist。</li><li><strong>时效约束</strong>：执行容器存在 TTL，跨步调用需要超时恢复和幂等设计。</li><li><strong>观测约束</strong>：需要保留 action trace、tool I&#x2F;O、版本化 prompt 以支持回放。</li><li><strong>成本约束</strong>：执行时长、容器复用、上下文压缩共同决定单位任务成本。</li><li><strong>供应商锁定约束</strong>：执行环境上下文（container state、intermediate artifacts、tool-call runtime state）主要保存在 API 厂商侧，迁移到其他模型供应商时，状态可移植性与行为一致性难以保证。</li></ul><p>关于供应商锁定，这一形态有两个具体后果：</p><ol><li><strong>状态不可携带</strong>：迁移供应商时，通常只能带走消息历史，难以带走运行时上下文。</li><li><strong>行为不可等价</strong>：即使接口相似，不同厂商在执行容器、超时策略、工具调用协议上的差异会导致任务行为漂移。</li></ol><p>锁定面的核心不在 API schema，而在运行时语义：</p><pre><code class=" mermaid">flowchart TB    subgraph HighPortability[High Portability]        M[Message History&lt;br/&gt;input/output transcripts]    end    subgraph MediumPortability[Medium Portability]        T[Tool Contracts&lt;br/&gt;name/schema/params]    end    subgraph LowPortability[Low Portability]        R[Runtime State&lt;br/&gt;container/session/artifacts]        S[Execution Semantics&lt;br/&gt;timeout/retry/caller policy]    end    M --&gt; T --&gt; R    T --&gt; S</code></pre><p>可迁移性通常呈分层下降：消息层 &gt; 工具协议层 &gt; 运行时状态层。</p><blockquote><p>这里和很多厂商（OpenAI &amp; Gemini）在API中隐藏自己的thinking过程不同，环境状态的丢失使得迁移供应商是不可能的，而非之前的“丢失一些中间思考”。</p></blockquote><h2 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h2><p>这对于模型厂商获取数据和用户锁定都有积极意义，也为第一方应用提供了非常强大的能力，但对于开发者来说，几乎不可能选择这类方案，其供应商锁定和不透明问题带来的风险太高了。</p><h1 id="Next-Agent-Mounts-Environment"><a href="#Next-Agent-Mounts-Environment" class="headerlink" title="Next: Agent Mounts Environment"></a>Next: Agent Mounts Environment</h1><p>Agent in VM 形态虽然提供了强交互能力，但存在两个核心问题：一是每用户独占 VM 的成本模式导致空闲成本高；二是 Agent 与用户共域，API Keys 与内部逻辑暴露在 VM 环境，难以安全整合私有服务。</p><p><strong>Agent Mounts Environment</strong> 将 Agent 与执行环境解耦：Agent 仍运行在平台侧（可访问私有服务、密钥隔离），但将执行环境降级为一个”挂载点”。VM 内仅运行轻量级 Executor，负责接收 Agent 的指令（Shell 命令、文件操作、浏览器控制）并反馈结果。</p><pre><code class=" mermaid">flowchart LR    subgraph Platform[&quot;Platform (Secure Zone)&quot;]        Agent[&quot;Agent Service&quot;]        PrivateAPI[&quot;Private Services&quot;]    end    subgraph VM[&quot;User VM (Mounted Environment)&quot;]        Executor[&quot;Executor&quot;]        Shell[&quot;Shell&quot;]        FS[&quot;Filesystem&quot;]    end    Agent &lt;--&gt;|&quot;SSH/CDP&quot;| Executor    Agent &lt;--&gt;|&quot;API&quot;| PrivateAPI    Executor --&gt; Shell    Executor --&gt; FS</code></pre><h2 id="好处"><a href="#好处" class="headerlink" title="好处"></a>好处</h2><p><strong>1. 安全隔离</strong>：Agent 与用户环境物理分离，密钥与内部逻辑不暴露在 VM 中。Agent 可安全访问私有数据库、内部 API，而无需在 VM 中存储凭证。</p><p><strong>2. 成本优化</strong>：Agent 进程不与用户 VM 绑定，可跨多用户复用（或采用共享计算资源池）。VM 仅作为执行容器，不需要持续为每用户保留计算资源。</p><p><strong>3. 权限边界清晰</strong>：Executor 可施加细粒度权限控制：允许的工具集、可访问的文件路径、外部 API 调用权限均由平台策略决定，用户无法绕过。</p><p><strong>4. 与 Agentic LLM API 兼容</strong>：该架构天然支持多模型厂商，Agent 逻辑独立于执行环境实现，迁移或切换 LLM 供应商时，运行时语义保持一致。</p><h2 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h2><p><strong>1. 通信协议复杂性</strong>：Agent 需要通过 SSH、SFTP、CDP Over SSH Tunnel 等多种协议与 VM 通信。每种协议的超时、重试、错误恢复策略都需要精心设计。协议层的不稳定会导致任务中断或重复执行。</p><p><strong>2. 状态一致性困难</strong>：Agent 侧的任务状态与 VM 侧的执行状态可能不一致（如网络分区、超时导致的半成功操作）。需要设计操作幂等性、事务语义与恢复协议来保证最终一致性。</p><p><strong>3. VM 快照与 Git 同步</strong>：若采用 VM 快照持久化完整运行时状态，需要在恢复后重新同步 Git 状态，否则磁盘内容与 Remote 不一致。这引入了额外的状态管理层。</p><h2 id="结论-2"><a href="#结论-2" class="headerlink" title="结论"></a>结论</h2><p>Agent Mounts Environment 是 Agent in VM 与 Agentic LLM API 之间的中间态：保留了本地执行环境的完整可观察性和持久状态能力，同时恢复了平台侧的安全边界与成本效率。代价是需要投入工程力量处理分布式系统的一致性问题。</p><h1 id="Bonus-Durable-Execution"><a href="#Bonus-Durable-Execution" class="headerlink" title="Bonus: Durable Execution"></a>Bonus: Durable Execution</h1><p>Agentic Loop 中的执行容易失败：网络中断、超时、工具错误、模型幻觉都可能中断任务。要支持重试与恢复，需要把任务状态分层管理，而不是盲目地快照整个 VM 或重新执行任务。</p><p>关键是认识到不同类型的状态有不同的恢复语义：</p><h2 id="1-上下文（Context）"><a href="#1-上下文（Context）" class="headerlink" title="1) 上下文（Context）"></a>1) 上下文（Context）</h2><p><strong>定义</strong>：LLM 模型推理所需的信息，包括prompt、当前目标、已执行步骤。</p><p><strong>恢复策略</strong>：上下文应完全可重建，无需从快照恢复。重试时重新加载上一步的结果，让模型基于完整的执行历史做出新的决策。</p><p><strong>为什么</strong>：即使模型上一次的决策有误，完整的history 能让它在第二次重试中做出不同的选择。</p><pre><code class=" mermaid">sequenceDiagram    participant A as Agent    participant M as Message Store    participant L as LLM    A-&gt;&gt;M: Load history + last result    M--&gt;&gt;A: [step1, step2, step3: failed]    A-&gt;&gt;L: Continue with full context    L--&gt;&gt;A: New action based on failure</code></pre><h2 id="2-消息状态（Message-State）"><a href="#2-消息状态（Message-State）" class="headerlink" title="2) 消息状态（Message State）"></a>2) 消息状态（Message State）</h2><p><strong>定义</strong>：对话历史、用户请求、AI 响应。</p><p><strong>恢复策略</strong>：消息状态应仅追加（append-only），支持版本化重试。一次重试对应一条新分支，记录”第一次尝试失败”与”第二次重试成功”的完整轨迹。</p><p><strong>存储方案</strong>：</p><pre><code class=" mermaid">flowchart LR    Root[&quot;Message 1: user request&quot;]    Exec1[&quot;Message 2a: Agent planning (attempt 1)&quot;]    Exec2[&quot;Message 2b: Agent planning (attempt 2)&quot;]    Result1[&quot;Message 3a: Tool result FAILED&quot;]    Result2[&quot;Message 3b: Tool result OK&quot;]    Root --&gt; Exec1    Root --&gt; Exec2    Exec1 --&gt; Result1    Exec2 --&gt; Result2</code></pre><p>通过消息树（而非单线性链表）记录重试分支，支持审计与故障分析。</p><h2 id="3-文件系统状态（Filesystem-State）"><a href="#3-文件系统状态（Filesystem-State）" class="headerlink" title="3) 文件系统状态（Filesystem State）"></a>3) 文件系统状态（Filesystem State）</h2><p><strong>定义</strong>：VM 或容器内的文件、目录、权限。</p><p><strong>恢复策略</strong>：文件系统状态通过 git commit 或 VM 快照持久化。重试时的关键决策是：<strong>是否需要恢复到操作前的状态还是基于当前状态重新操作</strong>。</p><p><strong>两种模式</strong>：</p><p><strong>模式 A：Ephemeral + Rollback（容器&#x2F;Sandbox 方案）</strong></p><ul><li>每次工具调用在隔离容器内执行</li><li>失败时容器销毁，文件系统回滚</li><li>下一次重试从 Git Checkout 状态开始</li></ul><pre><code class=" mermaid">sequenceDiagram    participant A as Agent    participant C1 as Container (attempt 1)    participant C2 as Container (attempt 2)    participant FS as Shared FS (Git managed)    FS--&gt;&gt;C1: mount current HEAD    A-&gt;&gt;C1: npm install    C1-&gt;&gt;FS: write XXX (not committed)    Note over C1: Fails, container destroyed    FS--&gt;&gt;C2: mount current HEAD (clean state)    A-&gt;&gt;C2: npm install (retry)    C2-&gt;&gt;FS: write YYY</code></pre><p>优点：简单、可预测。缺点：无法利用前一次的中间成果（如部分安装的依赖）。</p><p><strong>模式 B：Persistent VM + Checkpoint（VM 快照方案）</strong></p><ul><li>使用 Firecracker Snapshot 保存完整运行时状态</li><li>失败时从快照恢复，保留内存中的进程、缓存</li><li>需要额外的 checkpoint 机制记录”哪些文件修改可以撤销”</li></ul><pre><code class=" mermaid">sequenceDiagram    participant A as Agent    participant VM as VM    participant S as Snapshot    A-&gt;&gt;VM: Save checkpoint (git state + memory snapshot)    S--&gt;&gt;VM: checkpoint_id_v1    A-&gt;&gt;VM: npm install    VM-&gt;&gt;VM: partial install (process in memory)    Note over VM: Fails    A-&gt;&gt;S: Restore checkpoint_id_v1    S--&gt;&gt;VM: Memory + disk restored    A-&gt;&gt;VM: npm install (retry, hot cache)</code></pre><p>优点：快速恢复、热启动。缺点：快照本身占用存储空间，恢复涉及内存一致性问题。</p><h2 id="4-外部资源状态（External-Resource-State）"><a href="#4-外部资源状态（External-Resource-State）" class="headerlink" title="4) 外部资源状态（External Resource State）"></a>4) 外部资源状态（External Resource State）</h2><p><strong>定义</strong>：API 调用、数据库事务、第三方服务的执行结果。</p><p><strong>恢复策略</strong>：外部资源操作的可恢复性取决于操作是否幂等。</p><p><strong>分类</strong>：</p><table><thead><tr><th>操作类型</th><th>幂等性</th><th>恢复策略</th></tr></thead><tbody><tr><td>Read DB</td><td>✓</td><td>直接重试</td></tr><tr><td>GET API</td><td>✓</td><td>直接重试</td></tr><tr><td>Create resource</td><td>✗</td><td>需要 idempotency key，防止重复创建</td></tr><tr><td>Update resource</td><td>✓&#x2F;✗</td><td>需要检验当前状态是否已更新</td></tr><tr><td>Delete resource</td><td>~</td><td>重试返回 404，需要 idempotent delete semantic</td></tr><tr><td>Payment &#x2F; Charge</td><td>✗</td><td>必须通过事务 ID 检查是否已执行</td></tr></tbody></table><p><strong>实现模式</strong>：Idempotency Key</p><pre><code class=" mermaid">sequenceDiagram    participant A as Agent    participant API as External API    participant Cache as Idempotency Cache    A-&gt;&gt;Cache: Lookup idempotency_key = &quot;task_step_3&quot;    Cache--&gt;&gt;A: Not found    A-&gt;&gt;API: POST /charge (idempotency_key: &quot;task_step_3&quot;, amount: 100)    API-&gt;&gt;Cache: Save result (charged: true)    Cache--&gt;&gt;API: OK    API--&gt;&gt;A: success    Note over A: Network timeout, retry    A-&gt;&gt;Cache: Lookup idempotency_key = &quot;task_step_3&quot;    Cache--&gt;&gt;A: Found (charged: true)    A-&gt;&gt;A: Return cached result without re-charging</code></pre><p>系统需要为每个工具调用分配全局唯一的 idempotency_key，存储每次调用的结果。重试时先查询缓存，避免重复操作。</p><h2 id="综合设计"><a href="#综合设计" class="headerlink" title="综合设计"></a>综合设计</h2><p>完整的 Durable Execution 系统需要四层状态管理：</p><pre><code class=" mermaid">flowchart TB    Context[&quot;Context Layer&lt;br/&gt;(Stateless, computed from Message State)&quot;]    Message[&quot;Message State&lt;br/&gt;(Append-only, versioned history)&quot;]    FileSystem[&quot;Filesystem State&lt;br/&gt;(Git checkpoint or VM snapshot)&quot;]    External[&quot;External State&lt;br/&gt;(Idempotency cache + verification)&quot;]    Message --&gt; Context    Message --&gt; FileSystem    Message --&gt; External    Context -.-&gt; Recovery[&quot;Recovery via Context + Message Replay&quot;]</code></pre><p>在故障发生时，恢复顺序为：</p><ol><li>检查外部资源状态（是否已执行）→ 如有缓存结果则直接返回</li><li>检查文件系统状态（Snapshot 或 Git HEAD）→ 重建执行环境</li><li>加载消息历史 → 重建 LLM 上下文</li><li>重新执行失败的步骤</li></ol><h2 id="结论-3"><a href="#结论-3" class="headerlink" title="结论"></a>结论</h2><p>Durable Execution 的核心是认识到：<strong>系统的可恢复性不源于单一快照，而源于分层的状态管理与操作的幂等性设计</strong>。不同类型的状态有不同的持久化需求与恢复成本，混为一谈会导致过度的快照成本或无法恢复的局面。</p>]]></content>
    
    
    <summary type="html">本文将深入探讨云上Agent的架构设计，分析其核心组件和功能，以及如何实现高效的Agent系统。</summary>
    
    
    
    <category term="AI Engineering" scheme="https://blog.wh1isper.top/categories/AI-Engineering/"/>
    
    
    <category term="Agent" scheme="https://blog.wh1isper.top/tags/Agent/"/>
    
    <category term="架构设计" scheme="https://blog.wh1isper.top/tags/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="CloudAgent" scheme="https://blog.wh1isper.top/tags/CloudAgent/"/>
    
  </entry>
  
  <entry>
    <title>AI Native 组织思考</title>
    <link href="https://blog.wh1isper.top/2026/02/08/2026-02-08-AI-Native-Organization-Thoughts/"/>
    <id>https://blog.wh1isper.top/2026/02/08/2026-02-08-AI-Native-Organization-Thoughts/</id>
    <published>2026-02-08T01:51:00.000Z</published>
    <updated>2026-02-18T04:43:56.821Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>前言：</strong> 聊到了 AI Native 组织的形态，记录几个当下的直觉。</p></blockquote><h2 id="1-组织即灵魂"><a href="#1-组织即灵魂" class="headerlink" title="1. 组织即灵魂"></a>1. 组织即灵魂</h2><p>同样是基于 LLM，OpenAI 做出来的产品像大众工具，DeepMind 做出来的像科学仪器。产品不再是工业流水线上的标准化件，而是组织认知的直接投射。</p><p><strong>直觉：</strong> 不同的组织“性格”，决定了 AI 产品的“灵魂”。</p><h2 id="2-网络式共振"><a href="#2-网络式共振" class="headerlink" title="2. 网络式共振"></a>2. 网络式共振</h2><p>在 AI 时代，传统的层级式指令管理正在失效。因为创新的源头往往在一线研发的探索中涌现。</p><p><strong>直觉：</strong> 未来的组织更像是一个“超级兴趣小组”。大家因为一个远期目标聚在一起，通过高频的网络式协作（你读一篇 Paper，我试一个 Demo），把个体的认知拉齐、拉高。最终那个超越时代的产品，是在整个团队<strong>高认知水位</strong>上自然浮现的结果。</p><h2 id="3-商业化是真实世界的-RLHF"><a href="#3-商业化是真实世界的-RLHF" class="headerlink" title="3. 商业化是真实世界的 RLHF"></a>3. 商业化是真实世界的 RLHF</h2><p>商业化不仅仅是为了生存，它更是一个<strong>高价值信号过滤器</strong>。</p><p>免费用户的反馈往往是“好玩”（Novelty），付费用户的反馈往往是“没用”或“不准”（Utility）。只有那些愿意付费的用户反馈，才是最真实的 Reward Model，迫使团队直面真实世界的复杂性（Corner Cases），把“玩具”打磨成“工具”。</p><p><strong>直觉：</strong> 商业化把“用户反馈”转化为了高质量的“训练数据”。</p><h2 id="注脚：组织环境即-RL-Environment"><a href="#注脚：组织环境即-RL-Environment" class="headerlink" title="注脚：组织环境即 RL Environment"></a>注脚：组织环境即 RL Environment</h2><p>如果我们把组织看作一个强化学习（RL）的环境：</p><ul><li><strong>Agent：</strong> 每一个团队成员。</li><li><strong>Action：</strong> 每天的探索、决策、代码、讨论。</li><li><strong>Reward：</strong> 组织鼓励什么？是快速试错给正反馈，还是按部就班给正反馈？</li><li><strong>State：</strong> 整个团队当前的认知水位、技术栈、氛围。</li></ul><p>如果不精心设计这个 Environment 的 Reward Function，你就得不到那种自驱、涌现式的创新。</p>]]></content>
    
    
    <summary type="html">探讨 AI Native 组织的三点直觉：组织即灵魂、网络式共振、商业化是真实世界的 RLHF，以及组织环境本身作为 RL Environment 的思考。</summary>
    
    
    
    <category term="随笔" scheme="https://blog.wh1isper.top/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="随笔" scheme="https://blog.wh1isper.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
    <category term="AI Native" scheme="https://blog.wh1isper.top/tags/AI-Native/"/>
    
    <category term="组织思考" scheme="https://blog.wh1isper.top/tags/%E7%BB%84%E7%BB%87%E6%80%9D%E8%80%83/"/>
    
  </entry>
  
  <entry>
    <title>AI 辅助开发下，如何保持项目一致性</title>
    <link href="https://blog.wh1isper.top/2026/02/02/2026-02-02-AI-Assisted-Development-Consistency/"/>
    <id>https://blog.wh1isper.top/2026/02/02/2026-02-02-AI-Assisted-Development-Consistency/</id>
    <published>2026-02-02T15:00:00.000Z</published>
    <updated>2026-02-18T04:43:56.821Z</updated>
    
    <content type="html"><![CDATA[<p>AI 正在改变软件开发的方式。Cursor、Claude Code、Copilot 这些工具让编码速度提升了数倍，但也带来了新的挑战：<strong>代码写得太快，理解跟不上，项目很容易腐化</strong>。</p><p>这篇文章分享我在 AI 辅助开发中保持项目一致性的一些实践。</p><h2 id="核心洞察：瓶颈变了"><a href="#核心洞察：瓶颈变了" class="headerlink" title="核心洞察：瓶颈变了"></a>核心洞察：瓶颈变了</h2><p>传统软件开发的瓶颈是编码。设计完成后，大量时间花在实现上。</p><pre><code class=" mermaid">flowchart LR    A[设计] --&gt; B[编码]    B --&gt; C[Review]    C --&gt; D[修改]    D --&gt; B</code></pre><p>AI 辅助开发打破了这个瓶颈。编码变得很快，但新的问题出现了：</p><pre><code class=" mermaid">flowchart LR    A[设计] --&gt; B[AI编码]    B --&gt; C[代码量暴增]    C --&gt; D[理解跟不上]    D --&gt; E[架构腐化]    E --&gt; F[更多patch]    F --&gt; E</code></pre><p><strong>编码不再是瓶颈，理解和架构才是。</strong></p><h2 id="方法论：控制点上移"><a href="#方法论：控制点上移" class="headerlink" title="方法论：控制点上移"></a>方法论：控制点上移</h2><p>要打破这个负循环，关键是把控制点从”代码”上移到”架构”。</p><pre><code class=" mermaid">flowchart TB    subgraph Control[控制层]        A[Spec文档]        B[Owner审阅]    end    subgraph Constraint[约束层]        C[agents.md]        D[类型系统]        E[项目规则]    end    subgraph Execution[执行层]        F[AI生成代码]        G[Prototype]        H[生产代码]    end    A --&gt; B    B --&gt; C    B --&gt; D    B --&gt; E    C --&gt; F    D --&gt; F    E --&gt; F    F --&gt; G    G --&gt;|验证通过| H    G --&gt;|需要调整| A</code></pre><h3 id="1-Spec-文档驱动"><a href="#1-Spec-文档驱动" class="headerlink" title="1. Spec 文档驱动"></a>1. Spec 文档驱动</h3><p>把脑海里的架构变成和 AI 讨论的 spec 文档：</p><ul><li><strong>概要设计</strong>：系统边界、模块划分、核心流程</li><li><strong>架构图&#x2F;流程图</strong>：用 mermaid 或手绘，让 AI 理解上下文</li><li><strong>DDD 思路</strong>：bounded context、聚合、领域语言</li></ul><p>不需要细到接口签名。模块边界清晰，AI 就不会跨模块乱搞。</p><pre><code class=" mermaid">flowchart LR    A[脑海中的架构] --&gt; B[Spec文档]    B --&gt; C[与AI讨论]    C --&gt; D[AI生成代码]    D --&gt; E[代码符合架构]</code></pre><h3 id="2-自底向上构建"><a href="#2-自底向上构建" class="headerlink" title="2. 自底向上构建"></a>2. 自底向上构建</h3><p>很多人习惯自顶向下：先设计接口，再实现细节。但在 AI 辅助开发中，我发现<strong>自底向上更有效</strong>：</p><ul><li><strong>先做配置</strong>：配置是系统的”骨架”，定下来后 AI 生成的代码就有约束</li><li><strong>先定类型</strong>：类型系统是天然的约束，让 AI 在框里活动</li><li><strong>从不易变的开始</strong>：基础设施、工具函数、配置管理</li></ul><p>这样做的好处：</p><ul><li>更容易获得良好的抽象</li><li>更容易进行测试覆盖</li><li>AI 生成的代码有锚点，不会飘</li></ul><h3 id="3-约束前置"><a href="#3-约束前置" class="headerlink" title="3. 约束前置"></a>3. 约束前置</h3><p>与其事后 review 代码，不如事前约束 AI：</p><pre><code class=" mermaid">flowchart LR    subgraph Constraints[约束]        A[agents.md]        B[pyright]        C[ESLint]        D[项目规则]    end    E[AI] --&gt; Constraints    Constraints --&gt; F[符合规范的代码]</code></pre><ul><li><strong>agents.md &#x2F; AGENTS.md</strong>：写清楚项目的架构、约定、禁忌</li><li><strong>类型系统</strong>：pyright、TypeScript，静态分析是最好的约束</li><li><strong>项目规则</strong>：命名规范、目录结构、commit 格式</li></ul><p>AI 读了这些约束，生成的代码一致性会好很多。</p><h3 id="4-早期重构"><a href="#4-早期重构" class="headerlink" title="4. 早期重构"></a>4. 早期重构</h3><p>重构应该在<strong>最早的时候</strong>进行，而不是等代码堆积成山：</p><ul><li>代码量小，重构成本低</li><li>最容易利用 AI 的生成能力</li><li>最不容易受幻觉影响</li></ul><p>等到项目复杂了再重构，AI 会产生更多幻觉，因为它无法完全理解所有上下文。</p><h3 id="5-快速验证，慎重生产"><a href="#5-快速验证，慎重生产" class="headerlink" title="5. 快速验证，慎重生产"></a>5. 快速验证，慎重生产</h3><pre><code class=" mermaid">flowchart LR    A[想法] --&gt; B[Prototype]    B --&gt; C&#123;验证&#125;    C --&gt;|通过| D[生产化]    C --&gt;|失败| E[调整想法]    E --&gt; A    D --&gt; F[留重构空间]</code></pre><ul><li><strong>尽快 prototype</strong>：用 AI 快速验证想法</li><li><strong>慎重生产</strong>：验证通过后再决定是否生产化</li><li><strong>留重构空间</strong>：不要过早固化，保持灵活性</li></ul><h2 id="正反馈循环"><a href="#正反馈循环" class="headerlink" title="正反馈循环"></a>正反馈循环</h2><p>好的实践会形成正反馈：</p><pre><code class=" mermaid">flowchart LR    A[好架构] --&gt; B[AI生成正确代码]    B --&gt; C[省时间]    C --&gt; D[优化架构]    D --&gt; A</code></pre><p>差的实践会形成负反馈（要避免）：</p><pre><code class=" mermaid">flowchart LR    A[烂架构] --&gt; B[代码到处patch]    B --&gt; C[越来越乱]    C --&gt; D[没时间重构]    D --&gt; A</code></pre><h2 id="团队协作"><a href="#团队协作" class="headerlink" title="团队协作"></a>团队协作</h2><p>在多人协作中，一致性更重要：</p><ul><li><strong>项目有 Owner</strong>：Owner 审阅 spec，保持架构一致性</li><li><strong>Review spec，不只是 review 代码</strong>：比传统 code review 更高效</li><li><strong>AI review 辅助</strong>：让 AI 检查代码是否符合 spec</li></ul><pre><code class=" mermaid">flowchart TB    A[Owner] --&gt;|审阅| B[Spec]    B --&gt;|指导| C[开发者1]    B --&gt;|指导| D[开发者2]    C --&gt; E[代码]    D --&gt; E    E --&gt;|AI Review| F&#123;符合Spec&#125;    F --&gt;|是| G[合并]    F --&gt;|否| H[修改]    H --&gt; E</code></pre><h2 id="工程师角色转变"><a href="#工程师角色转变" class="headerlink" title="工程师角色转变"></a>工程师角色转变</h2><p>AI 辅助开发正在改变工程师的角色：</p><table><thead><tr><th>传统</th><th>AI 辅助</th></tr></thead><tbody><tr><td>写代码</td><td>设计架构</td></tr><tr><td>Debug</td><td>审阅 spec</td></tr><tr><td>重复劳动</td><td>创造性思考</td></tr></tbody></table><p>工程师的核心价值变成了：</p><ul><li><strong>架构抽象能力</strong>：基于当前理解设计合理的架构</li><li><strong>业务理解</strong>：对当前情况和未来进行抉择</li><li><strong>质量把控</strong>：确保 AI 生成的代码符合预期</li></ul><p>这本就是架构师的任务。有了 AI 辅助编码，我们可以有更多时间放在设计更容易维护的架构上，反过来又方便了 AI 编码，实现正反馈。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>AI 辅助开发的核心是<strong>人机协作</strong>：</p><ul><li><strong>人负责</strong>：架构决策、业务理解、质量把控</li><li><strong>AI 负责</strong>：快速实现、重复劳动、代码生成</li></ul><p>保持项目一致性的关键：</p><ol><li><strong>控制点上移</strong>：从代码到 spec</li><li><strong>约束前置</strong>：用规则和工具约束 AI</li><li><strong>早期重构</strong>：趁代码量小的时候</li><li><strong>快速验证</strong>：prototype 快，生产慢</li><li><strong>正反馈循环</strong>：好架构 → 好代码 → 更好架构</li></ol><p>AI 不会取代工程师，但会取代不会用 AI 的工程师。掌握 AI 辅助开发的方法论，才能在这个时代保持竞争力。</p>]]></content>
    
    
    <summary type="html">AI 辅助开发改变了软件工程的瓶颈，编码不再是瓶颈，理解和架构才是。本文分享如何通过 Spec 文档驱动、约束前置、早期重构等方法保持项目一致性。</summary>
    
    
    
    <category term="技术分享" scheme="https://blog.wh1isper.top/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"/>
    
    
    <category term="AI" scheme="https://blog.wh1isper.top/tags/AI/"/>
    
    <category term="软件工程" scheme="https://blog.wh1isper.top/tags/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/"/>
    
    <category term="架构设计" scheme="https://blog.wh1isper.top/tags/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>Agent 产品的软件腐化:一种新型技术债</title>
    <link href="https://blog.wh1isper.top/2026/02/01/2026-02-01-Agent-Software-Rot/"/>
    <id>https://blog.wh1isper.top/2026/02/01/2026-02-01-Agent-Software-Rot/</id>
    <published>2026-02-01T14:22:00.000Z</published>
    <updated>2026-02-18T04:43:56.821Z</updated>
    
    <content type="html"><![CDATA[<h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>最近和 AI 讨论了一个有意思的话题:在 AI Native 时代,Agent 产品存在一种独特的”软件腐化”——它不是传统意义上的代码腐化,而是发生在<strong>智能层面</strong>的劣化。</p><p>传统软件腐化讲的是代码层面:重复、耦合、复杂度上升。Agent 腐化是另一种东西——系统逐渐变得更蠢、更僵化、更难迭代,而且这种劣化在常规指标上往往不可见。</p><h2 id="一个典型场景"><a href="#一个典型场景" class="headerlink" title="一个典型场景"></a>一个典型场景</h2><p>产品经理说:”用户一提到数据库,就推荐我们的数据库产品,这样能提升功能渗透率。”</p><p>技术团队加上了这条规则。渗透率确实上去了。</p><p>然后产品经理又说:”用户提到性能问题,就推荐我们的监控方案。”</p><p>又加了一条规则。</p><p>一年后,Agent 里有 50 条这样的规则。产品经理拿着一个”纯净版”Agent 对比说:”为什么我们的 Agent 比这个蠢这么多?技术团队要提高智能水平。”</p><p>技术团队:”……”</p><h2 id="腐化的几种形态"><a href="#腐化的几种形态" class="headerlink" title="腐化的几种形态"></a>腐化的几种形态</h2><h3 id="1-规则堆积-Rule-Accumulation"><a href="#1-规则堆积-Rule-Accumulation" class="headerlink" title="1. 规则堆积 (Rule Accumulation)"></a>1. 规则堆积 (Rule Accumulation)</h3><p>每条规则单独看都合理,加起来就是:</p><ul><li>Prompt 越来越长,优先级冲突难以调试</li><li>模型的自主判断空间被不断压缩</li><li>改一个地方,另一个地方出 bug</li></ul><p>最讽刺的是:你花钱买了 GPT-4 的智能,然后用 if-else 覆盖了它的判断。</p><h3 id="2-上下文膨胀-Context-Bloat"><a href="#2-上下文膨胀-Context-Bloat" class="headerlink" title="2. 上下文膨胀 (Context Bloat)"></a>2. 上下文膨胀 (Context Bloat)</h3><p>为了让 Agent “更懂用户”,不断往 context 里塞东西:用户历史、产品信息、各种 metadata。</p><p>结果:上下文窗口被低价值信息填满,真正重要的信号被稀释,模型”注意力”被分散。</p><h3 id="3-工具蔓延-Tool-Sprawl"><a href="#3-工具蔓延-Tool-Sprawl" class="headerlink" title="3. 工具蔓延 (Tool Sprawl)"></a>3. 工具蔓延 (Tool Sprawl)</h3><p>一开始 5 个工具,边界清晰。后来 50 个工具,功能重叠,Agent 自己都不知道该调哪个。</p><p>工具选择错误率上升,维护成本爆炸。</p><h3 id="4-评估漂移-Evaluation-Drift"><a href="#4-评估漂移-Evaluation-Drift" class="headerlink" title="4. 评估漂移 (Evaluation Drift)"></a>4. 评估漂移 (Evaluation Drift)</h3><p>早期评估 Agent 真正的智能水平。后来评估变成”有没有触发这个规则””有没有推荐这个产品”。</p><p>指标和智能脱钩,团队在优化错误的东西,智能下降但指标上升,问题被掩盖。</p><h3 id="5-人设分裂-Persona-Fragmentation"><a href="#5-人设分裂-Persona-Fragmentation" class="headerlink" title="5. 人设分裂 (Persona Fragmentation)"></a>5. 人设分裂 (Persona Fragmentation)</h3><p>产品说要”专业可靠”,运营说要”活泼有趣能带货”,客服说要”严谨不能出错”。</p><p>同一个 Agent 被拉向不同方向,最后人格分裂,用户感知”这个 AI 很奇怪”。</p><h2 id="温水煮青蛙"><a href="#温水煮青蛙" class="headerlink" title="温水煮青蛙"></a>温水煮青蛙</h2><p>这种腐化最危险的地方在于:它是渐进的。</p><p>每条规则单独看都”有效”——短期数据确实在涨:</p><ul><li>功能渗透率:↑</li><li>转化率:持平或略升</li><li>结论:”有效,继续加”</li></ul><p>但债务在暗处累积。用户体验不是一下子变差,是慢慢变得”不那么聪明”。用户说不出哪里不对,但就是不想用了。留存慢慢掉,归因不到任何单一功能。</p><p>等你意识到问题的时候,系统已经改不动了。</p><h2 id="根源在哪"><a href="#根源在哪" class="headerlink" title="根源在哪"></a>根源在哪</h2><ol><li><p><strong>短期指标驱动</strong>:每个决策都优化局部指标,没人为 Agent 整体智能负责。</p></li><li><p><strong>技术缺乏话语权</strong>:技术知道加规则有问题,但产品数据说”有效”,技术说了不算。</p></li><li><p><strong>因果被切断</strong>:产品加规则拿到渗透率功劳,智能下降技术背锅。制造债务的人不承担后果。</p></li><li><p><strong>没有”智能债务”概念</strong>:传统技术债有行业共识,Agent 智能债没有度量、没有意识。</p></li></ol><h2 id="如何对抗"><a href="#如何对抗" class="headerlink" title="如何对抗"></a>如何对抗</h2><h3 id="借鉴软件工程的经验"><a href="#借鉴软件工程的经验" class="headerlink" title="借鉴软件工程的经验"></a>借鉴软件工程的经验</h3><p>在传统软件工程中,架构师的职责是保持一致性、做对的抽象、防止腐化,辅以及时重构。</p><p>Agent 产品需要类似的角色:<strong>智能架构师</strong>。</p><h3 id="智能架构师的职责"><a href="#智能架构师的职责" class="headerlink" title="智能架构师的职责"></a>智能架构师的职责</h3><ol><li><p><strong>定义 Agent 的”宪法”</strong>:核心行为准则,所有规则都要服从它。</p></li><li><p><strong>守护 Prompt 的一致性</strong>:不是谁都能往里加东西。</p></li><li><p><strong>把控工具边界</strong>:新工具要审核,工具粒度要合理。</p></li><li><p><strong>拥有否决权</strong>:产品要加破坏智能的规则,可以说不。</p></li><li><p><strong>维护纯净 baseline</strong>:始终有一个无业务污染的版本作为参照。</p></li></ol><h3 id="建立”智能债务”指标"><a href="#建立”智能债务”指标" class="headerlink" title="建立”智能债务”指标"></a>建立”智能债务”指标</h3><ul><li>硬编码规则数量</li><li>Prompt 复杂度</li><li>模型自主决策比例 vs 规则覆盖比例</li><li>纯净版 vs 当前版的智能评分差距</li></ul><p>让债务可见,而不是只看功能渗透率。</p><h3 id="流程机制"><a href="#流程机制" class="headerlink" title="流程机制"></a>流程机制</h3><ul><li><strong>偿还计划</strong>:每条规则要有下线条件和时间,不是加了就永远在。</li><li><strong>定期清理</strong>:每季度 review 所有硬编码逻辑。</li><li><strong>决策权和后果绑定</strong>:谁加的规则,谁对智能指标负责。</li></ul><h3 id="组织架构"><a href="#组织架构" class="headerlink" title="组织架构"></a>组织架构</h3><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs excel"><span class="hljs-built_in">Product</span> (What/Why) ←→ Agent Architect (守护智能) ←→ Engineering (How)<br>                              ↓<br>                      Evaluation/QA (度量智能)<br></code></pre></td></tr></table></figure><p>关键:Agent Architect 要独立,有独立 KPI,可以 challenge 产品和工程双方。</p><h2 id="AI-Native-时代的特殊性"><a href="#AI-Native-时代的特殊性" class="headerlink" title="AI Native 时代的特殊性"></a>AI Native 时代的特殊性</h2><p>这里有一个更深的问题:在 Agent 产品中,技术团队的角色发生了变化。</p><p>传统软件:PM 定义 What&#x2F;Why,工程定义 How。</p><p>但 Agent 不一样:</p><ul><li><strong>能力边界不清晰</strong>:能不能做到、做到什么程度,取决于技术实现。</li><li><strong>How 决定了 What 的可能性</strong>:用什么模型、怎么做 tool calling、怎么处理上下文——这些不是实现细节,是产品形态的根本约束。</li><li><strong>体验是涌现的</strong>:Agent 的体验取决于它”怎么思考”,这完全是技术层面的事。</li></ul><p>所以,Agent 产品中的技术团队,不只是 How,而是要参与 Why:</p><ul><li>Why this approach works</li><li>What’s actually possible</li><li>Where the real value is</li></ul><p>这不是越界,是 Agent 产品的本质要求。</p><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>Google DORA 2025 报告有一句话说得很好:</p><blockquote><p>AI doesn’t fix a team; it amplifies what’s already there.</p></blockquote><p>Agent 腐化本质上是技术债 + 组织债的结合体。每个人都在优化自己的局部指标,没人看整体——这和 LLM “优先保证局部功能正确,而不是全局架构一致性”是同一个问题。</p><p>只解决技术层面不够,需要组织层面的改变。</p><p>如果你也在做 Agent 产品,希望这篇文章能让你在下次加规则之前,多问一句:这条规则的智能债务是什么?谁来偿还?</p>]]></content>
    
    
    <summary type="html">探讨 AI Agent 产品中特有的&quot;软件腐化&quot;现象——智能层面的劣化,以及如何通过组织设计和流程机制来对抗它。</summary>
    
    
    
    <category term="技术分享" scheme="https://blog.wh1isper.top/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"/>
    
    
    <category term="AI" scheme="https://blog.wh1isper.top/tags/AI/"/>
    
    <category term="Agent" scheme="https://blog.wh1isper.top/tags/Agent/"/>
    
    <category term="软件工程" scheme="https://blog.wh1isper.top/tags/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/"/>
    
    <category term="技术债务" scheme="https://blog.wh1isper.top/tags/%E6%8A%80%E6%9C%AF%E5%80%BA%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>Environment as Dependency Inversion</title>
    <link href="https://blog.wh1isper.top/2026/01/19/2026-01-20-environment-as-dependency-inversion/"/>
    <id>https://blog.wh1isper.top/2026/01/19/2026-01-20-environment-as-dependency-inversion/</id>
    <published>2026-01-19T16:00:00.000Z</published>
    <updated>2026-02-18T04:43:56.821Z</updated>
    
    <content type="html"><![CDATA[<p>When building AI agents, developers almost instinctively reach for file system operations and shell commands as their first tools. This isn’t accidental - it reflects a deeply ingrained assumption: <strong>the operating system is the natural environment for agents to act in</strong>.</p><p>In this post, we explore how the <code>Environment</code> abstraction in <a href="https://github.com/youware-labs/pai-agent-sdk">pai-agent-sdk</a> implements dependency inversion, why this design embeds OS-centric assumptions, and how it shapes everything from tool implementation to context engineering.</p><h2 id="The-Instinctive-Path"><a href="#The-Instinctive-Path" class="headerlink" title="The Instinctive Path"></a>The Instinctive Path</h2><p>Watch any developer build their first agent. The conversation typically goes:</p><ol><li>“I need my agent to do things”</li><li>“Doing things means reading&#x2F;writing files and running commands”</li><li>“Therefore, I need FileOperator and Shell”</li></ol><p>This mental model is so pervasive that it feels like the only way. But it’s actually a specific architectural choice that assumes agents operate in OS-like environments.</p><h2 id="Dependency-Inversion-The-Code-Level"><a href="#Dependency-Inversion-The-Code-Level" class="headerlink" title="Dependency Inversion: The Code Level"></a>Dependency Inversion: The Code Level</h2><p>From a pure code perspective, <a href="https://github.com/youware-labs/pai-agent-sdk">pai-agent-sdk</a> implements classic dependency inversion:</p><pre><code class=" mermaid">graph TB    subgraph &quot;High-Level Modules&quot;        Agent[Agent]        Toolset[Toolset]    end    subgraph &quot;Abstractions&quot;        Env[Environment ABC]        FO[FileOperator Protocol]        SH[Shell Protocol]    end    subgraph &quot;Low-Level Implementations&quot;        Local[LocalEnvironment]        Docker[DockerEnvironment]    end    Agent --&gt; Env    Toolset --&gt; Env    Env --&gt; FO    Env --&gt; SH    Local -.-&gt;|implements| Env    Docker -.-&gt;|implements| Env</code></pre><p>Tools don’t know whether they’re running locally or in a container:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ViewTool</span>(<span class="hljs-title class_ inherited__">BaseTool</span>):<br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, ctx: RunContext[AgentContext], file_path: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">str</span>:<br>        <span class="hljs-comment"># Uses abstraction, not concrete implementation</span><br>        file_operator = ctx.deps.file_operator<br>        content = <span class="hljs-keyword">await</span> file_operator.read_file(file_path)<br>        <span class="hljs-keyword">return</span> content<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ShellTool</span>(<span class="hljs-title class_ inherited__">BaseTool</span>):<br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, ctx: RunContext[AgentContext], command: <span class="hljs-built_in">str</span></span>) -&gt; ShellResult:<br>        <span class="hljs-comment"># Same pattern - abstract Shell interface</span><br>        shell = ctx.deps.shell<br>        exit_code, stdout, stderr = <span class="hljs-keyword">await</span> shell.execute(command)<br>        <span class="hljs-keyword">return</span> ShellResult(stdout=stdout, stderr=stderr, return_code=exit_code)<br></code></pre></td></tr></table></figure><p>This is textbook dependency inversion:</p><ul><li>High-level modules (Agent, Tools) depend on abstractions</li><li>Low-level modules (LocalEnvironment, DockerEnvironment) implement abstractions</li><li>Abstractions don’t depend on details</li></ul><h2 id="The-Conceptual-Leakage"><a href="#The-Conceptual-Leakage" class="headerlink" title="The Conceptual Leakage"></a>The Conceptual Leakage</h2><p>But here’s the subtle issue: <strong>the shape of our abstractions is molded by OS concepts</strong>.</p><table><thead><tr><th>Abstraction</th><th>Derived From</th></tr></thead><tbody><tr><td><code>FileOperator</code></td><td>POSIX filesystem semantics</td></tr><tr><td><code>Shell</code></td><td>Unix shell execution model</td></tr><tr><td><code>ResourceRegistry</code></td><td>Process resource management</td></tr><tr><td><code>tmp_dir</code></td><td><code>/tmp</code> directory concept</td></tr></tbody></table><p>Even when we successfully invert dependencies at the code level, we’re still thinking in terms of “files”, “directories”, “commands”, and “environment variables”. The abstraction has absorbed the OS worldview.</p><p>This isn’t necessarily wrong - it’s a pragmatic choice. But it limits our imagination when considering alternative environments.</p><h2 id="The-Ripple-Effect-Tool-Availability"><a href="#The-Ripple-Effect-Tool-Availability" class="headerlink" title="The Ripple Effect: Tool Availability"></a>The Ripple Effect: Tool Availability</h2><p>The OS-centric design manifests in how tools check their availability:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ShellTool</span>(<span class="hljs-title class_ inherited__">BaseTool</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">is_available</span>(<span class="hljs-params">self, ctx: RunContext[AgentContext]</span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        <span class="hljs-comment"># Tool becomes unavailable if shell isn&#x27;t configured</span><br>        <span class="hljs-keyword">if</span> ctx.deps.shell <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure><p>This creates an implicit contract: environments without shell capability simply can’t use shell-based tools. The tool system gracefully degrades, but the degradation path is defined by OS capabilities.</p><h2 id="Context-Engineering-Environment-Shapes-the-Prompt"><a href="#Context-Engineering-Environment-Shapes-the-Prompt" class="headerlink" title="Context Engineering: Environment Shapes the Prompt"></a>Context Engineering: Environment Shapes the Prompt</h2><p>Perhaps the most profound impact is on context engineering. The environment doesn’t just provide tools - it shapes how we communicate with the model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># From filters/environment_instructions.py</span><br><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">inject_environment_instructions</span>(<span class="hljs-params"></span><br><span class="hljs-params">    ctx: RunContext[<span class="hljs-type">Any</span>],</span><br><span class="hljs-params">    message_history: <span class="hljs-built_in">list</span>[ModelMessage],</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-built_in">list</span>[ModelMessage]:<br>    <span class="hljs-comment"># Get environment-specific instructions</span><br>    instructions = <span class="hljs-keyword">await</span> env.get_context_instructions()<br><br>    <span class="hljs-comment"># Inject into the conversation</span><br>    env_part = UserPromptPart(content=instructions)<br>    last_request.parts = [*last_request.parts, env_part]<br></code></pre></td></tr></table></figure><p>What does <code>get_context_instructions()</code> typically return? Something like:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">environment-context</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">file-system</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">default-directory</span>&gt;</span>/home/user/project<span class="hljs-tag">&lt;/<span class="hljs-name">default-directory</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">allowed-paths</span>&gt;</span>/home/user/project, /tmp/workspace<span class="hljs-tag">&lt;/<span class="hljs-name">allowed-paths</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">file-system</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">shell-execution</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">default-timeout</span>&gt;</span>30s<span class="hljs-tag">&lt;/<span class="hljs-name">default-timeout</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">working-directory</span>&gt;</span>/home/user/project<span class="hljs-tag">&lt;/<span class="hljs-name">working-directory</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">shell-execution</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">environment-context</span>&gt;</span><br></code></pre></td></tr></table></figure><p>The model receives instructions framed entirely in OS terminology. We’re not just providing tools - we’re teaching the model to think in terms of paths, directories, and shell commands.</p><h2 id="The-Three-Layers-of-Environment-Influence"><a href="#The-Three-Layers-of-Environment-Influence" class="headerlink" title="The Three Layers of Environment Influence"></a>The Three Layers of Environment Influence</h2><pre><code class=" mermaid">graph LR    subgraph &quot;1. Tool Implementation&quot;        TI[Tools use FileOperator/Shell abstractions]    end    subgraph &quot;2. Tool Availability&quot;        TA[Tools check environment capabilities]    end    subgraph &quot;3. Context Engineering&quot;        CE[Prompts include environment instructions]    end    Env[Environment] --&gt; TI    Env --&gt; TA    Env --&gt; CE    TI --&gt; Agent    TA --&gt; Agent    CE --&gt; Agent</code></pre><ol><li><strong>Tool Implementation</strong>: Tools operate through environment abstractions</li><li><strong>Tool Availability</strong>: Tools self-disable based on environment capabilities</li><li><strong>Context Engineering</strong>: System prompts are shaped by environment context</li></ol><p>All three layers reinforce the OS-as-environment paradigm.</p><h2 id="Alternative-Perspectives"><a href="#Alternative-Perspectives" class="headerlink" title="Alternative Perspectives"></a>Alternative Perspectives</h2><p>What if we didn’t assume OS as the default? Consider alternative environment types:</p><table><thead><tr><th>Environment Type</th><th>Core Abstractions</th><th>Use Case</th></tr></thead><tbody><tr><td><strong>OS Environment</strong></td><td>FileOperator, Shell</td><td>Code agents, automation</td></tr><tr><td><strong>API Environment</strong></td><td>HTTPClient, AuthProvider</td><td>API-only agents</td></tr><tr><td><strong>Data Environment</strong></td><td>QueryExecutor, SchemaProvider</td><td>Data analysis agents</td></tr><tr><td><strong>Conversation Environment</strong></td><td>MessageBus, StateStore</td><td>Pure dialogue agents</td></tr><tr><td><strong>Browser Environment</strong></td><td>DOMOperator, NavigationController</td><td>Web automation agents</td></tr></tbody></table><p>Each would require different:</p><ul><li>Tool implementations</li><li>Availability checks</li><li>Context instructions</li></ul><h2 id="Takeaways"><a href="#Takeaways" class="headerlink" title="Takeaways"></a>Takeaways</h2><ol><li><p><strong>Dependency inversion at code level</strong>: Achieved through Environment&#x2F;FileOperator&#x2F;Shell abstractions</p></li><li><p><strong>Conceptual dependency on OS</strong>: The abstractions themselves reflect OS-centric thinking</p></li><li><p><strong>Three-layer influence</strong>: Environment shapes tool implementation, availability, and context engineering</p></li></ol><p>The next time you build an agent and instinctively reach for file system tools, pause and ask: “Is this the right environment for my agent?” The answer might still be yes - but it’s worth asking the question.</p>]]></content>
    
    
    <summary type="html">Exploring how the Environment abstraction in pai-agent-sdk implements dependency inversion, why this design embeds OS-centric assumptions, and how it shapes everything from tool implementation to context engineering.</summary>
    
    
    
    <category term="技术分享" scheme="https://blog.wh1isper.top/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"/>
    
    
    <category term="LLM" scheme="https://blog.wh1isper.top/tags/LLM/"/>
    
    <category term="Agent" scheme="https://blog.wh1isper.top/tags/Agent/"/>
    
    <category term="AGI" scheme="https://blog.wh1isper.top/tags/AGI/"/>
    
  </entry>
  
  <entry>
    <title>PTC是一种端到端的方案</title>
    <link href="https://blog.wh1isper.top/2025/12/02/2025-12-03-ptc-is-end-to-end/"/>
    <id>https://blog.wh1isper.top/2025/12/02/2025-12-03-ptc-is-end-to-end/</id>
    <published>2025-12-02T16:00:00.000Z</published>
    <updated>2026-02-18T04:43:56.821Z</updated>
    
    <content type="html"><![CDATA[<p>最近在做大模型网关，之前也积累了比较丰富的Coding Agent经验，看了一些针对<a href="https://platform.claude.com/docs/en/agents-and-tools/tool-use/programmatic-tool-calling">Anthropic’s Programmatic Tool Calling</a>的分析，感觉都有一些不到位，技术上来说，Anthropic实现了一个服务端的<a href="https://arxiv.org/pdf/2402.01030">CodeAct</a>工具，将代码编写和执行都放在服务端进行，并不在API中完全暴露，由此，API的使用者可以在减少token消耗的情况下实现目标。</p><blockquote><p>如果在客户端实现，则至少需要编写代码-执行代码两个轮次，甚至更多</p></blockquote><p>下面这张图很好的解释了整个工作流程：</p><p><img src="https://www.anthropic.com/_next/image?url=https://www-cdn.anthropic.com/images/4zrzovbb/website/65737d69a3290ed5c1f3c3b8dc873645a9dcc2eb-1999x1491.png&w=3840&q=75"></p><p>以上基本上是大部分自媒体&#x2F;公众号&#x2F;营销号对于它的理解，以下我提供一些不一样的看法，可能不一定成熟。</p><h2 id="有状态API应该包含环境状态，而不是消息状态"><a href="#有状态API应该包含环境状态，而不是消息状态" class="headerlink" title="有状态API应该包含环境状态，而不是消息状态"></a>有状态API应该包含环境状态，而不是消息状态</h2><p>有状态API的起始是OpenAI的Responses API，在我看来其主要目的有二：</p><ol><li>允许客户端可以在发起任务之后异步获取结果，以减少服务器压力</li><li>更好地在隐藏推理细节的同时，提供连贯推理的服务</li></ol><p>但实际上，Responses API只是在性能上稍好，大部分时候OpenAI只享受到其数据安全的部分，因为Responses API实际上无状态模式，而大部分时候，我是使用无状态模式进行交互：实时拉取流，保存thinking signature而非id，完整回填整个消息列表</p><p><strong>PTC提供了一种带有环境状态的API，其编写、执行代码将对其服务端的对应环境造成影响</strong>，简单来说，过去我们让Agent改文件，所有文件状态的更新发生在我的本地，而Agent需要主动获取我本地的环境信息，这依赖于我，确切的说是我所使用的客户端，Claude Code、Codex CLI、Cline等等具体的工具实现，而PTC模式下，这些工具是在服务端沙盒实现的，没有实现者的bias、没有普适性要求、也没有那么多需要考虑的适配和安全问题。</p><h2 id="端到端的数据积累"><a href="#端到端的数据积累" class="headerlink" title="端到端的数据积累"></a>端到端的数据积累</h2><p>过去，模型公司收集到的用户使用数据只能通过消息，我们常说Cursor的价值在于积累了很多用户交互的真实数据，实际上指的就是环境数据和消息数据的结合。现在，PTC展示了一种模型厂直接端到端收集Agent数据的方式，通过一个已经跑通的、需要智能的场景，通过收集这方面的数据，或许能够切实地推进从ReAct到CodeAct的效率和智能提升。</p><p>Claude Code Agent SDK远远不够，PTC是Anthropic真正想要的东西。</p><p>那么，或许对bun的收购也顺理成章？</p>]]></content>
    
    
    <summary type="html">深入分析Anthropic的Programmatic Tool Calling(PTC)技术，探讨有状态API的环境状态设计理念，以及端到端数据积累对AI Agent发展的价值</summary>
    
    
    
    <category term="技术分享" scheme="https://blog.wh1isper.top/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"/>
    
    
    <category term="LLM" scheme="https://blog.wh1isper.top/tags/LLM/"/>
    
    <category term="AGI" scheme="https://blog.wh1isper.top/tags/AGI/"/>
    
  </entry>
  
  <entry>
    <title>Free from the coding language</title>
    <link href="https://blog.wh1isper.top/2025/11/28/2025-11-29-free-from-coding-language/"/>
    <id>https://blog.wh1isper.top/2025/11/28/2025-11-29-free-from-coding-language/</id>
    <published>2025-11-28T16:00:00.000Z</published>
    <updated>2026-02-18T04:43:56.821Z</updated>
    
    <content type="html"><![CDATA[<p>一篇碎碎念，好久没更新了。</p><p>最近花了很多时间在研究各个模型之间的差别，同一个prompt下面，不同的厂商的模型所表现出的trajectory差别巨大。同时，随着年末大家的混战，我们惊喜的发现OpenAI、Anthropic、Google三足鼎立的局面似乎正在形成。当我们觉得GPT-5 Codex横扫四方时，Sonnet 4.5的出色表现让我感觉Anthropic并未落后，而Gemini 3 Pro非常惊喜地让我们看到一个经济、速度、性能都非常均衡的选择。</p><p>最近我在使用Claude Opus 4.5来进行Rust项目的编写（构建一个LLM网关来进行智能路由，等我完成后会有博客来介绍），明显感觉到与Sonnet 4.5相比，Opus更加谦逊且精准，就我而言，目前最佳的使用方式仍然是与AI讨论设计，输出技术架构和详细设计文档，然后在手动控制上下文长度的前提下（大部分时候是控制每次的任务大小），让Agent能够通过编写测试或其他方式验证实现的情况下，来完成代码编写工作。这一次更不一样的是，我选择了我没有那么熟悉，但是编译器和工具链都非常成熟的Rust语言，结果也非常令人满意。这表明，随着模型能力的提升，我们或许可以更加激进地探索和学习新的技术栈，而不必过于担心自己Debug的能力不足，相反，架构设计、可测试性、可维护性等软实力将变得更加重要。</p>]]></content>
    
    
    <summary type="html">探讨AI时代编程语言选择的自由度，分享使用Claude Opus进行Rust项目开发的经验，以及模型能力提升对技术栈学习的影响</summary>
    
    
    
    <category term="随笔" scheme="https://blog.wh1isper.top/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="随笔" scheme="https://blog.wh1isper.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>Design Agentic Coding Agent</title>
    <link href="https://blog.wh1isper.top/2025/10/16/2025-10-17-design-agentic-coding-agent/"/>
    <id>https://blog.wh1isper.top/2025/10/16/2025-10-17-design-agentic-coding-agent/</id>
    <published>2025-10-16T16:00:00.000Z</published>
    <updated>2026-02-18T04:43:56.821Z</updated>
    
    <content type="html"><![CDATA[<p>是时候思考如何构建一个可扩展的云原生Coding Agent系统了。</p><h2 id="Agentic-Workflow"><a href="#Agentic-Workflow" class="headerlink" title="Agentic Workflow"></a>Agentic Workflow</h2><p>自<a href="https://www.claude.com/product/claude-code">Claude code</a>横空出世，人们越来越倾向于采用一个<a href="https://simonwillison.net/2025/Sep/18/agents/">简单的定义</a>来描述Agent：大型语言模型在循环中自主使用工具来完成某个目标。</p><p><img src="/../img/2025-10-17-design-agentic-coding-agent/agent-meme.png" alt="Meme of agent workflow"></p><h2 id="Agent-Design-Considerations"><a href="#Agent-Design-Considerations" class="headerlink" title="Agent Design Considerations"></a>Agent Design Considerations</h2><p>鉴于大模型的消息是无状态的，我们很容易拆分出LLM消息和工具实现两部分，MCP协议给了我们这样的一个example，通过streamable http或者本地stdio的方式，基于JSONRPC对工具定义进行分离。</p><p>接下来，我们很自然地思考，工具本身是否是有状态的？这就回到了Agent所针对的目标中。对于Coding Agent来说，其所处环境应是与人类程序员编程时使用的环境一样的开发环境，由以下组成：</p><ul><li>代码和相关文件，或者说repo</li><li>运行时及运行依赖（编译和调试容器、其他已部署的服务、数据库、本地需要安装的调试库等等）</li></ul><p>Agent本质上是在通过工具与上述两个环境进行交互，我们可以得出这样的描述：Agent通过不变的工具对环境进行改变，从而获得观察（observation），再指导其下一步动作。</p><p>这里及引出两个问题：</p><ol><li>工具一定是同步执行的吗？</li><li>环境如何与Agent消息进行同步？</li></ol><h2 id="Async-Tool-Calling-and-other-jobs"><a href="#Async-Tool-Calling-and-other-jobs" class="headerlink" title="Async Tool Calling(and other jobs)"></a>Async Tool Calling(and other jobs)</h2><p>大多数API都允许Tool Response与User Message同时包含在一次请求中，只需要满足Tool Call和Tool Response在一次LLM请求和响应之间是成对出现的即可。因此，我们可以通过User Message，或包装Tool Response来提醒Agent哪些任务已经完成可以再次获取，或者将其他的系统异步任务添加到消息中。</p><p>另一种方式是让Agent直接管理异步任务，但由于自动压缩等上下文管理策略，我们需要确保Agent不会忘记已经启动的任务，并观测其结果</p><p><img src="/../img/2025-10-17-design-agentic-coding-agent/async-job.png"></p><h2 id="Sync-Message-and-Environment"><a href="#Sync-Message-and-Environment" class="headerlink" title="Sync Message and Environment"></a>Sync Message and Environment</h2><p>现在，我们需要将消息和环境进行绑定，如果我们想在任意时刻进行回滚重试，那么对于每一次工具调用都对应了一个环境快照，当这一依赖影响到数据库等不一定能回滚的资源时，我们就必须针对这类资源进行特别设计。</p><p>基于不同的开发模式，我们可以为用户提供不同程度的重试和回滚策略，从最基本的staging环境+prod环境，再到通过脚本自动创建本地环境模拟等等方式，一种思路是通过Infrastructure as code (IaC)+unit test的方式，使用脚本来确保开发环境的可重现，另一种思路则是在基础设施层就支持这一特性。而针对Agent消息，我们可以通过各类durable execution的基础设施来实现，搭配RPC Tool Calling，实现Agent消息的编排，具体可以参考：</p><ul><li><a href="https://temporal.io/blog/what-is-durable-execution">https://temporal.io/blog/what-is-durable-execution</a></li><li><a href="https://langchain-ai.github.io/langgraph/concepts/durable_execution/#using-tasks-in-nodes">https://langchain-ai.github.io/langgraph/concepts/durable_execution/#using-tasks-in-nodes</a></li><li><a href="https://ai.pydantic.dev/durable_execution/overview/">https://ai.pydantic.dev/durable_execution/overview/</a></li></ul><h2 id="User-experience"><a href="#User-experience" class="headerlink" title="User experience"></a>User experience</h2><p>用户体验通常是Agent系统设计忽略的一环，实际上工具调用可能长时间的无法流式输出，特别是编辑特别大的代码文件时，这会造成很大的用户体验问题。<strong>良好的用户体验常常可以让用户享受创作和解决问题的过程，而非仅仅交付物本身</strong>。我认为我们可以通过拆解工具调用的流式阶段，再通过一个非常轻量化的模型来进行流式输出，以提供流畅、易懂的用户体验。如果我们将异步任务视为一个消息系统，则可以考虑“Agent发起任务” - “Agent等待任务完成” - “任务已完成，等待Agent响应” - “Agent正在处理响应并进行下一步”的循环流程，而不是只能向用户展示“Agent调用工具中” - “Agent调用工具完成”的序列，用户也可以更清楚的了解系统的工作流程。</p>]]></content>
    
    
    <summary type="html">探讨如何构建可扩展的云原生Coding Agent系统，分析Agentic Workflow设计理念，介绍基于MCP协议的Agent架构设计考量</summary>
    
    
    
    <category term="技术分享" scheme="https://blog.wh1isper.top/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"/>
    
    
    <category term="LLM" scheme="https://blog.wh1isper.top/tags/LLM/"/>
    
    <category term="Agent" scheme="https://blog.wh1isper.top/tags/Agent/"/>
    
    <category term="Code Agent" scheme="https://blog.wh1isper.top/tags/Code-Agent/"/>
    
    <category term="Coding" scheme="https://blog.wh1isper.top/tags/Coding/"/>
    
  </entry>
  
  <entry>
    <title>Thinking about debug agent</title>
    <link href="https://blog.wh1isper.top/2025/10/08/2025-10-09-about-debug-agent/"/>
    <id>https://blog.wh1isper.top/2025/10/08/2025-10-09-about-debug-agent/</id>
    <published>2025-10-08T16:00:00.000Z</published>
    <updated>2026-02-18T04:43:56.821Z</updated>
    
    <content type="html"><![CDATA[<p>简单记录一些对于Debug Agent的思考，与Coding Agent不同，Debug Agent需要包含更多的环境感知，需要更多的细节设计</p><h2 id="Context"><a href="#Context" class="headerlink" title="Context"></a>Context</h2><p>Debug Agent应当由三个重要部分组成：用户上下文、程序上下文和自动化交互方案</p><h3 id="用户上下文"><a href="#用户上下文" class="headerlink" title="用户上下文"></a>用户上下文</h3><p>当人们深入地使用Agent进行编程的时候，常常陷入debug困难的境地，表现为：</p><ol><li>很难描述自己遇到了什么问题，一种方案是进行截图或者录屏，然后交给一个有视觉、甚至可以处理视频（一般而言webp或者gif也可以）的agent来分析解决</li><li>很难给出问题栈，比如点击某个按钮之后，http请求出错了，如何把问题提交给Agent进行解决</li></ol><p>我希望通过“用户上下文”来描述此类场景，对应用户在使用产品的过程中遇到的bug和各种现象，也包括了运行时产生的各种上下文</p><h3 id="程序上下文"><a href="#程序上下文" class="headerlink" title="程序上下文"></a>程序上下文</h3><p>程序上下文实际上是Agent来理解软件的工程，软件不仅仅是代码组成，还包括了对代码业务的理解和说明，类似所有Spec-drive开发，对于需求文档、技术选型、代码仓库的长短期记忆与规划和代码本身构成了程序上下文。Agent基于对程序上下文进行决策，理解和解决问题。</p><h3 id="自动化交互方案"><a href="#自动化交互方案" class="headerlink" title="自动化交互方案"></a>自动化交互方案</h3><p>自动化交互方案是自动化测试在Agent上的实现，通过Agent进行交互来自动化地获取“用户上下文”。通过不同细粒度的自动化交互方案设计，如Browser use&#x2F;Unit test&#x2F;End-to-end test都对应了不同的用户上下文收集方式。</p><h2 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h2><p><img src="/../img/2025-10-09-about-debug-agent/workflow.png"></p><h2 id="发展阶段"><a href="#发展阶段" class="headerlink" title="发展阶段"></a>发展阶段</h2><p>我们分方面来看，程序上下文其实和Coding Agent基本一致，主要问题是保持软件开发过程中的文档和知识能够持续传承和更新；用户上下文与自动化交互相辅相成，是Debug Agent的重点。</p><h3 id="程序上下文-1"><a href="#程序上下文-1" class="headerlink" title="程序上下文"></a>程序上下文</h3><p>第一阶段，引入最基本的记忆文件，类似AGENTS.md, CLAUDE.md，记录项目的重要信息</p><p>第二阶段，结构化记忆，使用或结合memory文件夹&#x2F;RAG&#x2F;抽取等方案，自动地存取用户对项目的一些要求、用户的偏好</p><p>第三阶段，规范驱动，结合用户体验一起，设计交互模式来推进产品需求、设计、功能开发和测试的全套程序上下文记忆存储</p><h3 id="用户上下文与自动化交互"><a href="#用户上下文与自动化交互" class="headerlink" title="用户上下文与自动化交互"></a>用户上下文与自动化交互</h3><p>第一阶段，我们可以设计一系列工具（交互），让用户尽可能简单的反馈正确的用户上下文，同时集成一些简单的自动化交互来进行测试，比如截图、单元测试支持。目前看到良好的用户交互有：</p><ul><li>截图标注</li><li>gif&#x2F;webp录屏</li></ul><p>第二阶段，我们将设计一系列采集工具，对用户交互的运行时上下文进行自动化收集和分析，与分布式系统追踪类似，例如：</p><ul><li>network &amp; console日志自动捕获</li><li>后端日志自动化收集</li><li>其他指标监控</li></ul><p>第三阶段，我们需要自动化交互，并为自动化交互构建收集系统，这是自动化测试和监控的进阶，需要为LLM特别优化的输出才能得到足够好的效果</p>]]></content>
    
    
    <summary type="html">探讨Debug Agent的设计思考，分析用户上下文、程序上下文和自动化交互方案三个核心组成部分，与Coding Agent的差异与环境感知需求</summary>
    
    
    
    <category term="技术分享" scheme="https://blog.wh1isper.top/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"/>
    
    
    <category term="LLM" scheme="https://blog.wh1isper.top/tags/LLM/"/>
    
    <category term="Agent" scheme="https://blog.wh1isper.top/tags/Agent/"/>
    
    <category term="Vibe coding" scheme="https://blog.wh1isper.top/tags/Vibe-coding/"/>
    
    <category term="Debug" scheme="https://blog.wh1isper.top/tags/Debug/"/>
    
  </entry>
  
  <entry>
    <title>Evals is misleading?</title>
    <link href="https://blog.wh1isper.top/2025/09/09/2025-09-10-evals-is-misleading/"/>
    <id>https://blog.wh1isper.top/2025/09/09/2025-09-10-evals-is-misleading/</id>
    <published>2025-09-09T16:00:00.000Z</published>
    <updated>2026-02-18T04:43:56.821Z</updated>
    
    <content type="html"><![CDATA[<p>最近看了一些LLM评估的文章，很明显有两个倾向</p><ul><li>使用LLM进行评估（LLM-as-Judge）是一种AI-Native的方式，或许在Human alignement（对齐）上可以做到比较好，但仍然受限于简单任务，对于复杂任务人们很难模拟并自动化评估</li><li>由于复杂性，大多数产品不使用自动评估方法，而是通过研究员&#x2F;工程师的自主洞见，或者设计信号（Signal），进行A&#x2F;B实验来判断模型是否变好。Claude code“降智”事件可以看做是一次大型的量化模型A&#x2F;B实验（有人有证据证明某些时间sonnet和opus是使用量化模型进行serve的，anthropic声称是Bug）</li></ul><p>从我的理解上看，没有办法通过一个同等智能的模型评估另一个模型的思考过程，就如同使用AI检测AI一样，如果能被检测，那就一定能骗过检测，而当我们有更高级的智能来评估时，谁又来评估这个“更高级”的智能给我们带来了多少提升？最终我们只能达到两个结果：</p><ol><li>做了很多的事，得到了当前结果的算法验证，证明了目前的方法有用，可能产出一些对于当前方法为什么有用的洞见，仅此而已，并不对接下来的技术路线有指导意义</li><li>仍然通过人类来探索新方向，评估永远滞后</li></ol><p>既然评估只能解决一部分问题，我们应该做什么？<strong>或许我们不应该在现在开始研究评估，或许我们评估的目标并非中间产物</strong></p><p>这一观察可能与我们目前正在AI Coding的前沿有关，我们很明显的碰到了LLM的能力边界，因此开始研究各种Context Engineering的方式，以及思考Context和LLM如何协作。因此我更倾向于将模块拿出来进行评估，衡量每个模块在任务过程中的成本和性能，而非优化出某种想要的结果。简单说，我们应该衡量我们驱动LLM的方式，通过A&#x2F;B实验捕捉信号、还是通过定性定量分析，都是可以尝试的。</p><blockquote><p>世界上大部分人没有用过AI Coding，以后的AI Coding也不会是现在这个样子</p></blockquote><p><strong>警惕局部最优</strong></p><h2 id="参考阅读"><a href="#参考阅读" class="headerlink" title="参考阅读"></a>参考阅读</h2><ul><li>X上的一些讨论：<a href="https://x.com/justinstorre/status/1964029634796015685">https://x.com/justinstorre/status/1964029634796015685</a></li><li>A&#x2F;B测试平台表示没有auto judge，全是监控：<a href="https://www.raindrop.ai/blog/thoughts-on-evals">https://www.raindrop.ai/blog/thoughts-on-evals</a></li><li>系统性的评估是有益的：<a href="https://www.sh-reya.com/blog/in-defense-ai-evals/">https://www.sh-reya.com/blog/in-defense-ai-evals/</a></li></ul>]]></content>
    
    
    <summary type="html">探讨LLM评估方法的局限性，分析LLM-as-Judge和A/B实验等评估方式的问题，思考我们应该如何正确看待AI模型评估</summary>
    
    
    
    <category term="技术分享" scheme="https://blog.wh1isper.top/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"/>
    
    
    <category term="随笔" scheme="https://blog.wh1isper.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
    <category term="LLM" scheme="https://blog.wh1isper.top/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>LLM只是计算，Context才是内存</title>
    <link href="https://blog.wh1isper.top/2025/09/01/2025-09-02-context-is-memory/"/>
    <id>https://blog.wh1isper.top/2025/09/01/2025-09-02-context-is-memory/</id>
    <published>2025-09-01T16:00:00.000Z</published>
    <updated>2026-02-18T04:43:56.821Z</updated>
    
    <content type="html"><![CDATA[<p>LLM并非一台计算机，LLM目前只是一个处理器，人们通常将记忆、RAG等外置存储手段作为内存看待，但实际上，只有Context才能被看做内存，而这些外挂的存储手段，可以看作是一种“虚拟内存”，LLM通过工具调用或者工程师通过工程化的手段进行“换页”，人们将此称为Context Engineering。</p><p>我之前介绍过<a href="https://blog.wh1isper.top/2025/06/16/2025-06-17-context-engineering/">工程上的Context Engineering</a>策略，而LLM进行工具调用的方式，目前看分为两种模式：</p><ol><li>检索模式：通过向量检索、搜索引擎等方式进行搜索，理解返回结果</li><li>阅读模式：通过直接阅读文档进行理解</li></ol><p>显而易见，检索模式效率更高，但容易受限于RAG等技术，精确度低，工程难度大，这种方式流行的原因其实是因为简单，而非性能。</p><p>目前看，阅读模式的性能更优，但实现上需要有更多考虑：一方面，上下文长度的控制和对应工具实现很重要，通常会提供类似grep、glob等工具来进行代码搜索；另一方面，通过sub-agent的方式进行上下文隔离，可以减少context的消耗。</p><h2 id="未来如何"><a href="#未来如何" class="headerlink" title="未来如何"></a>未来如何</h2><p>我们看到从输入的Prompt Engineering到Context Engineering，我们已经将对LLM应用从简单的汇编语言操作寄存器（仅有输入的prompt）进化到C语言类似的，可进行内存管理的高级语言模式，更进一步地看，下一步或许是发明更高效的编译器技术，让用户的自然语言能够更好地被高级语言所理解和编译，也就是说，Agent（LLM+工程）能够根据用户的输入来更加自主、智能地控制上下文。这是我认为的，除去预训练和记忆模式以外的另一种Learning实现。</p>]]></content>
    
    
    <summary type="html">深入探讨LLM的Context Engineering理念，分析检索模式与阅读模式的优劣，理解Context作为LLM内存的核心概念</summary>
    
    
    
    <category term="技术分享" scheme="https://blog.wh1isper.top/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"/>
    
    
    <category term="随笔" scheme="https://blog.wh1isper.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
    <category term="LLM" scheme="https://blog.wh1isper.top/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>时间是人类的幻觉</title>
    <link href="https://blog.wh1isper.top/2025/08/31/2025-09-01-time-is-illusion/"/>
    <id>https://blog.wh1isper.top/2025/08/31/2025-09-01-time-is-illusion/</id>
    <published>2025-08-31T16:00:00.000Z</published>
    <updated>2026-02-18T04:43:56.821Z</updated>
    
    <content type="html"><![CDATA[<h4 id="LLM是没有时间概念的"><a href="#LLM是没有时间概念的" class="headerlink" title="LLM是没有时间概念的"></a>LLM是没有时间概念的</h4><p>我想起人们让Deepseek深度思考三秒后给出答案，Deepseek真的考虑一下什么是三秒，以及如何思考三秒。或许这就是肉身人类与硅基生命的区别。</p><h4 id="时间是人类最重要的幻觉"><a href="#时间是人类最重要的幻觉" class="headerlink" title="时间是人类最重要的幻觉"></a>时间是人类最重要的幻觉</h4><p>认识到时间对于自己的重要性，是认识到自身意义的开始。</p><p>如果一个人一直按部就班地活着，时间对他来说是最不值钱的，相比之下，不被破坏的规律是他最重要的东西。</p><p>但某一天，突然发现，一个人的人生之所以不同，就是因为每个人所体验到的世界是独一无二的，而体验世界的唯一必须，就是时间。我们可以简单的说，时间之于物是没有意义的，物随时间变化形态，往往是相同或者相似的，如聚沙成塔、滴水石穿。但时间之于思想缺失最重要的元素，因为思想，所以感受到了时间，因为时间，思想得以发展。</p><p>对于物质的人而言，时间是人类最重要的幻觉</p>]]></content>
    
    
    <summary type="html">探讨LLM没有时间概念的本质，反思时间对于人类的意义，以及时间之于思想发展的重要性</summary>
    
    
    
    <category term="随笔" scheme="https://blog.wh1isper.top/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="随笔" scheme="https://blog.wh1isper.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
    <category term="LLM" scheme="https://blog.wh1isper.top/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>重拾发呆</title>
    <link href="https://blog.wh1isper.top/2025/08/20/2025-08-21-the-art-of-daydreaming/"/>
    <id>https://blog.wh1isper.top/2025/08/20/2025-08-21-the-art-of-daydreaming/</id>
    <published>2025-08-20T16:00:00.000Z</published>
    <updated>2026-02-18T04:43:56.821Z</updated>
    
    <content type="html"><![CDATA[<p>最近我又开始可以发呆了。</p><p>高考结束之后，人生仿佛按下了快进键，在大学卷，在实习卷，在工作卷，只有付出努力才能获得回报。</p><p>不论是在地铁、出游、还是在家里，我都在思考，思考着课程、作业、工作内容、架构设计、赚了多少钱，对比着自己和别人的生活，叹息着自己的生活不如意，于是又催促自己再加把劲。</p><p>或许我就是这样失去了发呆的能力。</p><p>我曾经认为，发呆是灵感的来源，是快速休息的方式，我总会在课堂上、公交车上、地铁上发呆，什么也不想，后来听说这叫正念，所以，我似乎很早很早就掌握了正念，又在忙碌中失去了它。</p><blockquote><p>或许有人会说发呆和正念完全不同，但对我来说，发呆就是正念。</p></blockquote><p>自从去年burnout之后，我开始慢慢地恢复到以前的状态，开始主动地放慢节奏，主动地观察内心，直到最近，我发现我的内心平静到一定程度时，我又找回了发呆的感觉。这是一种在放任思维流动的感觉，在这个状态下，我可以想象或者不想象、思考或者不思考，而回报是一时间的灵光闪现。</p><p>所以，多发发呆吧，如果发现自己无法发呆，或许是时候放慢脚步。</p><p>想想那个一十二岁的自己。</p>]]></content>
    
    
    <summary type="html">分享从burnout中恢复的心路历程，探讨发呆与正念的关系，以及在忙碌生活中重拾内心平静的重要性</summary>
    
    
    
    <category term="随笔" scheme="https://blog.wh1isper.top/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="随笔" scheme="https://blog.wh1isper.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>杠杆效应-人、LLM与杠杆</title>
    <link href="https://blog.wh1isper.top/2025/08/02/2025-08-03-leverage-of-llm/"/>
    <id>https://blog.wh1isper.top/2025/08/02/2025-08-03-leverage-of-llm/</id>
    <published>2025-08-02T16:00:00.000Z</published>
    <updated>2026-02-18T04:43:56.820Z</updated>
    
    <content type="html"><![CDATA[<p>这是一篇随笔，在思考人类应该如何利用LLM的时候，我意识到杠杆效应是一个很好的思考角度。</p><h2 id="工具杠杆"><a href="#工具杠杆" class="headerlink" title="工具杠杆"></a>工具杠杆</h2><p>从工具杠杆的角度上看，LLM是一个很平均的工具，在不对其进行微调的时候，人们总能通过chatbot提升自己的效率，但由于单纯聊天产生的价值不高，人们需要通过高价值的劳动行为来提升杠杆率。</p><p>目前得到验证的工具杠杆是Coding，在上一个时代（互联网时代）已经验证过，代码可以不间断的运行并带来价值，其边际成本极低，作为信息技术可以带来极大的杠杆率。如果我们可以使用LLM加速代码的生成，则可以利用互联网时代的基建和系统，提升整个互联网的发展速度</p><p>在其他领域上，比如PPT、AI员工等工作，我发现其缺少反馈环境机制，通常依靠人在回路进行反馈，也受限于人类认识和审美，这方面的最主要问题是人类缺少高质量的员工，而平均值的人类+平均值的AI并不能产生多少价值，也不能消灭多少岗位（因为AI的价格也很贵）。这很类似于以基本工资雇佣老头老太太来进行环卫、保安、售票员等工作，自动化本身的价格可能比他们还高或者持平，但是考虑到就业，社会不得不从经济效益和人的角度创造一些毫无意义的工作，这在任何行业都成立，比如互联网企业也存在一大堆的“职能岗位”，即便他们最擅长的就是用代码来提高效率。</p><h2 id="知识杠杆"><a href="#知识杠杆" class="headerlink" title="知识杠杆"></a>知识杠杆</h2><p>另一个想法是，LLM作为知识杠杆能够加速人类摄取知识的速度，从书籍与印刷业得到的启示是，当信息获取的成本降低后，社会效率会进一步提高。另一方面，科学上低垂的果实已经被消耗殆尽，越来越多的科研创新依赖着大组织协作，人们需要越来越多的时间来学习基础知识才能参与到科学创新中，如果我们可以加速人类摄取知识的速度，或许科学创新的速度也能被增加。</p><p>但科学创新的反馈链路太长，人类的经济制度是否能帮助这一过程，或者这是对未来的美好想象</p><h2 id="二者结合"><a href="#二者结合" class="headerlink" title="二者结合"></a>二者结合</h2><p>从我的角度上看，人类可能正处于从学堂学习到实践学习的转化过程中，只是没有人意识到这一件事。LLM最神奇的地方在于其工具属性和知识属性共存，以AI Coding举例，人们在使用LLM进行代码编写的过程中，也在和AI进行结对编程，学习相关的编程知识，那么为什么人类不能和LLM工具一起成长呢？</p><p>一个可能的问题是，知识杠杆由于当前的教育体制设计，其反馈回路太长，导致产品只能设计为“做卷子”（chatgpt study mode）的模式，而非渐进式学习的模式。人们总假设有一个固定的答案，而不是探索一个真实世界的解决方案，人们面向的销售目标也是在授课体制内的学生、老师和家长，而不是面临真实世界问题的每个普通人。</p><p>另一方面，LLM在工具中做的也不好，人们无法相信一个“自己都做不好”的老师，这一方面是LLM的幻觉与事实核查。另一方面是人类自己对于表达、理解和最重要的动力的缺失，很多时候人们已经被资本主义规训成“有自我”的人，虽然这个自我意识是千篇一律的，受到灌输的自我。我们称一个只知道享乐，不希望思考，使用产品的目的是“解决问题”的人为<em>现代人</em>，而称一个时刻都在思考解决问题的人为<em>原始人</em>，当我们用这一方式思考AI，人类居然妄想通过程序员、产品经理、测试人员的工作岗位划分Agent来模拟资本主义下低效的世界，将被规训的低智商人类映射到对真实人类智能仿真的神经网络上，果不其然极大地降低了LLM的智能。从这一角度上看，现在的人类头脑本身是非常适合和LLM共同成长的，而现存人类的肉体头脑因为其生存的时间长度，受到资本主义规训的影响，导致他们变成了“人力资源”而不是通用智能，则大大阻止了人类与LLM共同成长。</p><p>于是我发现，在下一代AI-Native人类成长起来（或许永远也成长不起来）之前，人们只能接受无脑产品或者研究机器，这一未来是光明的，但其曲折的过程，则是对于我们这一代人的局限和悲剧。</p>]]></content>
    
    
    <summary type="html">从工具杠杆和知识杠杆角度分析LLM的价值，探讨Coding作为验证的工具杠杆，以及LLM如何加速人类学习和成长</summary>
    
    
    
    <category term="随笔" scheme="https://blog.wh1isper.top/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="AI" scheme="https://blog.wh1isper.top/tags/AI/"/>
    
    <category term="LLM" scheme="https://blog.wh1isper.top/tags/LLM/"/>
    
    <category term="AGI" scheme="https://blog.wh1isper.top/tags/AGI/"/>
    
  </entry>
  
  <entry>
    <title>AI Native的产品更应该暴露错误</title>
    <link href="https://blog.wh1isper.top/2025/07/25/2025-07-26-expose-more-error/"/>
    <id>https://blog.wh1isper.top/2025/07/25/2025-07-26-expose-more-error/</id>
    <published>2025-07-25T16:00:00.000Z</published>
    <updated>2026-02-18T04:43:56.820Z</updated>
    
    <content type="html"><![CDATA[<p>我曾在<a href="https://blog.wh1isper.top/2025/06/07/2025-06-08-agi-product-design/">之前的文章</a>中讨论过AI产品需要更端到端的设计来帮助用户发挥Agent智能，在目前来看，暴露错误是一个很好的让用户学习、同时进一步发挥模型智能的方式。</p><p>在传统的产品设计中，人们总是倾向于所谓的简洁，将一切复杂的原理藏在产品后面，让用户能够下意识的完成操作，人们总是假设用户无法理解产品背后的运行逻辑，不具备或不愿意花时间理解产品的技术细节。但在AI时代，每个人都需要通过表达来创造自己想要的东西，这时简洁反而成为了不安全感和困惑的来源。如果人们一定要面对复杂性，那么产品就一定要正确的暴露足够的细节，以便让人们能够更好地理解和掌握自己生成的东西的工作原理。</p><p>AI时代一个重要的改变是人们不再被专业知识而困扰，一个没有接触过web开发的非技术人士也可以通过大模型进行编码并部署在vercel之类的平台上，如果越来越多的人被纳入AI教育中，越来越多人开始使用AI，那么假设用户都是“不愿意学习的懒人”无疑是愚蠢的行为。基于这一假设，如果我们相信未来的世界是技术民主的，未来的人类是更会表达、更不机械、更有创造力的，我们就应该暴露更多的产品细节，创造更有可能性而不是更具可预测性的产品。</p><p>暴露错误是另一个让用户和产品一起成长的重要方式（<a href="https://blog.wh1isper.top/2025/07/04/2025-07-05-ergonomics-to-agent/">还有一个是TODO Tool</a>），通过精巧的设计，用户不再是对错误无能为力的人群，而是能够借助AI能力对错误进行修复并在其中成长的人，如果每个人都会有类似的成长过程，那么最适应这一成长过程的产品将会获胜，这或许是比“好用”更加重要的AI时代产品设计原则——成长性。</p><p>最后，通过成长性原则，我们可以帮助产品和用户建立起反脆弱的特点：从错误中学习并成长，从而出更少的错、创建更好的产品。由于AI的不确定性，随着AI的能力提升，产品对于AI的约束减少，产品的不稳定性也有可能上升(通常是AI和非AI的黏合初，很容易松动)，不同技能水平的用户使用其能力的方差应该也会增大，若可以实现一个机制帮助用户成长，那么我们就赋予了用户减少产品不稳定性的能力，从而实现用户和产品的结合，这或许是AI时代的用户粘性：不是chat、memory，而是experience point。</p>]]></content>
    
    
    <summary type="html">探讨AI时代产品设计的成长性原则，分析暴露错误如何帮助用户和产品一起成长，构建反脆弱的产品体验</summary>
    
    
    
    <category term="随笔" scheme="https://blog.wh1isper.top/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="AI" scheme="https://blog.wh1isper.top/tags/AI/"/>
    
    <category term="LLM" scheme="https://blog.wh1isper.top/tags/LLM/"/>
    
    <category term="AGI" scheme="https://blog.wh1isper.top/tags/AGI/"/>
    
    <category term="产品设计" scheme="https://blog.wh1isper.top/tags/%E4%BA%A7%E5%93%81%E8%AE%BE%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>Architecture of Agent System</title>
    <link href="https://blog.wh1isper.top/2025/07/19/2025-07-20-architecture-of-agent-system/"/>
    <id>https://blog.wh1isper.top/2025/07/19/2025-07-20-architecture-of-agent-system/</id>
    <published>2025-07-19T16:00:00.000Z</published>
    <updated>2026-02-18T04:43:56.820Z</updated>
    
    <content type="html"><![CDATA[<p>记录一些最近调研的 Agent 系统的架构。</p><h2 id="Long-term-Memory-System"><a href="#Long-term-Memory-System" class="headerlink" title="Long-term Memory System"></a>Long-term Memory System</h2><p>参考<a href="https://github.com/mem0ai/mem0">Mem0</a>和<a href="https://github.com/awslabs/amazon-bedrock-agentcore-samples/blob/main/01-tutorials/04-AgentCore-memory/02-long-term-memory/01-single-agent/using-strands-agent-memory-tool/culinary-assistant.ipynb">AWS Bedrock AgentCore Memory</a>，长期记忆一般是 LLM 蒸馏用户消息，形成浓缩消息和对应 embedding，从而支持语义化检索。</p><p>核心组件：</p><ul><li>LLM：浓缩消息</li><li>Embedding：对浓缩消息进行Embedding并支持语义化检索</li><li>Vector Database：Embedding的索引，支持快速检索</li></ul><p><img src="/../img/2025-07-20-architecture-of-agent-system/long-term-memory-system.png" alt="Long-term Memory System"></p><h2 id="Browser-use-Tool-System-Design"><a href="#Browser-use-Tool-System-Design" class="headerlink" title="Browser-use Tool &amp; System Design"></a>Browser-use Tool &amp; System Design</h2><p>参考<a href="https://github.com/browser-use/browser-use">browser-use</a>进行工具设计，并针对工作负载进行优化。</p><p>核心设计：</p><ul><li>Browser Tool设计<ul><li>基础的操作有打开、滑动、点击等</li><li>更复杂的是2FA、验证码等自动化操作，可能需要更细节的工具封装和工程化解析工作</li><li>有时可以总结某些网页为流程，通过AI进行启发式测试，再通过非AI的方式重放以提高效率</li></ul></li><li>无头浏览器的负载分离和安全隔离<ul><li>页面内容不受信任，可能存在恶意代码</li><li>页面可能存在bug，导致资源耗尽</li><li>控制爆炸半径，当浏览器崩溃时，只影响当前用户的浏览器实例</li></ul></li><li>与反爬虫手段对抗：capture resolver、ip proxy等</li></ul><p><img src="/../img/2025-07-20-architecture-of-agent-system/remote-browser-system-design.png" alt="Remote browser system design"></p><h2 id="Code-Execution-Sandbox"><a href="#Code-Execution-Sandbox" class="headerlink" title="Code Execution Sandbox"></a>Code Execution Sandbox</h2><p>与LLM的交互只需要全量的消息历史作为上下文，LLM会返回两类响应：<code>Tool Call Request</code>（工具调用）和<code>Text Message</code>（文本消息），而相对应的，Client需要对<code>Tool Call Request</code>进行处理，并将结果返回给LLM，暂且称之为<code>Tool Result Response</code>。Client处理则是依赖于“假设的环境”中，例如我们假设Agent运行在某台Linux机器上，并为LLM提供了编写代码和执行代码的工具，则Client在处理工具调用时，需要维护这个假设的环境的一致性：即编写了代码执行的时候，需要正确地执行代码并返回结果。此外，Client还需要关注工具的副作用，执行代码是一个非常好的例子，当两个用户在同一台机器上执行代码，则可能会互相影响，因此对于有副作用的工具调用，我们需要设计合理的隔离和沙箱机制。</p><p>而其他工具，比如搜索、浏览器等工具，则大部分不需要维护环境的一致性，则可以认为他们是无状态的。</p><p><img src="/../img/2025-07-20-architecture-of-agent-system/code-execution-sandbox.png" alt="Code Execution Sandbox"></p><p>为了更高的资源利用率和用户体验，我们需要同时解决隔离环境的启动速度和资源利用率的问题，目前有两种架构选择：</p><ul><li>gVisor：定制化内核路线，更好性能但更多网络&#x2F;系统调用限制，可能需要定制化开发</li><li>Firecracker：vm(kata container)技术路线，开箱即用，更好的兼容性，但启动速度较慢一些（对比gVisor）</li></ul><p>这里不展开细节，不使用runC(docker)的原因是其容易受到内核漏洞的影响，且启动速度较慢（1-10s）级别，在初期可以考虑使用runC进行PMF，但在后期需要考虑替换。</p>]]></content>
    
    
    <summary type="html">记录Agent系统架构调研，包括Long-term Memory系统设计、Browser-use工具设计和代码执行沙箱架构</summary>
    
    
    
    <category term="技术分享" scheme="https://blog.wh1isper.top/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"/>
    
    
    <category term="LLM" scheme="https://blog.wh1isper.top/tags/LLM/"/>
    
    <category term="Architecture" scheme="https://blog.wh1isper.top/tags/Architecture/"/>
    
    <category term="Agent" scheme="https://blog.wh1isper.top/tags/Agent/"/>
    
  </entry>
  
  <entry>
    <title>Ergonomics to Agent</title>
    <link href="https://blog.wh1isper.top/2025/07/04/2025-07-05-ergonomics-to-agent/"/>
    <id>https://blog.wh1isper.top/2025/07/04/2025-07-05-ergonomics-to-agent/</id>
    <published>2025-07-04T16:00:00.000Z</published>
    <updated>2026-02-18T04:43:56.820Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Agent-Design-人体工学"><a href="#Agent-Design-人体工学" class="headerlink" title="Agent Design - 人体工学"></a>Agent Design - 人体工学</h1><p>我们提供了一个TODO工具给Agent，让它可以列出TODO项，并在任务过程中对TODO项进行修改，这一过程会完全展示给用户。</p><blockquote><p>当我要求制作一个个人博客时，Agent列出了以下TODO：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">-</span> [] 创建个人博客的HTML结构<br><span class="hljs-bullet">-</span> [] 设计博客样式和布局<br><span class="hljs-bullet">-</span> [] 添加导航和页面内容<br><span class="hljs-bullet">-</span> [] 添加响应式设计<br></code></pre></td></tr></table></figure></blockquote><p>起初，我对这一功能并不感冒，因为我知道Agent的工作流程，并通常不依赖他进行架构设计和技术方案选择，因此我只需要关注他对Thinking工具的使用就可以了解他的思路。同时，根据我对Agent的了解，我认为TODO工具某种程度上增加Agent在通用任务中的惰性，不利于其自由发挥，不一定适合我们的产品。</p><p>但在推出这一功能之后，我发现我错了。</p><p>不仅仅是我们的产品经理和最活跃的用户，大部分技术人员也认为这增强了他们对Agent的掌控和理解，这让我意识到，或许我们在追求Agent性能的道路上，忽略了很多人体工学的内容，我们常常想着如何设计一个产品来交付结果，但忽略了人类与工具交互过程中，人类对于工具的控制、学习和理解。</p><h2 id="让用户在使用中成长"><a href="#让用户在使用中成长" class="headerlink" title="让用户在使用中成长"></a>让用户在使用中成长</h2><p>TODO工具最好的地方在于，让用户在使用的过程中成长。通过TODO工具，非技术人员可以了解到Agent对问题的拆解，从而学习到软件开发、架构设计等领域的知识。也许AI会有幻觉做出错误的编码或设计，但用户可以通过进一步地交互，和AI一起解决问题。这为非技术用户构建了一种在使用中学习的可能性，这是以前的工具类产品所不具有的特性。</p><p>上个月我曾<a href="https://blog.wh1isper.top/2025/06/07/2025-06-08-agi-product-design/">讨论</a>了AGI的产品设计，从端到端的来看，“让用户在使用中成长”可能是最重要的设计理念，这比过去美观的界面、易用的UI又或者是更高的付费转化率更为重要。这代表了AI智能的被驱动程度，而在这过程中人类输入-AI输出的方差，或许能够成为“场景为王”的强化学习下半场中重要的数据资产。</p><h1 id="Ergonomics-in-Agent-Design"><a href="#Ergonomics-in-Agent-Design" class="headerlink" title="Ergonomics in Agent Design"></a>Ergonomics in Agent Design</h1><blockquote><p>English version translated by Claude and Wh1isper(Human in the loop).</p></blockquote><p>We provided a TODO tool for the Agent, allowing it to list TODO items and modify them during the task process, with the entire process fully visible to users.</p><blockquote><p>When I requested to create a personal blog, the Agent listed the following TODO:</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">-</span> [] Create HTML structure for personal blog<br><span class="hljs-bullet">-</span> [] Design blog styles and layout<br><span class="hljs-bullet">-</span> [] Add navigation and page content<br><span class="hljs-bullet">-</span> [] Add responsive design<br></code></pre></td></tr></table></figure></blockquote><p>Initially, I wasn’t enthusiastic about this feature because I understand the Agent’s workflow and typically don’t rely on it for architectural design and technical solution selection. Therefore, I only needed to focus on its use of the Thinking tool to understand its thought process. At the same time, based on my understanding of the Agent, I believed the TODO tool would somewhat increase the Agent’s laziness in general tasks, hindering its creative freedom and not necessarily suiting our product.</p><p>But after launching this feature, I discovered I was wrong.</p><p>Not only our product managers and most active users, but also most technical personnel believed this enhanced their control and understanding of the Agent. This made me realize that perhaps in our pursuit of Agent performance, we’ve overlooked many ergonomic aspects. We often think about how to design a product to deliver results, but ignore human control, learning, and understanding during the human-tool interaction process.</p><h2 id="Enabling-Users-to-Grow-Through-Usage"><a href="#Enabling-Users-to-Grow-Through-Usage" class="headerlink" title="Enabling Users to Grow Through Usage"></a>Enabling Users to Grow Through Usage</h2><p>The best aspect of the TODO tool is that it enables users to grow through usage. Through the TODO tool, non-technical personnel can understand the Agent’s problem decomposition, thereby learning knowledge in areas such as software development and architectural design. AI might have hallucinations leading to incorrect coding or design, but users can work with AI to solve problems through further interaction. This creates a possibility for non-technical users to learn through usage, which is a characteristic that previous tool-based products didn’t possess.</p><p>Last month I <a href="https://blog.wh1isper.top/2025/06/07/2025-06-08-agi-product-design/">discussed</a> AGI product design. From an end-to-end perspective, “enabling users to grow through usage” might be the most important design philosophy, more important than past beautiful interfaces, user-friendly UI, or higher paid conversion rates. This represents the degree to which AI intelligence is driven, and the variance in human input-AI output during this process might become an important data asset in the second half of “scenario-driven” reinforcement learning.</p>]]></content>
    
    
    <summary type="html">探讨Agent产品的人体工学设计，分析TODO工具如何增强用户对Agent的掌控和理解，让用户在使用中成长</summary>
    
    
    
    <category term="随笔" scheme="https://blog.wh1isper.top/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="产品设计" scheme="https://blog.wh1isper.top/tags/%E4%BA%A7%E5%93%81%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="Product Design" scheme="https://blog.wh1isper.top/tags/Product-Design/"/>
    
  </entry>
  
  <entry>
    <title>AGI is possible and impossible</title>
    <link href="https://blog.wh1isper.top/2025/06/20/2025-06-21-AGI-is-possible-and-impossible/"/>
    <id>https://blog.wh1isper.top/2025/06/20/2025-06-21-AGI-is-possible-and-impossible/</id>
    <published>2025-06-20T16:00:00.000Z</published>
    <updated>2026-02-18T04:43:56.820Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>我与claude共同创作，大部分是我在写随笔，claude再帮我整理成文</p></blockquote><p>AGI作为一种革命性的生产力，正在与现有的制度框架、治理结构和权力配置发生深刻的冲突。已经有无数人告诉我们AGI可能在不远的将来就将达到，而人们也意识到模型的训练优化来到下半场，我们已经有了足够智力的模型来进行生产活动等等……所以AGI is possible，这点毋庸置疑，But…</p><p>但当我们将视角转向制度层面时，AGI的发展却面临着巨大的阻力。这种阻力主要来自于现有上层建筑对新兴生产力的不适应性。如果AGI的到来必然导致政府制度的重构，那么现有的制度安排必然会对AGI的发展形成阻碍。</p><h3 id="治理结构的滞后性"><a href="#治理结构的滞后性" class="headerlink" title="治理结构的滞后性"></a>治理结构的滞后性</h3><p>现有的政府治理结构是为工业时代和早期信息时代设计的，面对AGI这种颠覆性技术时显得力不从心。政府部门的科层制结构、决策流程的冗长性、以及对新技术理解的不足，都导致了政策制定的滞后。</p><p>与互联网发展初期的ARPANET项目不同，当时的技术发展相对线性可控，政府能够通过明确的目标和路径进行引导。而AGI的发展具有高度的不确定性和复杂性，传统的项目管理模式难以适应。政府往往只能在技术已经成熟后进行被动的监管，而非主动的引导和孵化。</p><h3 id="资源配置的扭曲"><a href="#资源配置的扭曲" class="headerlink" title="资源配置的扭曲"></a>资源配置的扭曲</h3><p>由于缺乏有效的政府引导机制，AI从业者只能通过”政绩工程”的方式获取资金支持。这种资源配置方式存在严重的扭曲：</p><ol><li><strong>短期导向</strong>：政绩工程通常追求短期可见的成果，而AGI的研发需要长期持续的投入</li><li><strong>形式主义</strong>：资金往往流向容易展示的应用项目，而非基础研究</li><li><strong>重复建设</strong>：各地政府为了政绩竞相上马AI项目，导致资源的严重浪费</li></ol><h3 id="法律法规的不适应性"><a href="#法律法规的不适应性" class="headerlink" title="法律法规的不适应性"></a>法律法规的不适应性</h3><p>现有的法律法规体系是基于传统生产方式建立的，面对AGI带来的新问题时显得捉襟见肘：</p><ol><li><strong>数据权属问题</strong>：大规模训练数据的获取和使用涉及复杂的知识产权和隐私权问题</li><li><strong>责任归属问题</strong>：AI系统的决策结果出现问题时，责任如何界定</li><li><strong>就业冲击问题</strong>：AGI可能导致大规模失业，现有的社会保障体系难以应对</li></ol><h2 id="权力关系的深层矛盾"><a href="#权力关系的深层矛盾" class="headerlink" title="权力关系的深层矛盾"></a>权力关系的深层矛盾</h2><p>AGI发展受阻的根本原因在于它对现有权力结构构成了威胁。政府作为权力的执掌者，对可能削弱其控制力的技术天然保持警惕。</p><h3 id="信息控制的失效"><a href="#信息控制的失效" class="headerlink" title="信息控制的失效"></a>信息控制的失效</h3><p>传统上，政府通过控制信息流动来维护权威。但AGI具备强大的信息处理和生成能力，可能绕过传统的信息控制机制。这使得政府对AGI技术既依赖又恐惧——既希望利用其能力提升治理效率，又担心失去对信息的垄断控制。</p><h3 id="决策权威的挑战"><a href="#决策权威的挑战" class="headerlink" title="决策权威的挑战"></a>决策权威的挑战</h3><p>AGI在某些领域的决策能力可能超越人类专家，这对传统的决策权威构成挑战。如果AI系统能够做出更优的政策建议，那么传统的官僚决策体系的合法性将受到质疑。</p><h3 id="监管悖论"><a href="#监管悖论" class="headerlink" title="监管悖论"></a>监管悖论</h3><p>政府面临着一个根本性的监管悖论：过度监管会扼杀创新，监管不足又可能失控。在这种两难境地下，政府往往选择相对保守的策略，这客观上限制了AGI的发展。</p><hr><p>从另一个更加深刻的角度来看，AGI的发展可能加剧一种新型的社会分化——不是传统意义上的阶级分化，而是基于对技术态度和使用方式的分化。我曾讨论过AI的两个重要方向：生成式娱乐和生产力替代，目前看来，二者是这一分化的核心。</p><h3 id="奶头乐效应的放大"><a href="#奶头乐效应的放大" class="headerlink" title="奶头乐效应的放大"></a>奶头乐效应的放大</h3><p>布兹曼和乔姆斯基早就警告过信息娱乐化的危险，而AGI可能将这种”奶头乐”效应推向极致。当AI能够提供无限定制化的娱乐内容、虚拟陪伴和即时满足时，大部分人可能会陷入一种舒适的被动消费状态中。</p><p>这种现象已经在短视频、游戏、社交媒体中初见端倪。AI的介入会让这种”算法投喂”变得更加精准和令人上瘾。个性化推荐系统会越来越了解用户的喜好，提供恰到好处的刺激，让人们在信息茧房中获得持续的多巴胺释放。</p><h3 id="人群的进一步分化"><a href="#人群的进一步分化" class="headerlink" title="人群的进一步分化"></a>人群的进一步分化</h3><p>在这种背景下，社会可能分化为两个截然不同的群体：</p><p><strong>创造者阶层</strong>：这是少数拥抱AI工具、与之协作的人群。他们不是被AI替代，而是将AI作为放大器，发挥人类独有的创造力、批判思维、价值判断和情感表达。他们掌握了人机协作的艺术，成为真正的生产力创造者。</p><p><strong>消费者阶层</strong>：这是大多数通过AI享受便利和娱乐的人群。他们习惯于被算法服务，逐渐丧失主动思考和创造的能力。虽然生活变得更加舒适，但本质上成为了高级的消费终端。</p><h3 id="生产力提升的虚假性"><a href="#生产力提升的虚假性" class="headerlink" title="生产力提升的虚假性"></a>生产力提升的虚假性</h3><p>这种分化带来一个深刻的悖论：虽然技术在快速进步，但整体的生产力提升可能并不显著。原因在于：</p><ol><li><strong>创造力的集中化</strong>：真正的创新和生产力提升集中在少数创造者手中，而他们的边际效应递减</li><li><strong>消费的无效性</strong>：大部分人的AI使用主要用于娱乐和消费，而非生产性活动</li><li><strong>人力资源的浪费</strong>：大量人力被困在低价值的信息消费中，无法转化为有效的生产力</li></ol><h3 id="信息囚笼的新形态"><a href="#信息囚笼的新形态" class="headerlink" title="信息囚笼的新形态"></a>信息囚笼的新形态</h3><p>马歇尔·麦克卢汉曾说”媒介即信息”，在AGI时代，这句话可能变成”算法即现实”。当AI能够无缝地生成符合个人偏好的内容时，人们可能永远被困在一个由算法构建的信息囚笼中，失去与真实世界的有效连接。</p><p>这种囚笼比传统的物理监禁更加隐蔽和有效，因为它提供的是舒适和快乐，而非痛苦和限制。正如赫胥黎在《美丽新世界》中描绘的那样，最可怕的控制不是通过恐惧，而是通过快乐。</p><hr><p>又或者，我们真的需要那么多生产力进步吗？在一个已经能够生产足够食物养活全球人口、足够住房容纳所有人、足够商品满足基本需求的世界里，我们面临的核心问题究竟是生产力不足，还是分配不公？</p><p>从这个角度重新审视AI的发展，我们会发现一个令人不安的事实：AI可能正在加剧而非解决根本性的不平等问题。</p><h2 id="生产力过剩的现实"><a href="#生产力过剩的现实" class="headerlink" title="生产力过剩的现实"></a>生产力过剩的现实</h2><h3 id="物质丰富与贫困并存的悖论"><a href="#物质丰富与贫困并存的悖论" class="headerlink" title="物质丰富与贫困并存的悖论"></a>物质丰富与贫困并存的悖论</h3><p>当代社会已经达到了历史上前所未有的物质丰富程度。全球粮食产量足以养活100亿人口，而地球人口还未达到80亿。发达国家的住房空置率居高不下，同时却有大量无家可归者。奢侈品市场蓬勃发展，基本生活用品却因为”没有利润”而供应不足给最需要的人群。</p><p>这种悖论表明，我们面临的不是生产能力的问题，而是分配机制的问题。继续追求生产力的无限增长，实际上可能是在回避真正的核心矛盾。</p><h3 id="技术发展的方向性偏差"><a href="#技术发展的方向性偏差" class="headerlink" title="技术发展的方向性偏差"></a>技术发展的方向性偏差</h3><p>当前的AI发展主要服务于两个目标：提高效率和创造利润。但效率的提高往往意味着人力的替代，利润的创造往往意味着资源向资本方的进一步集中。这种发展方向本质上是为了让有钱人更有钱，让有权人更有权，而不是为了解决分配不公的问题。</p><p>我们看到AI在金融交易、广告投放、消费者行为分析等领域的快速应用，这些都是为了更好地”榨取”价值，而不是创造真正有益于全人类的价值。</p><h2 id="AI发展对平权的双重影响"><a href="#AI发展对平权的双重影响" class="headerlink" title="AI发展对平权的双重影响"></a>AI发展对平权的双重影响</h2><h3 id="加剧不平等的机制"><a href="#加剧不平等的机制" class="headerlink" title="加剧不平等的机制"></a>加剧不平等的机制</h3><p>从平权的角度看，当前的AI发展呈现出明显的不平等放大效应：</p><p><strong>1. 技术门槛的提高</strong><br>AI技术的复杂性创造了新的知识壁垒。能够理解、使用和控制AI的人群获得了巨大的竞争优势，而无法跟上技术步伐的人群则被进一步边缘化。这种数字鸿沟比传统的教育差距更加难以跨越。</p><p><strong>2. 资本集中的加速</strong><br>AI的发展需要大量的计算资源、数据资源和人才资源，这些都向少数大型科技公司集中。这些公司因此获得了前所未有的市场垄断地位和社会影响力，进一步加剧了财富和权力的集中。</p><p><strong>3. 就业机会的两极化</strong><br>AI导致的就业替代主要影响中等技能的工作岗位，造成就业市场的”哑铃型”分布：高技能的AI开发者和低技能的服务业工作者，中间层被大量挤压。这种两极化加剧了社会的阶层固化。</p><p><strong>4. 决策权力的集中</strong><br>当AI系统越来越多地参与社会决策时，控制这些系统的少数人实际上获得了对大众生活的巨大影响力。算法的”黑箱”特性使得这种权力更加隐蔽和难以制衡。</p><h3 id="潜在的平权机会"><a href="#潜在的平权机会" class="headerlink" title="潜在的平权机会"></a>潜在的平权机会</h3><p>然而，AI发展也蕴含着一些平权的可能性：</p><p><strong>1. 知识获取的民主化</strong><br>AI可以让高质量的教育资源变得更加普及和个性化，理论上可以缩小知识差距。</p><p><strong>2. 创作门槛的降低</strong><br>AI工具可以让更多人参与到创作、设计、编程等原本需要专业技能的活动中，可能会催生新的创造者经济。</p><p><strong>3. 服务供给的均等化</strong><br>AI可以在一定程度上缓解优质服务（如医疗、法律咨询）供给不足的问题，让更多人享受到基本的服务。</p><h2 id="关键问题：技术为谁服务？"><a href="#关键问题：技术为谁服务？" class="headerlink" title="关键问题：技术为谁服务？"></a>关键问题：技术为谁服务？</h2><h3 id="市场逻辑的局限性"><a href="#市场逻辑的局限性" class="headerlink" title="市场逻辑的局限性"></a>市场逻辑的局限性</h3><p>当前AI发展主要由市场逻辑驱动，这意味着技术发展的方向主要服务于有支付能力的用户群体。富人的个性化需求得到精心满足，而穷人的基本需求却可能因为”没有商业价值”而被忽视。</p><p>这种市场导向的发展模式本质上是一种”技术势利主义”——技术进步主要服务于已经拥有优势的群体，而不是最需要帮助的群体。</p><h3 id="重新定义技术进步"><a href="#重新定义技术进步" class="headerlink" title="重新定义技术进步"></a>重新定义技术进步</h3><p>我们需要重新思考什么是真正的技术进步。如果技术发展不能让更多人过上更好的生活，不能缩小而是扩大社会差距，那么这种”进步”的意义何在？</p><p>真正的技术进步应该是：</p><ul><li>让基本需求得到更好满足的技术</li><li>让权力分布更加均衡的技术</li><li>让人类潜能得到更充分发挥的技术</li><li>让社会关系更加和谐的技术</li></ul><p>但就如最开始我们讨论的一样，AGI的技术平权挑战了资本主义的制度根本，那么政府在其中将会扮演什么样的角色？在现行体制下，是否真的impossible？</p><hr><p>抖音&#x2F;TikTok可能是当代最具争议性的技术产品之一。它既被赞誉为”内容创作的民主化革命”，也被批评为”注意力经济的终极陷阱”。在平权与不平等的天平上，抖音究竟扮演了什么角色？</p><p>这个问题的复杂性在于，抖音同时展现了技术进步的两种截然不同的可能性：它既是普通人表达自我、获得机会的平台，也是算法控制、注意力剥削的工具。理解这种悖论，对于我们思考整个数字时代的平权问题具有重要意义。</p><h2 id="民主化的一面：机会平等的技术实现"><a href="#民主化的一面：机会平等的技术实现" class="headerlink" title="民主化的一面：机会平等的技术实现"></a>民主化的一面：机会平等的技术实现</h2><h3 id="创作门槛的革命性降低"><a href="#创作门槛的革命性降低" class="headerlink" title="创作门槛的革命性降低"></a>创作门槛的革命性降低</h3><p>在抖音之前，内容创作是一个高门槛的活动。制作视频需要专业设备、剪辑技能、发行渠道，这些都将大多数普通人排除在外。电视台、影视公司、传统媒体控制着内容生产的全过程。</p><p>抖音的革命性在于它将视频制作简化到了极致：</p><ul><li><strong>技术门槛</strong>：一部智能手机就能完成拍摄、剪辑、发布的全流程</li><li><strong>学习成本</strong>：直观的界面设计让任何人都能快速上手</li><li><strong>分发渠道</strong>：智能推荐算法让优质内容有机会被更多人看到，不再依赖传统的关系网络</li></ul><h3 id="草根文化的崛起"><a href="#草根文化的崛起" class="headerlink" title="草根文化的崛起"></a>草根文化的崛起</h3><p>抖音确实催生了大量草根创作者的成功案例：</p><ul><li><strong>农村网红</strong>：像”华农兄弟”这样的农村创作者通过展示乡村生活获得了数百万粉丝</li><li><strong>手艺人获得新生</strong>：传统手工艺者通过短视频找到了新的传承和变现途径</li><li><strong>知识普及</strong>：专业人士通过短视频形式传播知识，如”快手菜”、科普内容等</li></ul><p>这些现象表明，抖音确实为原本被边缘化的群体提供了发声和发展的机会。</p><h3 id="知识传播的去中心化"><a href="#知识传播的去中心化" class="headerlink" title="知识传播的去中心化"></a>知识传播的去中心化</h3><p>抖音上的知识传播呈现出明显的去中心化特征：</p><ul><li><strong>专业知识的平民化</strong>：复杂的专业知识被包装成易懂的短视频</li><li><strong>实用技能的普及</strong>：烹饪、维修、种植等生活技能得到广泛传播</li><li><strong>文化交流的增进</strong>：不同地区、不同文化背景的人们能够更直接地交流</li></ul><p>从这个角度看，抖音确实起到了知识民主化的作用，让知识的获取不再受地域、阶层、教育背景的限制。</p><h2 id="剥削的一面：算法资本主义的新形态"><a href="#剥削的一面：算法资本主义的新形态" class="headerlink" title="剥削的一面：算法资本主义的新形态"></a>剥削的一面：算法资本主义的新形态</h2><h3 id="注意力的商品化"><a href="#注意力的商品化" class="headerlink" title="注意力的商品化"></a>注意力的商品化</h3><p>然而，抖音模式的核心逻辑是将用户的注意力转化为商业价值。这种模式存在根本性的不平等：</p><ul><li><strong>价值创造与收益分配的不对称</strong>：用户创造内容和提供注意力，但大部分商业价值被平台攫取</li><li><strong>数据主权的缺失</strong>：用户的行为数据被平台收集和利用，但用户无法从中获得相应回报</li><li><strong>创作者的依附性</strong>：看似”自由”的创作者实际上高度依赖平台的推荐机制</li></ul><h3 id="算法控制的隐蔽性"><a href="#算法控制的隐蔽性" class="headerlink" title="算法控制的隐蔽性"></a>算法控制的隐蔽性</h3><p>抖音的推荐算法看似公平，实际上隐含着深层的控制机制：</p><p><strong>1. 内容同质化的压力</strong><br>算法会奖励符合用户喜好的内容，这导致创作者不得不迎合算法的偏好，创作趋向同质化。真正的创新和批判性内容往往难以获得流量。</p><p><strong>2. 成瘾机制的设计</strong><br>无限滑动、精准推荐、间歇性奖励等设计都是为了最大化用户在平台上的停留时间，这种”时间收割”本质上是一种剥削。</p><p><strong>3. 价值观的隐性塑造</strong><br>算法会放大能够引起强烈情绪反应的内容，这往往意味着煽动性、娱乐性的内容获得更多曝光，而理性、深度的内容被淹没。</p><h3 id="新形式的数字鸿沟"><a href="#新形式的数字鸿沟" class="headerlink" title="新形式的数字鸿沟"></a>新形式的数字鸿沟</h3><p>抖音虽然降低了创作门槛，但也创造了新的不平等：</p><p><strong>1. 算法素养的差距</strong><br>理解并适应算法规则的创作者获得更多机会，而不懂算法逻辑的用户处于劣势。</p><p><strong>2. 资源投入的分化</strong><br>虽然基础创作门槛降低了，但要在激烈竞争中脱颖而出，仍需要专业团队、设备投入、营销策略等，这又将很多人排除在外。</p><p><strong>3. 平台依赖的风险</strong><br>创作者的成功高度依赖单一平台，一旦平台政策变化或账号被封，之前的积累可能瞬间归零。</p><h2 id="深层分析：技术民主化的幻象"><a href="#深层分析：技术民主化的幻象" class="headerlink" title="深层分析：技术民主化的幻象"></a>深层分析：技术民主化的幻象</h2><h3 id="参与不等于赋权"><a href="#参与不等于赋权" class="headerlink" title="参与不等于赋权"></a>参与不等于赋权</h3><p>抖音确实让更多人参与到了内容创作中，但参与本身并不等于真正的赋权。大多数用户仍然是算法和资本逻辑的被动接受者：</p><ul><li><strong>创作自由的有限性</strong>：创作者必须在平台规则框架内活动，真正的表达自由是有限的</li><li><strong>经济收益的马太效应</strong>：少数头部创作者获得大部分收益，绝大多数创作者收入微薄</li><li><strong>话语权的集中</strong>：虽然人人都能发声，但真正能影响社会议题的声音仍然集中在少数人手中</li></ul><h3 id="消费主义的强化"><a href="#消费主义的强化" class="headerlink" title="消费主义的强化"></a>消费主义的强化</h3><p>抖音虽然提供了创作机会，但其核心逻辑仍然是刺激消费：</p><ul><li><strong>种草经济</strong>：大量内容以推广商品为目的</li><li><strong>冲动消费</strong>：短视频格式特别适合激发用户的冲动购买欲望</li><li><strong>虚假需求的制造</strong>：算法会不断推送可能让用户产生购买欲望的内容</li></ul><p>这种逻辑实际上是在培养消费者，而不是创造者。</p><h2 id="地域和文化维度的复杂性"><a href="#地域和文化维度的复杂性" class="headerlink" title="地域和文化维度的复杂性"></a>地域和文化维度的复杂性</h2><h3 id="城乡差距的微妙变化"><a href="#城乡差距的微妙变化" class="headerlink" title="城乡差距的微妙变化"></a>城乡差距的微妙变化</h3><p>抖音对城乡差距的影响呈现出复杂的图景：</p><p><strong>缩小差距的方面</strong>：</p><ul><li>农村内容获得城市用户关注，一定程度上促进了城乡文化交流</li><li>农产品直播带货为农民提供了新的销售渠道</li><li>乡村旅游通过短视频获得推广机会</li></ul><p><strong>扩大差距的方面</strong>：</p><ul><li>城市创作者在技术、资源、营销能力方面仍有明显优势</li><li>乡村网红往往被要求表演”乡土性”，可能强化了城市对农村的刻板印象</li><li>注意力经济的逻辑可能会让农村地区更加依赖外部关注，而非内生发展</li></ul><h3 id="文化多样性的双重效应"><a href="#文化多样性的双重效应" class="headerlink" title="文化多样性的双重效应"></a>文化多样性的双重效应</h3><p><strong>积极效应</strong>：</p><ul><li>方言、地方文化通过短视频得到保护和传播</li><li>少数民族文化获得更多展示机会</li><li>传统手工艺、民俗文化找到新的传承方式</li></ul><p><strong>消极效应</strong>：</p><ul><li>文化展示可能趋于表面化、商业化</li><li>为了迎合主流审美，地方文化可能被”标准化”</li><li>文化的商品化可能损害其本真性</li></ul><h2 id="全球视角：TikTok的地缘政治意义"><a href="#全球视角：TikTok的地缘政治意义" class="headerlink" title="全球视角：TikTok的地缘政治意义"></a>全球视角：TikTok的地缘政治意义</h2><h3 id="信息主权的争夺"><a href="#信息主权的争夺" class="headerlink" title="信息主权的争夺"></a>信息主权的争夺</h3><p>TikTok在全球的发展也反映了平权问题的国际维度：</p><ul><li><strong>打破西方社交媒体垄断</strong>：为非西方国家提供了不同的社交媒体选择</li><li><strong>文化输出的新渠道</strong>：中国文化通过TikTok在全球范围内传播</li><li><strong>数据主权的争议</strong>：各国对TikTok的数据安全担忧反映了数字主权的重要性</li></ul><h3 id="发展中国家的机遇与挑战"><a href="#发展中国家的机遇与挑战" class="headerlink" title="发展中国家的机遇与挑战"></a>发展中国家的机遇与挑战</h3><p>对于发展中国家来说，TikTok既是机遇也是挑战：</p><ul><li><strong>绕过传统媒体门槛</strong>：直接向全球观众展示本国文化和产品</li><li><strong>数字鸿沟的风险</strong>：可能加剧与发达国家在数字技术方面的差距</li><li><strong>文化殖民的新形式</strong>：算法推荐可能强化某些文化的主导地位</li></ul><h2 id="结论：技术工具的中性神话"><a href="#结论：技术工具的中性神话" class="headerlink" title="结论：技术工具的中性神话"></a>结论：技术工具的中性神话</h2><p>抖音&#x2F;TikTok的案例清楚地表明，技术工具从来不是中性的。它的平权效应和不平等效应是同时存在的，关键在于我们如何设计、使用和监管这些技术。</p><h3 id="平权的真实性"><a href="#平权的真实性" class="headerlink" title="平权的真实性"></a>平权的真实性</h3><p>抖音确实在某些方面促进了平权：</p><ul><li>创作门槛的降低是真实的</li><li>知识传播的民主化是真实的</li><li>草根文化的崛起是真实的</li></ul><h3 id="剥削的隐蔽性"><a href="#剥削的隐蔽性" class="headerlink" title="剥削的隐蔽性"></a>剥削的隐蔽性</h3><p>但其剥削机制也是真实存在的：</p><ul><li>算法控制是真实的</li><li>注意力剥削是真实的</li><li>新形式的数字鸿沟是真实的</li></ul><p>我不知道这对人类是好还是坏，是更文明还是更野蛮，或许我们每一个人也不得不在这一滚滚向前的浪潮中找到自己的位置。</p>]]></content>
    
    
    <summary type="html">探讨AGI技术发展与制度框架的深层矛盾，分析治理滞后、资源扭曲、法律不适应等问题，以及AGI可能带来的社会分化。</summary>
    
    
    
    <category term="随笔" scheme="https://blog.wh1isper.top/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="AGI" scheme="https://blog.wh1isper.top/tags/AGI/"/>
    
  </entry>
  
  <entry>
    <title>Context Engineering-The Most Important Thing in Agent Development</title>
    <link href="https://blog.wh1isper.top/2025/06/16/2025-06-17-context-engineering/"/>
    <id>https://blog.wh1isper.top/2025/06/16/2025-06-17-context-engineering/</id>
    <published>2025-06-16T16:00:00.000Z</published>
    <updated>2026-02-18T04:43:56.820Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Context-Engineering-The-Most-Important-Thing-in-Agent-Development"><a href="#Context-Engineering-The-Most-Important-Thing-in-Agent-Development" class="headerlink" title="Context Engineering-The Most Important Thing in Agent Development"></a>Context Engineering-The Most Important Thing in Agent Development</h1><blockquote><p>English version is below. Translated by Claude and Wh1isper(Human in the loop).</p></blockquote><h2 id="What-is-Context-Engineering"><a href="#What-is-Context-Engineering" class="headerlink" title="What is Context Engineering"></a>What is Context Engineering</h2><p>前两个月我写了一篇<a href="https://blog.wh1isper.top/2025/04/07/2025-04-08-agent-design-note/">博客(Agent Design Note)</a>记录我在设计Coding Agent中的一些重点，现在我想完整的阐述一下对其中Context Engineering的理解。我认为这或许是目前构建任何Agent最重要的内容。</p><h2 id="Key-Points-of-Context-Engineering"><a href="#Key-Points-of-Context-Engineering" class="headerlink" title="Key Points of Context Engineering"></a>Key Points of Context Engineering</h2><p>LLM当前最重要的就是上下文窗口，虽然LLM在大海捞针的测试中表现得越来越好，但一旦有任何逻辑相关出现时，LLM通常很难理解这其中的逻辑。Context Engineering就是为了能够更好地管理上下文，让LLM更好地理解问题或完成任务。</p><p>这是我目前的思考重点：</p><ul><li>用Agent的角度思考并提供上下文<ul><li>通过Tool 的 description、参数、实现和返回值（Observation）</li><li>在system prompt中通过few shots替代workflow</li><li>其他上下文集成方案（RAG、HyDE、Post prompt等）</li></ul></li><li>保护Agent上下文<ul><li>通过分包子任务的方式（下面会讨论）</li><li>通过Compact Context的方式优化上下文</li></ul></li><li>保持简单，基建先行<ul><li>保持简单、原子化的通用工具设计，否则使用few shots+固定工具的工作流</li><li>先完成compact等基建，再考虑多agent等复杂系统</li></ul></li></ul><h2 id="Compact-Context"><a href="#Compact-Context" class="headerlink" title="Compact Context"></a>Compact Context</h2><p>首先我们讨论如何在长任务中管理Agent的上下文。目前，SOTA的模型有200K的上下文长度，Gemini 2.5 Pro甚至有1M的上下文长度，但对于代码工程而言，多轮对话往往很快就能吃光200K的上下文，这时我们就需要上下文压缩。</p><p>一种基于策略的上下文压缩方式是，设定一个固定的水位线，比如50%的token消耗，触发一次上下文压缩，在保留N条消息的情况下，对前N条上下文进行总结后，用总结的结果替换这N条消息。这个N可以选择为0（不保留消息），2（最近2条），1&#x2F;2（一半上下文）或者1&#x2F;4（25%上下文）。这种情况下主要调整三个方向：</p><ul><li>上下文压缩模型的prompt和输出结构</li><li>N值：这代表了统计学上有多少最近上下文是重要的</li><li>水位线：这代表压缩效率</li></ul><p>另一种策略是设计一个记忆系统，每次从记忆系统中获取上下文，而不是保留所有上下文。这一记忆系统可以是LLM based，也可以是基于RAG或其他搜索技术的。</p><p>二者对比，前者的缓存效率更高，调试重点更明确，更容易做出足够好的实现，后者则更加智能，但目前没有比较通用的实践。我在Cline、Claude code等看到的方式都是前者，而Windsurf据称是二者混合。</p><h2 id="Role-Based-Multi-Agent-Systems"><a href="#Role-Based-Multi-Agent-Systems" class="headerlink" title="Role Based Multi Agent Systems"></a>Role Based Multi Agent Systems</h2><p>针对上下文问题还有一个想法是通过多个Agent协作完成工作，为此，人们设计了工作流或者中心化Agent。其中一种方式是人工或自动地设计各种角色，然后让LLM以一种角色扮演的方式，沉浸于其角色之中，完成指定的任务。</p><p>我对这种方式表示怀疑，主要在于：</p><ul><li>角色扮演是额外的心智负担：对于理解自身角色，再到做出正确行动，本质是基于人类分工，基于个人认知，而LLM通常有非常广的知识，这二者并不能类比转换</li><li>基于角色的上下文隔离是低效的：人类常常陷入分工过细的“电话地狱”，Agent也不例外</li><li>基于角色的流程是脆弱的：经常由于某一个角色设计存在缺陷，导致整体处于木桶效应之中</li></ul><h2 id="Task-Based-Multi-Agent-Systems"><a href="#Task-Based-Multi-Agent-Systems" class="headerlink" title="Task Based Multi Agent Systems"></a>Task Based Multi Agent Systems</h2><p>Anthropic（和我）比较倡导的方案是按照任务划分子Agent，从而保护主Agent的上下文。比如在搜索场景中，可以并发多个Agent搜索多个领域，最后汇总成多份报告，再由审查者或主Agent进行分析。</p><p>这种方式的优势在于，主Agent从始至终负责用户需求或任务目标，而子Agent仅提供上下文层面的参考，而不是负责整个任务的执行。其次，子Agent不需要理解自身角色，从而可以更加专注自身任务，从而获得更好的性能表现。</p><p>但这一模式并不是万能的，我曾尝试分包一些代码编辑任务，但实际上表现并不好，目前来看，这一模式行之有效的只有信息搜集&#x2F;上下文获取，而不是进行修改。这和单个人类使用各种工具辅助最后完成任务非常相似，或许用超级个体来比喻这种构建方式更加适当。</p><h2 id="LLM-as-judge-evaluation"><a href="#LLM-as-judge-evaluation" class="headerlink" title="LLM-as-judge evaluation"></a>LLM-as-judge evaluation</h2><p>现在我们已经有了足够的经验来构建一个长时间运行的Agent，至少我们可以让他一直跑下去，并知道在某些任务中可以并行或协作。现在，我们还有一个最重要的问题有待解决：如何评估Agent的行动是否正确&#x2F;恰当&#x2F;有效。当一次代码编辑操作成功的时候，Agent只能从工具返回中获悉<code>编辑成功</code>，而不是真实看到所带来的改变。即使有测试用例，也只能规范代码的“围栏”而不是确认代码的正确。在其他领域，比如报告撰写、个人助手，则可能连测试用例也没有。</p><p>目前的一大研究重点是使用LLM进行评价，而其中最重要的是人类在LLM进行自动化评价的过程中，如何为评价Agent构建上下文，这包括：</p><ul><li>设计工具来观测当前任务状态和主Agent影响</li><li>设计工作流程来指导Agent进行评测</li><li>收集欺骗性案例，帮助评测Agent避开欺骗性事实</li></ul><p>正如<a href="https://ysymyth.github.io/The-Second-Half/">The Second Half</a>一文，现在Agent设计已经进入下半场，如何评测Agent将是把强化学习从后训练扩展到推理时的重要研究课题。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://ankitmaloo.com/bitter-lesson/">https://ankitmaloo.com/bitter-lesson/</a></li><li><a href="https://www.anthropic.com/engineering/building-effective-agents">https://www.anthropic.com/engineering/building-effective-agents</a></li><li><a href="https://www.anthropic.com/engineering/built-multi-agent-research-system">https://www.anthropic.com/engineering/built-multi-agent-research-system</a></li><li><a href="https://cognition.ai/blog/dont-build-multi-agents">https://cognition.ai/blog/dont-build-multi-agents</a></li></ul><h1 id="English-Version"><a href="#English-Version" class="headerlink" title="English Version"></a>English Version</h1><h2 id="What-is-Context-Engineering-1"><a href="#What-is-Context-Engineering-1" class="headerlink" title="What is Context Engineering"></a>What is Context Engineering</h2><p>Two months ago, I wrote a <a href="https://blog.wh1isper.top/2025/04/07/2025-04-08-agent-design-note/">blog post (Agent Design Note)</a> documenting some key points in designing a Coding Agent. Now I want to provide a complete exposition of my understanding of Context Engineering, which I believe is perhaps the most important aspect of building any Agent today.</p><h2 id="Key-Points-of-Context-Engineering-1"><a href="#Key-Points-of-Context-Engineering-1" class="headerlink" title="Key Points of Context Engineering"></a>Key Points of Context Engineering</h2><p>The most crucial aspect of LLMs currently is the context window. While LLMs perform increasingly well in needle-in-haystack tests, they typically struggle to understand logical relationships once any logic-related content appears. Context Engineering aims to better manage context, enabling LLMs to better understand problems or complete tasks.</p><p>These are my current focal points:</p><ul><li>Think from an Agent’s perspective and provide context<ul><li>Through Tool descriptions, parameters, implementations, and return values (Observations)</li><li>Replace workflows with few-shots in system prompts</li><li>Other context integration solutions (RAG, HyDE, Post prompt, etc.)</li></ul></li><li>Protect Agent context<ul><li>Through task decomposition approaches (discussed below)</li><li>Through Compact Context methods to optimize context</li></ul></li><li>Keep it simple, infrastructure first<ul><li>Maintain simple, atomic, general-purpose tool design; otherwise use workflows with few-shots + fixed tools</li><li>Complete infrastructure like compacting first, then consider complex systems like multi-agent</li></ul></li></ul><h2 id="Compact-Context-1"><a href="#Compact-Context-1" class="headerlink" title="Compact Context"></a>Compact Context</h2><p>First, let’s discuss how to manage Agent context in long tasks. Currently, SOTA models have 200K context length, with Gemini 2.5 Pro even reaching 1M context length. However, for code engineering, multi-turn conversations can quickly exhaust 200K context, necessitating context compression.</p><p>One policy-based context compression approach sets a fixed watermark, such as 50% token consumption, to trigger context compression. While preserving N messages, it summarizes the previous N contexts and replaces these N messages with the summary. N can be chosen as 0 (preserve no messages), 2 (most recent 2), 1&#x2F;2 (half the context), or 1&#x2F;4 (25% of context). This approach mainly adjusts three dimensions:</p><ul><li>Context compression model’s prompt and output structure</li><li>N value: represents how many recent contexts are statistically important</li><li>Watermark: represents compression efficiency</li></ul><p>Another strategy involves designing a memory system that retrieves context from the memory system rather than preserving all context. This memory system can be LLM-based or based on RAG or other search technologies.</p><p>Comparing the two, the former has higher caching efficiency, clearer debugging focus, and is easier to implement well, while the latter is more intelligent but lacks widely adopted practices. I’ve seen the former approach in Cline, Claude Code, etc., while Windsurf reportedly uses a hybrid of both.</p><h2 id="Role-Based-Multi-Agent-Systems-1"><a href="#Role-Based-Multi-Agent-Systems-1" class="headerlink" title="Role Based Multi Agent Systems"></a>Role Based Multi Agent Systems</h2><p>Another approach to address context issues involves multiple Agents collaborating to complete work, leading to the design of workflows or centralized Agents. One method involves manually or automatically designing various roles, then having LLMs engage in role-playing, immersing themselves in their roles to complete designated tasks.</p><p>I’m skeptical of this approach, mainly because:</p><ul><li>Role-playing creates additional cognitive burden: Understanding one’s role and then taking correct action is essentially based on human division of labor and individual cognition, while LLMs typically have very broad knowledge—these two cannot be analogously converted</li><li>Role-based context isolation is inefficient: Humans often fall into “phone hell” due to overly detailed division of labor, and Agents are no exception</li><li>Role-based processes are fragile: Often, flaws in a single role design cause the entire system to suffer from the barrel effect</li></ul><h2 id="Task-Based-Multi-Agent-Systems-1"><a href="#Task-Based-Multi-Agent-Systems-1" class="headerlink" title="Task Based Multi Agent Systems"></a>Task Based Multi Agent Systems</h2><p>Anthropic (and I) advocate for dividing sub-Agents by task to protect the main Agent’s context. For example, in search scenarios, multiple Agents can concurrently search different domains, finally consolidating into multiple reports for analysis by a reviewer or main Agent.</p><p>This approach’s advantage is that the main Agent remains responsible for user needs or task objectives throughout, while sub-Agents only provide contextual reference rather than being responsible for entire task execution. Additionally, sub-Agents don’t need to understand their own roles, allowing them to focus more on their tasks and achieve better performance.</p><p>However, this model isn’t universal. I’ve attempted to decompose some code editing tasks, but the actual performance wasn’t good. Currently, this model seems effective only for information gathering&#x2F;context acquisition, not for making modifications. This closely resembles a single human using various tools to ultimately complete tasks—perhaps describing this construction method as a “super individual” is more appropriate.</p><h2 id="LLM-as-judge-evaluation-1"><a href="#LLM-as-judge-evaluation-1" class="headerlink" title="LLM-as-judge evaluation"></a>LLM-as-judge evaluation</h2><p>Now we have sufficient experience to build a long-running Agent—at least we can keep it running continuously and know it can parallelize or collaborate on certain tasks. Now we face one final crucial question: how to evaluate whether an Agent’s actions are correct&#x2F;appropriate&#x2F;effective. When a code editing operation succeeds, the Agent can only learn “edit successful” from tool returns, not actually see the changes brought about. Even with test cases, they only regulate code “boundaries” rather than confirm code correctness. In other domains like report writing or personal assistance, there might not even be test cases.</p><p>A major current research focus is using LLMs for evaluation, with the most important aspect being how humans construct context for evaluation Agents during automated LLM evaluation processes, including:</p><ul><li>Designing tools to observe current task state and main Agent impact</li><li>Designing workflows to guide Agents in evaluation</li><li>Collecting deceptive cases to help evaluation Agents avoid deceptive facts</li></ul><p>As stated in <a href="https://ysymyth.github.io/The-Second-Half/">The Second Half</a>, Agent design has now entered the second half, and how to evaluate Agents will be an important research topic for extending reinforcement learning from post-training to inference time.</p><h2 id="References-1"><a href="#References-1" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://ankitmaloo.com/bitter-lesson/">https://ankitmaloo.com/bitter-lesson/</a></li><li><a href="https://www.anthropic.com/engineering/building-effective-agents">https://www.anthropic.com/engineering/building-effective-agents</a></li><li><a href="https://www.anthropic.com/engineering/built-multi-agent-research-system">https://www.anthropic.com/engineering/built-multi-agent-research-system</a></li><li><a href="https://cognition.ai/blog/dont-build-multi-agents">https://cognition.ai/blog/dont-build-multi-agents</a></li></ul>]]></content>
    
    
    <summary type="html">深入讲解Agent开发中最重要的上下文工程，涵盖上下文压缩、多Agent系统设计、LLM评估等核心议题。</summary>
    
    
    
    <category term="技术分享" scheme="https://blog.wh1isper.top/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"/>
    
    
    <category term="LLM" scheme="https://blog.wh1isper.top/tags/LLM/"/>
    
    <category term="Agent" scheme="https://blog.wh1isper.top/tags/Agent/"/>
    
    <category term="Context Engineering" scheme="https://blog.wh1isper.top/tags/Context-Engineering/"/>
    
  </entry>
  
</feed>
